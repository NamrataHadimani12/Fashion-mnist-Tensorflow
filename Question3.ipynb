{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2636,
     "status": "ok",
     "timestamp": 1618574622770,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "NcQG1iSXcHBM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3983,
     "status": "ok",
     "timestamp": 1618574624128,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "KaSNRELWcSoD",
    "outputId": "2faac264-04fd-42d7-8a19-1f111510e788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3981,
     "status": "ok",
     "timestamp": 1618574624130,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "f_ttrvqccSqj"
   },
   "outputs": [],
   "source": [
    "def softmax_activation(X):\n",
    "  expo = tf.exp(X - tf.reduce_max(X, axis=0))\n",
    "  s = tf.reduce_sum(expo, axis=0, keepdims=True)\n",
    "  output =  expo / s\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3979,
     "status": "ok",
     "timestamp": 1618574624131,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "RcQP5g0scSs0"
   },
   "outputs": [],
   "source": [
    "def forward_pass( X , W1 , b1, W2 , b2 , W3 , b3,probThreshold):\n",
    "  #push all the weights and data along with the bias through the first layer\n",
    "  hypo_1 = (W1@ X) + b1\n",
    "  relu_act1 = tf.nn.relu(hypo_1)\n",
    "\n",
    "  neuronsSize = relu_act1.shape[0]\n",
    "  trainingSize = relu_act1.shape[1]\n",
    "\n",
    "  drop_out_matrix = tf.cast(tf.random.normal(shape=(neuronsSize , trainingSize), dtype=tf.float64) * 0.01 < probThreshold,tf.float64)\n",
    "  \n",
    "  drop_out_layer = relu_act1 * drop_out_matrix\n",
    "  drop_out_layer  = relu_act1 / probThreshold\n",
    "\n",
    "  hypo_2 = (W2 @ drop_out_layer ) + b2\n",
    "  relu_act2 = tf.nn.relu(hypo_2)\n",
    "\n",
    "  hypo_3 = (W3 @ relu_act2 ) + b3\n",
    "  pred_output = softmax_activation(hypo_3)\n",
    "\n",
    "  return pred_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3977,
     "status": "ok",
     "timestamp": 1618574624132,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "2Zkuc2ZocSxq"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(tr_y , predictedYProb):\n",
    "  reduce_sum = -tf.reduce_sum(tr_y * tf.math.log(predictedYProb),axis = 0)\n",
    "  loss =  tf.cast(tf.reduce_mean(reduce_sum), tf.float64)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3975,
     "status": "ok",
     "timestamp": 1618574624133,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "lYAK8G5hcSz6"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y , predictedYProb):\n",
    "  predictedYValues = tf.round(predictedYProb)  \n",
    "  #we need only those values whose actualy value being 1 matches the predicted value being 1\n",
    "  pred_correction = tf.cast(tf.equal(np.argmax(predictedYValues, axis = 0),np.argmax(Y, axis = 0)),tf.float64)  \n",
    "  accuracy_sum = tf.reduce_sum(pred_correction)\n",
    "  acc_score = accuracy_sum / Y.shape[1]\n",
    "  return acc_score\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnkc0cqyhoyN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3974,
     "status": "ok",
     "timestamp": 1618574624134,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "7zlv8HpUcS2k"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(Xtrain,Ytrain, Xtest, Ytest, W1 , b1 , W2 , b2, W3 , b3):\n",
    "  learning_rate = 0.001\n",
    "  adam_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "  train_loss_list =[]\n",
    "  train_acc_list = []\n",
    "\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  no_of_iter = 500\n",
    "  dropOutprob = 0.3\n",
    "  probThreshold = 1 - 0.3\n",
    "\n",
    "  for i in range(no_of_iter + 1):\n",
    "    #GradientTape will record all the operation till it encounters gradient\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      train_predictedYProb  = forward_pass(X_train,W1 , b1 , W2 , b2, W3 , b3 ,probThreshold)\n",
    "      train_loss = cross_entropy(Ytrain ,train_predictedYProb)\n",
    "\n",
    "    gradients = tape.gradient(train_loss,[W1 , b1 , W2 , b2, W3 , b3])\n",
    "    train_accuracy  = calculate_accuracy(Ytrain ,train_predictedYProb)\n",
    "\n",
    "    test_predictedYProb  = forward_pass(X_test , W1 , b1 , W2 , b2, W3 , b3 ,probThreshold)\n",
    "    test_loss = cross_entropy(Ytest ,test_predictedYProb)\n",
    "    test_accuracy = calculate_accuracy(Ytest ,test_predictedYProb)\n",
    "    \n",
    "    train_loss_list.append(train_loss.numpy())\n",
    "    train_acc_list.append(train_accuracy)\n",
    "    test_loss_list.append(test_loss.numpy())\n",
    "    test_acc_list.append(test_accuracy)\n",
    "\n",
    "    adam_optimizer.apply_gradients(zip(gradients , [W1 , b1 , W2 , b2, W3 , b3]))\n",
    "\n",
    "    print(\"Train Iteration :\", i , \"Training Loss :\",train_loss.numpy() , \"Training Accuracy\",train_accuracy.numpy())\n",
    "    print(\"Test Iteration :\", i , \"Test Loss :\",test_loss.numpy() , \"test Accuracy\",test_accuracy.numpy())\n",
    "\n",
    "  return train_loss_list , train_acc_list , test_loss_list, test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QT5zlYmdC5s",
    "outputId": "07433284-9441-49a8-d3b0-b526802f69ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iteration : 0 Training Loss : 2.302612394240942 Training Accuracy 0.1\n",
      "Test Iteration : 0 Test Loss : 2.3026213800671997 test Accuracy 0.1\n",
      "Train Iteration : 1 Training Loss : 2.2976512022570383 Training Accuracy 0.1\n",
      "Test Iteration : 1 Test Loss : 2.297676998705274 test Accuracy 0.1\n",
      "Train Iteration : 2 Training Loss : 2.2887215532811265 Training Accuracy 0.1\n",
      "Test Iteration : 2 Test Loss : 2.2887944731391734 test Accuracy 0.1\n",
      "Train Iteration : 3 Training Loss : 2.2743779798793957 Training Accuracy 0.1\n",
      "Test Iteration : 3 Test Loss : 2.2745502357880047 test Accuracy 0.1\n",
      "Train Iteration : 4 Training Loss : 2.252564220526116 Training Accuracy 0.1\n",
      "Test Iteration : 4 Test Loss : 2.252844599335378 test Accuracy 0.1\n",
      "Train Iteration : 5 Training Loss : 2.221871511051411 Training Accuracy 0.1\n",
      "Test Iteration : 5 Test Loss : 2.222313576417062 test Accuracy 0.1\n",
      "Train Iteration : 6 Training Loss : 2.181491593968509 Training Accuracy 0.1\n",
      "Test Iteration : 6 Test Loss : 2.1821495616741595 test Accuracy 0.1\n",
      "Train Iteration : 7 Training Loss : 2.130609719760643 Training Accuracy 0.1\n",
      "Test Iteration : 7 Test Loss : 2.1315807925162042 test Accuracy 0.1\n",
      "Train Iteration : 8 Training Loss : 2.0698068677766366 Training Accuracy 0.1\n",
      "Test Iteration : 8 Test Loss : 2.071199875675567 test Accuracy 0.1\n",
      "Train Iteration : 9 Training Loss : 1.9999685554769646 Training Accuracy 0.1\n",
      "Test Iteration : 9 Test Loss : 2.001893958622365 test Accuracy 0.1\n",
      "Train Iteration : 10 Training Loss : 1.922354817664372 Training Accuracy 0.1\n",
      "Test Iteration : 10 Test Loss : 1.9248886594944778 test Accuracy 0.1\n",
      "Train Iteration : 11 Training Loss : 1.8390747436982369 Training Accuracy 0.1\n",
      "Test Iteration : 11 Test Loss : 1.8422402578129156 test Accuracy 0.1\n",
      "Train Iteration : 12 Training Loss : 1.7543329470302682 Training Accuracy 0.10015\n",
      "Test Iteration : 12 Test Loss : 1.7580214514352437 test Accuracy 0.1\n",
      "Train Iteration : 13 Training Loss : 1.6719843383095443 Training Accuracy 0.10076666666666667\n",
      "Test Iteration : 13 Test Loss : 1.6760319183631867 test Accuracy 0.1008\n",
      "Train Iteration : 14 Training Loss : 1.5938813144764723 Training Accuracy 0.1042\n",
      "Test Iteration : 14 Test Loss : 1.598159532407867 test Accuracy 0.1053\n",
      "Train Iteration : 15 Training Loss : 1.5200624392005035 Training Accuracy 0.12616666666666668\n",
      "Test Iteration : 15 Test Loss : 1.5245783320977477 test Accuracy 0.1262\n",
      "Train Iteration : 16 Training Loss : 1.4497197114973022 Training Accuracy 0.1469\n",
      "Test Iteration : 16 Test Loss : 1.454642869552988 test Accuracy 0.1457\n",
      "Train Iteration : 17 Training Loss : 1.3830887739072966 Training Accuracy 0.15363333333333334\n",
      "Test Iteration : 17 Test Loss : 1.3888047663005554 test Accuracy 0.1528\n",
      "Train Iteration : 18 Training Loss : 1.3224479995126395 Training Accuracy 0.15285\n",
      "Test Iteration : 18 Test Loss : 1.3293050089240144 test Accuracy 0.1517\n",
      "Train Iteration : 19 Training Loss : 1.2691857787689502 Training Accuracy 0.15691666666666668\n",
      "Test Iteration : 19 Test Loss : 1.2771151605162598 test Accuracy 0.1566\n",
      "Train Iteration : 20 Training Loss : 1.2212625495143532 Training Accuracy 0.17243333333333333\n",
      "Test Iteration : 20 Test Loss : 1.229976087218228 test Accuracy 0.1696\n",
      "Train Iteration : 21 Training Loss : 1.1772702842388696 Training Accuracy 0.2021\n",
      "Test Iteration : 21 Test Loss : 1.186757925533476 test Accuracy 0.1995\n",
      "Train Iteration : 22 Training Loss : 1.1391014008498128 Training Accuracy 0.26811666666666667\n",
      "Test Iteration : 22 Test Loss : 1.149689089002393 test Accuracy 0.2661\n",
      "Train Iteration : 23 Training Loss : 1.1070652120148594 Training Accuracy 0.3098\n",
      "Test Iteration : 23 Test Loss : 1.1190123699825145 test Accuracy 0.3064\n",
      "Train Iteration : 24 Training Loss : 1.0761023674603152 Training Accuracy 0.33623333333333333\n",
      "Test Iteration : 24 Test Loss : 1.0893887316645752 test Accuracy 0.3317\n",
      "Train Iteration : 25 Training Loss : 1.0479108534084987 Training Accuracy 0.37101666666666666\n",
      "Test Iteration : 25 Test Loss : 1.0626884072153473 test Accuracy 0.3668\n",
      "Train Iteration : 26 Training Loss : 1.022465869435438 Training Accuracy 0.40573333333333333\n",
      "Test Iteration : 26 Test Loss : 1.0384290673810093 test Accuracy 0.4011\n",
      "Train Iteration : 27 Training Loss : 0.9990353445784371 Training Accuracy 0.4285333333333333\n",
      "Test Iteration : 27 Test Loss : 1.0156740384396785 test Accuracy 0.4264\n",
      "Train Iteration : 28 Training Loss : 0.9750912026419404 Training Accuracy 0.44208333333333333\n",
      "Test Iteration : 28 Test Loss : 0.9926725333604008 test Accuracy 0.4412\n",
      "Train Iteration : 29 Training Loss : 0.9542977393775233 Training Accuracy 0.4536\n",
      "Test Iteration : 29 Test Loss : 0.973680673634374 test Accuracy 0.4525\n",
      "Train Iteration : 30 Training Loss : 0.9348600437475237 Training Accuracy 0.4751\n",
      "Test Iteration : 30 Test Loss : 0.9556599899552662 test Accuracy 0.4706\n",
      "Train Iteration : 31 Training Loss : 0.9152100415878449 Training Accuracy 0.4822\n",
      "Test Iteration : 31 Test Loss : 0.9361965561817563 test Accuracy 0.4785\n",
      "Train Iteration : 32 Training Loss : 0.8989310133502396 Training Accuracy 0.493\n",
      "Test Iteration : 32 Test Loss : 0.9196152417842081 test Accuracy 0.4866\n",
      "Train Iteration : 33 Training Loss : 0.8835323428588777 Training Accuracy 0.5233\n",
      "Test Iteration : 33 Test Loss : 0.9054080908211658 test Accuracy 0.5141\n",
      "Train Iteration : 34 Training Loss : 0.8685286021951715 Training Accuracy 0.52685\n",
      "Test Iteration : 34 Test Loss : 0.8918452970695778 test Accuracy 0.5167\n",
      "Train Iteration : 35 Training Loss : 0.8563594343042634 Training Accuracy 0.5328666666666667\n",
      "Test Iteration : 35 Test Loss : 0.8802213472540563 test Accuracy 0.5238\n",
      "Train Iteration : 36 Training Loss : 0.8443201134683046 Training Accuracy 0.5567833333333333\n",
      "Test Iteration : 36 Test Loss : 0.8684054848534702 test Accuracy 0.5466\n",
      "Train Iteration : 37 Training Loss : 0.8329025108548486 Training Accuracy 0.5602166666666667\n",
      "Test Iteration : 37 Test Loss : 0.8556202555287715 test Accuracy 0.552\n",
      "Train Iteration : 38 Training Loss : 0.8232230788590046 Training Accuracy 0.5560333333333334\n",
      "Test Iteration : 38 Test Loss : 0.8468130268457764 test Accuracy 0.5464\n",
      "Train Iteration : 39 Training Loss : 0.8127959077855283 Training Accuracy 0.5904666666666667\n",
      "Test Iteration : 39 Test Loss : 0.8367025482433798 test Accuracy 0.5829\n",
      "Train Iteration : 40 Training Loss : 0.8008052315923507 Training Accuracy 0.5827\n",
      "Test Iteration : 40 Test Loss : 0.8250135846966008 test Accuracy 0.5747\n",
      "Train Iteration : 41 Training Loss : 0.7914782497926394 Training Accuracy 0.5855666666666667\n",
      "Test Iteration : 41 Test Loss : 0.815175155795824 test Accuracy 0.578\n",
      "Train Iteration : 42 Training Loss : 0.7822532967053321 Training Accuracy 0.6100666666666666\n",
      "Test Iteration : 42 Test Loss : 0.8056041222031086 test Accuracy 0.6041\n",
      "Train Iteration : 43 Training Loss : 0.7717041482875485 Training Accuracy 0.6025666666666667\n",
      "Test Iteration : 43 Test Loss : 0.794900635987623 test Accuracy 0.5953\n",
      "Train Iteration : 44 Training Loss : 0.7630888426552042 Training Accuracy 0.6051833333333333\n",
      "Test Iteration : 44 Test Loss : 0.7854379863396229 test Accuracy 0.5994\n",
      "Train Iteration : 45 Training Loss : 0.7537585744980215 Training Accuracy 0.62205\n",
      "Test Iteration : 45 Test Loss : 0.7764019230423163 test Accuracy 0.6154\n",
      "Train Iteration : 46 Training Loss : 0.7432722540662907 Training Accuracy 0.62255\n",
      "Test Iteration : 46 Test Loss : 0.7659237746330223 test Accuracy 0.6147\n",
      "Train Iteration : 47 Training Loss : 0.7353861174460234 Training Accuracy 0.6264166666666666\n",
      "Test Iteration : 47 Test Loss : 0.7582134490987863 test Accuracy 0.6181\n",
      "Train Iteration : 48 Training Loss : 0.727108895630959 Training Accuracy 0.6415666666666666\n",
      "Test Iteration : 48 Test Loss : 0.7500930943436531 test Accuracy 0.635\n",
      "Train Iteration : 49 Training Loss : 0.7177554334089191 Training Accuracy 0.6415833333333333\n",
      "Test Iteration : 49 Test Loss : 0.7402669105370474 test Accuracy 0.6342\n",
      "Train Iteration : 50 Training Loss : 0.7102911467877964 Training Accuracy 0.6423166666666666\n",
      "Test Iteration : 50 Test Loss : 0.7332895942826987 test Accuracy 0.6357\n",
      "Train Iteration : 51 Training Loss : 0.703087972186865 Training Accuracy 0.6622666666666667\n",
      "Test Iteration : 51 Test Loss : 0.7262147688651251 test Accuracy 0.6548\n",
      "Train Iteration : 52 Training Loss : 0.6944472294429688 Training Accuracy 0.6576833333333333\n",
      "Test Iteration : 52 Test Loss : 0.7180016674111723 test Accuracy 0.6505\n",
      "Train Iteration : 53 Training Loss : 0.6875733425701649 Training Accuracy 0.6634666666666666\n",
      "Test Iteration : 53 Test Loss : 0.7114937857159385 test Accuracy 0.6558\n",
      "Train Iteration : 54 Training Loss : 0.6808188735853301 Training Accuracy 0.67355\n",
      "Test Iteration : 54 Test Loss : 0.7054612211056482 test Accuracy 0.6661\n",
      "Train Iteration : 55 Training Loss : 0.6732910943269876 Training Accuracy 0.6752833333333333\n",
      "Test Iteration : 55 Test Loss : 0.6979015261980529 test Accuracy 0.6691\n",
      "Train Iteration : 56 Training Loss : 0.667125490363502 Training Accuracy 0.678\n",
      "Test Iteration : 56 Test Loss : 0.6921813201452423 test Accuracy 0.6724\n",
      "Train Iteration : 57 Training Loss : 0.6612663275839776 Training Accuracy 0.6885166666666667\n",
      "Test Iteration : 57 Test Loss : 0.686994816930943 test Accuracy 0.6799\n",
      "Train Iteration : 58 Training Loss : 0.6545612576452327 Training Accuracy 0.6866\n",
      "Test Iteration : 58 Test Loss : 0.680638339218896 test Accuracy 0.6796\n",
      "Train Iteration : 59 Training Loss : 0.6486887105307079 Training Accuracy 0.6909666666666666\n",
      "Test Iteration : 59 Test Loss : 0.6749633247457985 test Accuracy 0.6843\n",
      "Train Iteration : 60 Training Loss : 0.6431876262895616 Training Accuracy 0.6987\n",
      "Test Iteration : 60 Test Loss : 0.6700286421982568 test Accuracy 0.69\n",
      "Train Iteration : 61 Training Loss : 0.6369938362378493 Training Accuracy 0.69835\n",
      "Test Iteration : 61 Test Loss : 0.6644943691138154 test Accuracy 0.6905\n",
      "Train Iteration : 62 Training Loss : 0.6312634692730321 Training Accuracy 0.703\n",
      "Test Iteration : 62 Test Loss : 0.6591559254615429 test Accuracy 0.695\n",
      "Train Iteration : 63 Training Loss : 0.6260469563158169 Training Accuracy 0.7071166666666666\n",
      "Test Iteration : 63 Test Loss : 0.6542574912318263 test Accuracy 0.6993\n",
      "Train Iteration : 64 Training Loss : 0.6207181414547464 Training Accuracy 0.7106166666666667\n",
      "Test Iteration : 64 Test Loss : 0.6490720838750889 test Accuracy 0.7001\n",
      "Train Iteration : 65 Training Loss : 0.6154293584325708 Training Accuracy 0.7119333333333333\n",
      "Test Iteration : 65 Test Loss : 0.6440887577064798 test Accuracy 0.7009\n",
      "Train Iteration : 66 Training Loss : 0.6111622066984803 Training Accuracy 0.7196\n",
      "Test Iteration : 66 Test Loss : 0.6400812316090971 test Accuracy 0.7096\n",
      "Train Iteration : 67 Training Loss : 0.6091005588643008 Training Accuracy 0.7108333333333333\n",
      "Test Iteration : 67 Test Loss : 0.6386723423435876 test Accuracy 0.698\n",
      "Train Iteration : 68 Training Loss : 0.6102071346552178 Training Accuracy 0.7286833333333333\n",
      "Test Iteration : 68 Test Loss : 0.6395999323124225 test Accuracy 0.7203\n",
      "Train Iteration : 69 Training Loss : 0.6080831209149548 Training Accuracy 0.7080833333333333\n",
      "Test Iteration : 69 Test Loss : 0.6381564587810824 test Accuracy 0.6958\n",
      "Train Iteration : 70 Training Loss : 0.5945242667723246 Training Accuracy 0.7278666666666667\n",
      "Test Iteration : 70 Test Loss : 0.6243641540787412 test Accuracy 0.7171\n",
      "Train Iteration : 71 Training Loss : 0.5907263577942126 Training Accuracy 0.73165\n",
      "Test Iteration : 71 Test Loss : 0.6208117894166797 test Accuracy 0.7198\n",
      "Train Iteration : 72 Training Loss : 0.5924288503546322 Training Accuracy 0.7184166666666667\n",
      "Test Iteration : 72 Test Loss : 0.6229171614711139 test Accuracy 0.7047\n",
      "Train Iteration : 73 Training Loss : 0.5818940771918026 Training Accuracy 0.734\n",
      "Test Iteration : 73 Test Loss : 0.6120844022056303 test Accuracy 0.7213\n",
      "Train Iteration : 74 Training Loss : 0.579016018278766 Training Accuracy 0.7378\n",
      "Test Iteration : 74 Test Loss : 0.6096625131971799 test Accuracy 0.7236\n",
      "Train Iteration : 75 Training Loss : 0.5787123273163565 Training Accuracy 0.7269666666666666\n",
      "Test Iteration : 75 Test Loss : 0.6099042088269344 test Accuracy 0.7127\n",
      "Train Iteration : 76 Training Loss : 0.5688733909025997 Training Accuracy 0.73825\n",
      "Test Iteration : 76 Test Loss : 0.59959422929793 test Accuracy 0.7267\n",
      "Train Iteration : 77 Training Loss : 0.569531636694128 Training Accuracy 0.743\n",
      "Test Iteration : 77 Test Loss : 0.6004295715539231 test Accuracy 0.7306\n",
      "Train Iteration : 78 Training Loss : 0.5645310581933166 Training Accuracy 0.7374833333333334\n",
      "Test Iteration : 78 Test Loss : 0.5961976645105856 test Accuracy 0.7237\n",
      "Train Iteration : 79 Training Loss : 0.5587318925749322 Training Accuracy 0.7420666666666667\n",
      "Test Iteration : 79 Test Loss : 0.5902574446865134 test Accuracy 0.7286\n",
      "Train Iteration : 80 Training Loss : 0.559160472659858 Training Accuracy 0.7484\n",
      "Test Iteration : 80 Test Loss : 0.5904353225227055 test Accuracy 0.7355\n",
      "Train Iteration : 81 Training Loss : 0.5519635237809852 Training Accuracy 0.74665\n",
      "Test Iteration : 81 Test Loss : 0.5837993836211876 test Accuracy 0.7316\n",
      "Train Iteration : 82 Training Loss : 0.5506069612749527 Training Accuracy 0.7459333333333333\n",
      "Test Iteration : 82 Test Loss : 0.5828905390652797 test Accuracy 0.7312\n",
      "Train Iteration : 83 Training Loss : 0.5470670872276826 Training Accuracy 0.75455\n",
      "Test Iteration : 83 Test Loss : 0.5790613861771432 test Accuracy 0.7402\n",
      "Train Iteration : 84 Training Loss : 0.5426664658399426 Training Accuracy 0.7543333333333333\n",
      "Test Iteration : 84 Test Loss : 0.5747493049577436 test Accuracy 0.7389\n",
      "Train Iteration : 85 Training Loss : 0.5409758253202441 Training Accuracy 0.7513833333333333\n",
      "Test Iteration : 85 Test Loss : 0.5735122920365214 test Accuracy 0.7368\n",
      "Train Iteration : 86 Training Loss : 0.5363271111699909 Training Accuracy 0.7587\n",
      "Test Iteration : 86 Test Loss : 0.5690604900549514 test Accuracy 0.7435\n",
      "Train Iteration : 87 Training Loss : 0.5343775075228432 Training Accuracy 0.7606333333333334\n",
      "Test Iteration : 87 Test Loss : 0.5671737513544213 test Accuracy 0.7446\n",
      "Train Iteration : 88 Training Loss : 0.5305918089098584 Training Accuracy 0.7588666666666667\n",
      "Test Iteration : 88 Test Loss : 0.5632368329158235 test Accuracy 0.7453\n",
      "Train Iteration : 89 Training Loss : 0.5278621232106772 Training Accuracy 0.7614666666666666\n",
      "Test Iteration : 89 Test Loss : 0.5608609083758286 test Accuracy 0.7473\n",
      "Train Iteration : 90 Training Loss : 0.5254224334573633 Training Accuracy 0.7659166666666667\n",
      "Test Iteration : 90 Test Loss : 0.5590286187860909 test Accuracy 0.7508\n",
      "Train Iteration : 91 Training Loss : 0.5214688533400262 Training Accuracy 0.76575\n",
      "Test Iteration : 91 Test Loss : 0.5547286416039909 test Accuracy 0.7521\n",
      "Train Iteration : 92 Training Loss : 0.519811330128345 Training Accuracy 0.7656666666666667\n",
      "Test Iteration : 92 Test Loss : 0.553055753044966 test Accuracy 0.7513\n",
      "Train Iteration : 93 Training Loss : 0.5162625705194237 Training Accuracy 0.7709666666666667\n",
      "Test Iteration : 93 Test Loss : 0.5502939319311104 test Accuracy 0.7556\n",
      "Train Iteration : 94 Training Loss : 0.5136195710864552 Training Accuracy 0.7738166666666667\n",
      "Test Iteration : 94 Test Loss : 0.547561226549169 test Accuracy 0.7577\n",
      "Train Iteration : 95 Training Loss : 0.5111047171724185 Training Accuracy 0.7713666666666666\n",
      "Test Iteration : 95 Test Loss : 0.5448598152645043 test Accuracy 0.7556\n",
      "Train Iteration : 96 Training Loss : 0.5079302183205795 Training Accuracy 0.7747166666666667\n",
      "Test Iteration : 96 Test Loss : 0.5421774973386648 test Accuracy 0.7601\n",
      "Train Iteration : 97 Training Loss : 0.5060235573277212 Training Accuracy 0.7790166666666667\n",
      "Test Iteration : 97 Test Loss : 0.5403890476710204 test Accuracy 0.7629\n",
      "Train Iteration : 98 Training Loss : 0.5029634255814455 Training Accuracy 0.7775333333333333\n",
      "Test Iteration : 98 Test Loss : 0.5374062665111123 test Accuracy 0.7624\n",
      "Train Iteration : 99 Training Loss : 0.500334877248625 Training Accuracy 0.7798166666666667\n",
      "Test Iteration : 99 Test Loss : 0.5346128561962258 test Accuracy 0.7641\n",
      "Train Iteration : 100 Training Loss : 0.49801077198572524 Training Accuracy 0.78375\n",
      "Test Iteration : 100 Test Loss : 0.5328015777078647 test Accuracy 0.7674\n",
      "Train Iteration : 101 Training Loss : 0.4950106052218403 Training Accuracy 0.7840166666666667\n",
      "Test Iteration : 101 Test Loss : 0.5297255717314361 test Accuracy 0.7674\n",
      "Train Iteration : 102 Training Loss : 0.49310225743808067 Training Accuracy 0.7843666666666667\n",
      "Test Iteration : 102 Test Loss : 0.5276216170250958 test Accuracy 0.7679\n",
      "Train Iteration : 103 Training Loss : 0.49051034291877815 Training Accuracy 0.7874833333333333\n",
      "Test Iteration : 103 Test Loss : 0.5256755231896143 test Accuracy 0.7714\n",
      "Train Iteration : 104 Training Loss : 0.4880662057106733 Training Accuracy 0.7895666666666666\n",
      "Test Iteration : 104 Test Loss : 0.5228109471114006 test Accuracy 0.7737\n",
      "Train Iteration : 105 Training Loss : 0.4859591431326419 Training Accuracy 0.7877666666666666\n",
      "Test Iteration : 105 Test Loss : 0.5209324824099951 test Accuracy 0.7736\n",
      "Train Iteration : 106 Training Loss : 0.48320325155312116 Training Accuracy 0.7926\n",
      "Test Iteration : 106 Test Loss : 0.5182451371760649 test Accuracy 0.7762\n",
      "Train Iteration : 107 Training Loss : 0.48100457060497415 Training Accuracy 0.7936666666666666\n",
      "Test Iteration : 107 Test Loss : 0.5163814376948651 test Accuracy 0.7795\n",
      "Train Iteration : 108 Training Loss : 0.47872700234103616 Training Accuracy 0.7939833333333334\n",
      "Test Iteration : 108 Test Loss : 0.5136580505071177 test Accuracy 0.7786\n",
      "Train Iteration : 109 Training Loss : 0.47622465983485185 Training Accuracy 0.7962166666666667\n",
      "Test Iteration : 109 Test Loss : 0.5115150157056482 test Accuracy 0.7806\n",
      "Train Iteration : 110 Training Loss : 0.47422713200677147 Training Accuracy 0.79825\n",
      "Test Iteration : 110 Test Loss : 0.5097792981362158 test Accuracy 0.7827\n",
      "Train Iteration : 111 Training Loss : 0.4719477783818858 Training Accuracy 0.79835\n",
      "Test Iteration : 111 Test Loss : 0.507256850222092 test Accuracy 0.7829\n",
      "Train Iteration : 112 Training Loss : 0.4697104869717574 Training Accuracy 0.7995666666666666\n",
      "Test Iteration : 112 Test Loss : 0.5051807924658475 test Accuracy 0.7835\n",
      "Train Iteration : 113 Training Loss : 0.46775471402217095 Training Accuracy 0.8021\n",
      "Test Iteration : 113 Test Loss : 0.5034474379686908 test Accuracy 0.7868\n",
      "Train Iteration : 114 Training Loss : 0.4656296534929701 Training Accuracy 0.8017333333333333\n",
      "Test Iteration : 114 Test Loss : 0.5014665065957459 test Accuracy 0.7864\n",
      "Train Iteration : 115 Training Loss : 0.46385632207323557 Training Accuracy 0.8035333333333333\n",
      "Test Iteration : 115 Test Loss : 0.49935452557371174 test Accuracy 0.7881\n",
      "Train Iteration : 116 Training Loss : 0.46296794627330323 Training Accuracy 0.8029333333333334\n",
      "Test Iteration : 116 Test Loss : 0.499448768735281 test Accuracy 0.7883\n",
      "Train Iteration : 117 Training Loss : 0.4645663847305864 Training Accuracy 0.8044333333333333\n",
      "Test Iteration : 117 Test Loss : 0.499981871113966 test Accuracy 0.7906\n",
      "Train Iteration : 118 Training Loss : 0.4669858919687272 Training Accuracy 0.79925\n",
      "Test Iteration : 118 Test Loss : 0.5043452876214893 test Accuracy 0.7839\n",
      "Train Iteration : 119 Training Loss : 0.4706535541873608 Training Accuracy 0.8045833333333333\n",
      "Test Iteration : 119 Test Loss : 0.5061644098603829 test Accuracy 0.7909\n",
      "Train Iteration : 120 Training Loss : 0.4560904827153678 Training Accuracy 0.805\n",
      "Test Iteration : 120 Test Loss : 0.492454821579184 test Accuracy 0.7907\n",
      "Train Iteration : 121 Training Loss : 0.4561235900025087 Training Accuracy 0.8061333333333334\n",
      "Test Iteration : 121 Test Loss : 0.4930625014370423 test Accuracy 0.7903\n",
      "Train Iteration : 122 Training Loss : 0.46355061893719823 Training Accuracy 0.8083666666666667\n",
      "Test Iteration : 122 Test Loss : 0.49938242671951283 test Accuracy 0.794\n",
      "Train Iteration : 123 Training Loss : 0.45113425264369994 Training Accuracy 0.8076166666666666\n",
      "Test Iteration : 123 Test Loss : 0.4874725243148501 test Accuracy 0.7937\n",
      "Train Iteration : 124 Training Loss : 0.452133095828958 Training Accuracy 0.8082333333333334\n",
      "Test Iteration : 124 Test Loss : 0.48916474867612353 test Accuracy 0.7919\n",
      "Train Iteration : 125 Training Loss : 0.45703090306591937 Training Accuracy 0.8109666666666666\n",
      "Test Iteration : 125 Test Loss : 0.4931321310150386 test Accuracy 0.7949\n",
      "Train Iteration : 126 Training Loss : 0.4455077352785625 Training Accuracy 0.8110333333333334\n",
      "Test Iteration : 126 Test Loss : 0.48175336319874196 test Accuracy 0.7978\n",
      "Train Iteration : 127 Training Loss : 0.45124877739478414 Training Accuracy 0.8090166666666667\n",
      "Test Iteration : 127 Test Loss : 0.4886506515104422 test Accuracy 0.792\n",
      "Train Iteration : 128 Training Loss : 0.45206933094440205 Training Accuracy 0.81295\n",
      "Test Iteration : 128 Test Loss : 0.48839700102162126 test Accuracy 0.7984\n",
      "Train Iteration : 129 Training Loss : 0.4416669831823242 Training Accuracy 0.8148166666666666\n",
      "Test Iteration : 129 Test Loss : 0.47784871765463577 test Accuracy 0.8013\n",
      "Train Iteration : 130 Training Loss : 0.4514336133177155 Training Accuracy 0.8077\n",
      "Test Iteration : 130 Test Loss : 0.48908910407775363 test Accuracy 0.7919\n",
      "Train Iteration : 131 Training Loss : 0.4446696354618504 Training Accuracy 0.8158333333333333\n",
      "Test Iteration : 131 Test Loss : 0.48113966602654284 test Accuracy 0.8021\n",
      "Train Iteration : 132 Training Loss : 0.44051818361645056 Training Accuracy 0.8161333333333334\n",
      "Test Iteration : 132 Test Loss : 0.47673736802870514 test Accuracy 0.8028\n",
      "Train Iteration : 133 Training Loss : 0.4459819398816644 Training Accuracy 0.8105666666666667\n",
      "Test Iteration : 133 Test Loss : 0.4834218639503148 test Accuracy 0.7947\n",
      "Train Iteration : 134 Training Loss : 0.4354045210414897 Training Accuracy 0.8193166666666667\n",
      "Test Iteration : 134 Test Loss : 0.4720293546997669 test Accuracy 0.8055\n",
      "Train Iteration : 135 Training Loss : 0.4390127558308451 Training Accuracy 0.81795\n",
      "Test Iteration : 135 Test Loss : 0.4753609226489722 test Accuracy 0.8045\n",
      "Train Iteration : 136 Training Loss : 0.4346663006661724 Training Accuracy 0.8158166666666666\n",
      "Test Iteration : 136 Test Loss : 0.47144913194987265 test Accuracy 0.8025\n",
      "Train Iteration : 137 Training Loss : 0.4329161044063095 Training Accuracy 0.8184666666666667\n",
      "Test Iteration : 137 Test Loss : 0.4700121058172619 test Accuracy 0.8049\n",
      "Train Iteration : 138 Training Loss : 0.4341520600997673 Training Accuracy 0.82\n",
      "Test Iteration : 138 Test Loss : 0.47072734004354505 test Accuracy 0.8068\n",
      "Train Iteration : 139 Training Loss : 0.4292180937938019 Training Accuracy 0.8207833333333333\n",
      "Test Iteration : 139 Test Loss : 0.4656753646211468 test Accuracy 0.8067\n",
      "Train Iteration : 140 Training Loss : 0.43226234723821655 Training Accuracy 0.8183333333333334\n",
      "Test Iteration : 140 Test Loss : 0.469638934980052 test Accuracy 0.8033\n",
      "Train Iteration : 141 Training Loss : 0.42759099370526954 Training Accuracy 0.8231\n",
      "Test Iteration : 141 Test Loss : 0.4644473499296208 test Accuracy 0.8097\n",
      "Train Iteration : 142 Training Loss : 0.42774417438438744 Training Accuracy 0.8225666666666667\n",
      "Test Iteration : 142 Test Loss : 0.4642827800645174 test Accuracy 0.8091\n",
      "Train Iteration : 143 Training Loss : 0.4265314938264142 Training Accuracy 0.8205166666666667\n",
      "Test Iteration : 143 Test Loss : 0.46363352957848303 test Accuracy 0.8071\n",
      "Train Iteration : 144 Training Loss : 0.4236906421590696 Training Accuracy 0.8241833333333334\n",
      "Test Iteration : 144 Test Loss : 0.46087199625455416 test Accuracy 0.8083\n",
      "Train Iteration : 145 Training Loss : 0.42464433222504294 Training Accuracy 0.8242333333333334\n",
      "Test Iteration : 145 Test Loss : 0.46141497287948496 test Accuracy 0.8099\n",
      "Train Iteration : 146 Training Loss : 0.4212956390418231 Training Accuracy 0.8234166666666667\n",
      "Test Iteration : 146 Test Loss : 0.4581676900069181 test Accuracy 0.8088\n",
      "Train Iteration : 147 Training Loss : 0.4219901781854768 Training Accuracy 0.82375\n",
      "Test Iteration : 147 Test Loss : 0.4593881970711167 test Accuracy 0.8088\n",
      "Train Iteration : 148 Training Loss : 0.42008979749257713 Training Accuracy 0.8263\n",
      "Test Iteration : 148 Test Loss : 0.4570941233512654 test Accuracy 0.811\n",
      "Train Iteration : 149 Training Loss : 0.4186866042665456 Training Accuracy 0.8263666666666667\n",
      "Test Iteration : 149 Test Loss : 0.4556015528331669 test Accuracy 0.811\n",
      "Train Iteration : 150 Training Loss : 0.4187617890691372 Training Accuracy 0.8250333333333333\n",
      "Test Iteration : 150 Test Loss : 0.45617191913360905 test Accuracy 0.8088\n",
      "Train Iteration : 151 Training Loss : 0.4161314395279953 Training Accuracy 0.8278833333333333\n",
      "Test Iteration : 151 Test Loss : 0.4533201362289466 test Accuracy 0.812\n",
      "Train Iteration : 152 Training Loss : 0.4162606775485035 Training Accuracy 0.8278\n",
      "Test Iteration : 152 Test Loss : 0.45334440746961807 test Accuracy 0.8129\n",
      "Train Iteration : 153 Training Loss : 0.41461559225612704 Training Accuracy 0.8274666666666667\n",
      "Test Iteration : 153 Test Loss : 0.45204644369821706 test Accuracy 0.8115\n",
      "Train Iteration : 154 Training Loss : 0.41343893632229434 Training Accuracy 0.8284\n",
      "Test Iteration : 154 Test Loss : 0.45085550366824845 test Accuracy 0.8124\n",
      "Train Iteration : 155 Training Loss : 0.413104867792999 Training Accuracy 0.8293333333333334\n",
      "Test Iteration : 155 Test Loss : 0.450371087450736 test Accuracy 0.813\n",
      "Train Iteration : 156 Training Loss : 0.41124782402699106 Training Accuracy 0.8301666666666667\n",
      "Test Iteration : 156 Test Loss : 0.44878024828387925 test Accuracy 0.8128\n",
      "Train Iteration : 157 Training Loss : 0.41097941876148525 Training Accuracy 0.829\n",
      "Test Iteration : 157 Test Loss : 0.44864421101788005 test Accuracy 0.8125\n",
      "Train Iteration : 158 Training Loss : 0.40974346193555866 Training Accuracy 0.8309833333333333\n",
      "Test Iteration : 158 Test Loss : 0.4472406169220303 test Accuracy 0.8147\n",
      "Train Iteration : 159 Training Loss : 0.40854305248530826 Training Accuracy 0.83175\n",
      "Test Iteration : 159 Test Loss : 0.4462562984083058 test Accuracy 0.8139\n",
      "Train Iteration : 160 Training Loss : 0.4081022643546354 Training Accuracy 0.8305\n",
      "Test Iteration : 160 Test Loss : 0.44594883207722397 test Accuracy 0.8134\n",
      "Train Iteration : 161 Training Loss : 0.4066316228882866 Training Accuracy 0.8324\n",
      "Test Iteration : 161 Test Loss : 0.4443765318454085 test Accuracy 0.8144\n",
      "Train Iteration : 162 Training Loss : 0.4059222474124659 Training Accuracy 0.8333833333333334\n",
      "Test Iteration : 162 Test Loss : 0.44386238267258016 test Accuracy 0.8155\n",
      "Train Iteration : 163 Training Loss : 0.4050641168886985 Training Accuracy 0.8324833333333334\n",
      "Test Iteration : 163 Test Loss : 0.44308511776351706 test Accuracy 0.8157\n",
      "Train Iteration : 164 Training Loss : 0.40382867848214965 Training Accuracy 0.834\n",
      "Test Iteration : 164 Test Loss : 0.4418531763585126 test Accuracy 0.8162\n",
      "Train Iteration : 165 Training Loss : 0.4032179234816607 Training Accuracy 0.8344833333333334\n",
      "Test Iteration : 165 Test Loss : 0.44141421352883736 test Accuracy 0.8173\n",
      "Train Iteration : 166 Training Loss : 0.4021510499939295 Training Accuracy 0.8345\n",
      "Test Iteration : 166 Test Loss : 0.4403560093911705 test Accuracy 0.8164\n",
      "Train Iteration : 167 Training Loss : 0.40119152801531593 Training Accuracy 0.8353\n",
      "Test Iteration : 167 Test Loss : 0.439516994469746 test Accuracy 0.8176\n",
      "Train Iteration : 168 Training Loss : 0.400495140692066 Training Accuracy 0.83585\n",
      "Test Iteration : 168 Test Loss : 0.4389791212616754 test Accuracy 0.8187\n",
      "Train Iteration : 169 Training Loss : 0.3994245849168172 Training Accuracy 0.8358666666666666\n",
      "Test Iteration : 169 Test Loss : 0.4378621452119523 test Accuracy 0.8182\n",
      "Train Iteration : 170 Training Loss : 0.39860237786313363 Training Accuracy 0.8365333333333334\n",
      "Test Iteration : 170 Test Loss : 0.4372308957203477 test Accuracy 0.8184\n",
      "Train Iteration : 171 Training Loss : 0.39781535962151937 Training Accuracy 0.8370166666666666\n",
      "Test Iteration : 171 Test Loss : 0.4365892444089881 test Accuracy 0.82\n",
      "Train Iteration : 172 Training Loss : 0.39681743260295044 Training Accuracy 0.8370833333333333\n",
      "Test Iteration : 172 Test Loss : 0.43552315357847254 test Accuracy 0.8198\n",
      "Train Iteration : 173 Training Loss : 0.3960387950199847 Training Accuracy 0.8377\n",
      "Test Iteration : 173 Test Loss : 0.43498599477114397 test Accuracy 0.8192\n",
      "Train Iteration : 174 Training Loss : 0.39520175867786195 Training Accuracy 0.83825\n",
      "Test Iteration : 174 Test Loss : 0.4342588829890868 test Accuracy 0.8212\n",
      "Train Iteration : 175 Training Loss : 0.39428119820259416 Training Accuracy 0.8384333333333334\n",
      "Test Iteration : 175 Test Loss : 0.43327065985649527 test Accuracy 0.8215\n",
      "Train Iteration : 176 Training Loss : 0.3935145285366054 Training Accuracy 0.8391\n",
      "Test Iteration : 176 Test Loss : 0.43281605505927545 test Accuracy 0.8198\n",
      "Train Iteration : 177 Training Loss : 0.3926568546369057 Training Accuracy 0.8393333333333334\n",
      "Test Iteration : 177 Test Loss : 0.4319972406328918 test Accuracy 0.822\n",
      "Train Iteration : 178 Training Loss : 0.39178268878756195 Training Accuracy 0.83965\n",
      "Test Iteration : 178 Test Loss : 0.431145697915634 test Accuracy 0.8229\n",
      "Train Iteration : 179 Training Loss : 0.3910212471318998 Training Accuracy 0.8401333333333333\n",
      "Test Iteration : 179 Test Loss : 0.43067634136755895 test Accuracy 0.8211\n",
      "Train Iteration : 180 Training Loss : 0.3901726262399423 Training Accuracy 0.8403833333333334\n",
      "Test Iteration : 180 Test Loss : 0.42982298662675156 test Accuracy 0.8232\n",
      "Train Iteration : 181 Training Loss : 0.3893223484030583 Training Accuracy 0.8408333333333333\n",
      "Test Iteration : 181 Test Loss : 0.4290875505699052 test Accuracy 0.8233\n",
      "Train Iteration : 182 Training Loss : 0.3885628470793536 Training Accuracy 0.8411\n",
      "Test Iteration : 182 Test Loss : 0.4285619907342378 test Accuracy 0.8225\n",
      "Train Iteration : 183 Training Loss : 0.3877323143659532 Training Accuracy 0.8415833333333333\n",
      "Test Iteration : 183 Test Loss : 0.4277308875142315 test Accuracy 0.8243\n",
      "Train Iteration : 184 Training Loss : 0.38689288657840715 Training Accuracy 0.84195\n",
      "Test Iteration : 184 Test Loss : 0.4270646735138247 test Accuracy 0.8241\n",
      "Train Iteration : 185 Training Loss : 0.3861306561767185 Training Accuracy 0.8423666666666667\n",
      "Test Iteration : 185 Test Loss : 0.42644624700180267 test Accuracy 0.8231\n",
      "Train Iteration : 186 Training Loss : 0.3853217819636142 Training Accuracy 0.8427333333333333\n",
      "Test Iteration : 186 Test Loss : 0.4257072324523876 test Accuracy 0.8248\n",
      "Train Iteration : 187 Training Loss : 0.3844823013566188 Training Accuracy 0.8429333333333333\n",
      "Test Iteration : 187 Test Loss : 0.42502471642299033 test Accuracy 0.8249\n",
      "Train Iteration : 188 Training Loss : 0.38371086073771465 Training Accuracy 0.8433\n",
      "Test Iteration : 188 Test Loss : 0.4244099713644544 test Accuracy 0.8245\n",
      "Train Iteration : 189 Training Loss : 0.38293310173721645 Training Accuracy 0.8437166666666667\n",
      "Test Iteration : 189 Test Loss : 0.42373642972617326 test Accuracy 0.8253\n",
      "Train Iteration : 190 Training Loss : 0.38210670722192747 Training Accuracy 0.8441666666666666\n",
      "Test Iteration : 190 Test Loss : 0.42308094229839355 test Accuracy 0.8251\n",
      "Train Iteration : 191 Training Loss : 0.3813108108542999 Training Accuracy 0.8443666666666667\n",
      "Test Iteration : 191 Test Loss : 0.4224581918800756 test Accuracy 0.8254\n",
      "Train Iteration : 192 Training Loss : 0.38054265209320126 Training Accuracy 0.8446\n",
      "Test Iteration : 192 Test Loss : 0.42172615896973636 test Accuracy 0.8264\n",
      "Train Iteration : 193 Training Loss : 0.379739643363295 Training Accuracy 0.8450166666666666\n",
      "Test Iteration : 193 Test Loss : 0.421159471949557 test Accuracy 0.8258\n",
      "Train Iteration : 194 Training Loss : 0.37893195699543253 Training Accuracy 0.8453\n",
      "Test Iteration : 194 Test Loss : 0.42042537145652115 test Accuracy 0.8272\n",
      "Train Iteration : 195 Training Loss : 0.3781550901324234 Training Accuracy 0.8456333333333333\n",
      "Test Iteration : 195 Test Loss : 0.41979044981572644 test Accuracy 0.8274\n",
      "Train Iteration : 196 Training Loss : 0.377378787120071 Training Accuracy 0.8458833333333333\n",
      "Test Iteration : 196 Test Loss : 0.4192245115957496 test Accuracy 0.8268\n",
      "Train Iteration : 197 Training Loss : 0.37658207120872816 Training Accuracy 0.8459833333333333\n",
      "Test Iteration : 197 Test Loss : 0.41845354689612213 test Accuracy 0.8281\n",
      "Train Iteration : 198 Training Loss : 0.3757909305599511 Training Accuracy 0.84645\n",
      "Test Iteration : 198 Test Loss : 0.4179214416568729 test Accuracy 0.8284\n",
      "Train Iteration : 199 Training Loss : 0.37501530076611134 Training Accuracy 0.84665\n",
      "Test Iteration : 199 Test Loss : 0.4172058140645962 test Accuracy 0.8282\n",
      "Train Iteration : 200 Training Loss : 0.37423796314994107 Training Accuracy 0.8471166666666666\n",
      "Test Iteration : 200 Test Loss : 0.41661933795906636 test Accuracy 0.829\n",
      "Train Iteration : 201 Training Loss : 0.3734516563840416 Training Accuracy 0.8473666666666667\n",
      "Test Iteration : 201 Test Loss : 0.41596368762369357 test Accuracy 0.8291\n",
      "Train Iteration : 202 Training Loss : 0.37266846631130573 Training Accuracy 0.8477333333333333\n",
      "Test Iteration : 202 Test Loss : 0.4153182888075813 test Accuracy 0.8298\n",
      "Train Iteration : 203 Training Loss : 0.3718973932736785 Training Accuracy 0.8481166666666666\n",
      "Test Iteration : 203 Test Loss : 0.41465438961660767 test Accuracy 0.8304\n",
      "Train Iteration : 204 Training Loss : 0.37112965719653623 Training Accuracy 0.8482666666666666\n",
      "Test Iteration : 204 Test Loss : 0.4140909267079218 test Accuracy 0.8296\n",
      "Train Iteration : 205 Training Loss : 0.3703567546361114 Training Accuracy 0.8485333333333334\n",
      "Test Iteration : 205 Test Loss : 0.41338066058249423 test Accuracy 0.8304\n",
      "Train Iteration : 206 Training Loss : 0.3695907497819742 Training Accuracy 0.84915\n",
      "Test Iteration : 206 Test Loss : 0.4128873838523496 test Accuracy 0.8306\n",
      "Train Iteration : 207 Training Loss : 0.36884451771109755 Training Accuracy 0.8488833333333333\n",
      "Test Iteration : 207 Test Loss : 0.4121171640137926 test Accuracy 0.8309\n",
      "Train Iteration : 208 Training Loss : 0.3681286013993144 Training Accuracy 0.84975\n",
      "Test Iteration : 208 Test Loss : 0.411788940375506 test Accuracy 0.8316\n",
      "Train Iteration : 209 Training Loss : 0.36746232190563066 Training Accuracy 0.8493333333333334\n",
      "Test Iteration : 209 Test Loss : 0.4109385914590156 test Accuracy 0.8315\n",
      "Train Iteration : 210 Training Loss : 0.366899959127014 Training Accuracy 0.8503666666666667\n",
      "Test Iteration : 210 Test Loss : 0.41102453748626483 test Accuracy 0.8326\n",
      "Train Iteration : 211 Training Loss : 0.36651183123909675 Training Accuracy 0.8496666666666667\n",
      "Test Iteration : 211 Test Loss : 0.4101800379615546 test Accuracy 0.8318\n",
      "Train Iteration : 212 Training Loss : 0.3664936388295666 Training Accuracy 0.8508\n",
      "Test Iteration : 212 Test Loss : 0.4110590282769643 test Accuracy 0.8335\n",
      "Train Iteration : 213 Training Loss : 0.366671847986919 Training Accuracy 0.8489833333333333\n",
      "Test Iteration : 213 Test Loss : 0.410655454384171 test Accuracy 0.8312\n",
      "Train Iteration : 214 Training Loss : 0.3672988124468301 Training Accuracy 0.8505333333333334\n",
      "Test Iteration : 214 Test Loss : 0.41218574485700193 test Accuracy 0.8323\n",
      "Train Iteration : 215 Training Loss : 0.36612994456241 Training Accuracy 0.8494333333333334\n",
      "Test Iteration : 215 Test Loss : 0.4105125654232803 test Accuracy 0.8324\n",
      "Train Iteration : 216 Training Loss : 0.36408222743479063 Training Accuracy 0.8513666666666667\n",
      "Test Iteration : 216 Test Loss : 0.4090248856037926 test Accuracy 0.8345\n",
      "Train Iteration : 217 Training Loss : 0.3617880695486388 Training Accuracy 0.85225\n",
      "Test Iteration : 217 Test Loss : 0.4065749351332485 test Accuracy 0.834\n",
      "Train Iteration : 218 Training Loss : 0.36123393841760215 Training Accuracy 0.8524833333333334\n",
      "Test Iteration : 218 Test Loss : 0.4062246012355742 test Accuracy 0.834\n",
      "Train Iteration : 219 Training Loss : 0.3617760136498663 Training Accuracy 0.85205\n",
      "Test Iteration : 219 Test Loss : 0.407188181518184 test Accuracy 0.8355\n",
      "Train Iteration : 220 Training Loss : 0.3615310723123324 Training Accuracy 0.8517166666666667\n",
      "Test Iteration : 220 Test Loss : 0.40670078023412404 test Accuracy 0.8329\n",
      "Train Iteration : 221 Training Loss : 0.3604582759441513 Training Accuracy 0.8529833333333333\n",
      "Test Iteration : 221 Test Loss : 0.40636586464632324 test Accuracy 0.8371\n",
      "Train Iteration : 222 Training Loss : 0.3587187890418776 Training Accuracy 0.8530333333333333\n",
      "Test Iteration : 222 Test Loss : 0.40427024007097984 test Accuracy 0.8357\n",
      "Train Iteration : 223 Training Loss : 0.3575863231965103 Training Accuracy 0.8536833333333333\n",
      "Test Iteration : 223 Test Loss : 0.4034657108995621 test Accuracy 0.8355\n",
      "Train Iteration : 224 Training Loss : 0.35740576756988474 Training Accuracy 0.8544\n",
      "Test Iteration : 224 Test Loss : 0.40372759059797 test Accuracy 0.8379\n",
      "Train Iteration : 225 Training Loss : 0.3573289831061164 Training Accuracy 0.8534\n",
      "Test Iteration : 225 Test Loss : 0.4032831120302717 test Accuracy 0.8353\n",
      "Train Iteration : 226 Training Loss : 0.35662757658311406 Training Accuracy 0.85475\n",
      "Test Iteration : 226 Test Loss : 0.4032647376618457 test Accuracy 0.8383\n",
      "Train Iteration : 227 Training Loss : 0.3553886833102406 Training Accuracy 0.8550166666666666\n",
      "Test Iteration : 227 Test Loss : 0.4019484380808597 test Accuracy 0.8361\n",
      "Train Iteration : 228 Training Loss : 0.3544166079678969 Training Accuracy 0.85495\n",
      "Test Iteration : 228 Test Loss : 0.4009606993480405 test Accuracy 0.8372\n",
      "Train Iteration : 229 Training Loss : 0.35391100996598646 Training Accuracy 0.8563666666666667\n",
      "Test Iteration : 229 Test Loss : 0.4010495152931631 test Accuracy 0.8394\n",
      "Train Iteration : 230 Training Loss : 0.3535573307905939 Training Accuracy 0.8551833333333333\n",
      "Test Iteration : 230 Test Loss : 0.40043877666081723 test Accuracy 0.8365\n",
      "Train Iteration : 231 Training Loss : 0.3530622034561221 Training Accuracy 0.8558666666666667\n",
      "Test Iteration : 231 Test Loss : 0.4003961108062065 test Accuracy 0.839\n",
      "Train Iteration : 232 Training Loss : 0.35221921170126896 Training Accuracy 0.8559333333333333\n",
      "Test Iteration : 232 Test Loss : 0.39961368551246057 test Accuracy 0.8373\n",
      "Train Iteration : 233 Training Loss : 0.3513733005183438 Training Accuracy 0.85715\n",
      "Test Iteration : 233 Test Loss : 0.39888541611196915 test Accuracy 0.8383\n",
      "Train Iteration : 234 Training Loss : 0.3506706271808969 Training Accuracy 0.85725\n",
      "Test Iteration : 234 Test Loss : 0.3984841699142668 test Accuracy 0.8399\n",
      "Train Iteration : 235 Training Loss : 0.350134234941705 Training Accuracy 0.8565333333333334\n",
      "Test Iteration : 235 Test Loss : 0.39784723458469046 test Accuracy 0.8383\n",
      "Train Iteration : 236 Training Loss : 0.34963551975782575 Training Accuracy 0.85785\n",
      "Test Iteration : 236 Test Loss : 0.3978860203650137 test Accuracy 0.8406\n",
      "Train Iteration : 237 Training Loss : 0.34899760240118644 Training Accuracy 0.85675\n",
      "Test Iteration : 237 Test Loss : 0.3969724640067155 test Accuracy 0.8373\n",
      "Train Iteration : 238 Training Loss : 0.34821171675111884 Training Accuracy 0.8587333333333333\n",
      "Test Iteration : 238 Test Loss : 0.39677178581725386 test Accuracy 0.84\n",
      "Train Iteration : 239 Training Loss : 0.34743589028646954 Training Accuracy 0.85805\n",
      "Test Iteration : 239 Test Loss : 0.39591217933177164 test Accuracy 0.8389\n",
      "Train Iteration : 240 Training Loss : 0.3467773444148759 Training Accuracy 0.8589\n",
      "Test Iteration : 240 Test Loss : 0.39546679735486223 test Accuracy 0.8399\n",
      "Train Iteration : 241 Training Loss : 0.3462184889040675 Training Accuracy 0.8590833333333333\n",
      "Test Iteration : 241 Test Loss : 0.39525247731174357 test Accuracy 0.8405\n",
      "Train Iteration : 242 Training Loss : 0.34572461519112924 Training Accuracy 0.8587833333333333\n",
      "Test Iteration : 242 Test Loss : 0.3946195031360817 test Accuracy 0.8391\n",
      "Train Iteration : 243 Training Loss : 0.3452597894133891 Training Accuracy 0.8599833333333333\n",
      "Test Iteration : 243 Test Loss : 0.39471574389883834 test Accuracy 0.8418\n",
      "Train Iteration : 244 Training Loss : 0.3447389937397077 Training Accuracy 0.8590333333333333\n",
      "Test Iteration : 244 Test Loss : 0.393910143587384 test Accuracy 0.8393\n",
      "Train Iteration : 245 Training Loss : 0.3441583565681175 Training Accuracy 0.8606833333333334\n",
      "Test Iteration : 245 Test Loss : 0.3939723514824226 test Accuracy 0.8422\n",
      "Train Iteration : 246 Training Loss : 0.3435669910132328 Training Accuracy 0.85975\n",
      "Test Iteration : 246 Test Loss : 0.3930256226122806 test Accuracy 0.8402\n",
      "Train Iteration : 247 Training Loss : 0.3430660032223888 Training Accuracy 0.8609166666666667\n",
      "Test Iteration : 247 Test Loss : 0.3932117903578744 test Accuracy 0.8423\n",
      "Train Iteration : 248 Training Loss : 0.34260437690308365 Training Accuracy 0.8604833333333334\n",
      "Test Iteration : 248 Test Loss : 0.39241495927440356 test Accuracy 0.8415\n",
      "Train Iteration : 249 Training Loss : 0.34241098150179966 Training Accuracy 0.8605\n",
      "Test Iteration : 249 Test Loss : 0.3928888094785067 test Accuracy 0.8422\n",
      "Train Iteration : 250 Training Loss : 0.3423047382472892 Training Accuracy 0.8603833333333334\n",
      "Test Iteration : 250 Test Loss : 0.3924505829379703 test Accuracy 0.8409\n",
      "Train Iteration : 251 Training Loss : 0.3426872589512281 Training Accuracy 0.8607833333333333\n",
      "Test Iteration : 251 Test Loss : 0.3935947822904404 test Accuracy 0.8421\n",
      "Train Iteration : 252 Training Loss : 0.34279394158394777 Training Accuracy 0.8601\n",
      "Test Iteration : 252 Test Loss : 0.39318223854812484 test Accuracy 0.8406\n",
      "Train Iteration : 253 Training Loss : 0.3432448512488331 Training Accuracy 0.8611333333333333\n",
      "Test Iteration : 253 Test Loss : 0.3946096885496264 test Accuracy 0.8427\n",
      "Train Iteration : 254 Training Loss : 0.3420129305374008 Training Accuracy 0.8603666666666666\n",
      "Test Iteration : 254 Test Loss : 0.3926583890833686 test Accuracy 0.8404\n",
      "Train Iteration : 255 Training Loss : 0.34054580853497624 Training Accuracy 0.8619\n",
      "Test Iteration : 255 Test Loss : 0.3920384430745677 test Accuracy 0.8431\n",
      "Train Iteration : 256 Training Loss : 0.33831911529277225 Training Accuracy 0.8623833333333333\n",
      "Test Iteration : 256 Test Loss : 0.3893825353662724 test Accuracy 0.8423\n",
      "Train Iteration : 257 Training Loss : 0.3370016525404152 Training Accuracy 0.8631166666666666\n",
      "Test Iteration : 257 Test Loss : 0.3885093259823476 test Accuracy 0.8433\n",
      "Train Iteration : 258 Training Loss : 0.33672643679120406 Training Accuracy 0.8629333333333333\n",
      "Test Iteration : 258 Test Loss : 0.3883664289199187 test Accuracy 0.8427\n",
      "Train Iteration : 259 Training Loss : 0.33708537002645 Training Accuracy 0.8628666666666667\n",
      "Test Iteration : 259 Test Loss : 0.3888607679637581 test Accuracy 0.8423\n",
      "Train Iteration : 260 Training Loss : 0.3374272478592013 Training Accuracy 0.8629\n",
      "Test Iteration : 260 Test Loss : 0.38963938894839395 test Accuracy 0.8431\n",
      "Train Iteration : 261 Training Loss : 0.3369846283875173 Training Accuracy 0.8627833333333333\n",
      "Test Iteration : 261 Test Loss : 0.38896755059558974 test Accuracy 0.8419\n",
      "Train Iteration : 262 Training Loss : 0.33599416700303597 Training Accuracy 0.8639666666666667\n",
      "Test Iteration : 262 Test Loss : 0.3885869009904229 test Accuracy 0.8427\n",
      "Train Iteration : 263 Training Loss : 0.3345152977583897 Training Accuracy 0.8638833333333333\n",
      "Test Iteration : 263 Test Loss : 0.3867074729135463 test Accuracy 0.8433\n",
      "Train Iteration : 264 Training Loss : 0.3333935857120658 Training Accuracy 0.8648166666666667\n",
      "Test Iteration : 264 Test Loss : 0.3861043931007236 test Accuracy 0.8444\n",
      "Train Iteration : 265 Training Loss : 0.33289372408967866 Training Accuracy 0.8645666666666667\n",
      "Test Iteration : 265 Test Loss : 0.3855531383903095 test Accuracy 0.8442\n",
      "Train Iteration : 266 Training Loss : 0.3328744169684316 Training Accuracy 0.8646833333333334\n",
      "Test Iteration : 266 Test Loss : 0.3857984498801827 test Accuracy 0.8439\n",
      "Train Iteration : 267 Training Loss : 0.3328326755113374 Training Accuracy 0.8645\n",
      "Test Iteration : 267 Test Loss : 0.3859915663233839 test Accuracy 0.8434\n",
      "Train Iteration : 268 Training Loss : 0.33259772796246123 Training Accuracy 0.86475\n",
      "Test Iteration : 268 Test Loss : 0.38590194336787376 test Accuracy 0.8436\n",
      "Train Iteration : 269 Training Loss : 0.3318452662722033 Training Accuracy 0.8648166666666667\n",
      "Test Iteration : 269 Test Loss : 0.3852392618463703 test Accuracy 0.8438\n",
      "Train Iteration : 270 Training Loss : 0.3309346095033095 Training Accuracy 0.86575\n",
      "Test Iteration : 270 Test Loss : 0.3845854455670208 test Accuracy 0.8448\n",
      "Train Iteration : 271 Training Loss : 0.3300271734363906 Training Accuracy 0.8658166666666667\n",
      "Test Iteration : 271 Test Loss : 0.38357739698347715 test Accuracy 0.8448\n",
      "Train Iteration : 272 Training Loss : 0.3294536773704172 Training Accuracy 0.8665833333333334\n",
      "Test Iteration : 272 Test Loss : 0.38345919641731946 test Accuracy 0.8455\n",
      "Train Iteration : 273 Training Loss : 0.32922550320794 Training Accuracy 0.8661833333333333\n",
      "Test Iteration : 273 Test Loss : 0.38295871328279035 test Accuracy 0.8448\n",
      "Train Iteration : 274 Training Loss : 0.3293103243488142 Training Accuracy 0.8672\n",
      "Test Iteration : 274 Test Loss : 0.38385695147925125 test Accuracy 0.8455\n",
      "Train Iteration : 275 Training Loss : 0.32927138864928374 Training Accuracy 0.8659333333333333\n",
      "Test Iteration : 275 Test Loss : 0.38319688013295267 test Accuracy 0.844\n",
      "Train Iteration : 276 Training Loss : 0.32956329116116495 Training Accuracy 0.8672333333333333\n",
      "Test Iteration : 276 Test Loss : 0.3846947505107905 test Accuracy 0.8455\n",
      "Train Iteration : 277 Training Loss : 0.32933080174880225 Training Accuracy 0.8657333333333334\n",
      "Test Iteration : 277 Test Loss : 0.3834394702882225 test Accuracy 0.8434\n",
      "Train Iteration : 278 Training Loss : 0.3297909844123321 Training Accuracy 0.8680666666666667\n",
      "Test Iteration : 278 Test Loss : 0.385424918677821 test Accuracy 0.8464\n",
      "Train Iteration : 279 Training Loss : 0.3299114605687885 Training Accuracy 0.8641\n",
      "Test Iteration : 279 Test Loss : 0.3841823837951722 test Accuracy 0.8423\n",
      "Train Iteration : 280 Training Loss : 0.3303014849020589 Training Accuracy 0.8673333333333333\n",
      "Test Iteration : 280 Test Loss : 0.38629466627733017 test Accuracy 0.8474\n",
      "Train Iteration : 281 Training Loss : 0.32968624018579407 Training Accuracy 0.8641666666666666\n",
      "Test Iteration : 281 Test Loss : 0.38429270573745783 test Accuracy 0.8421\n",
      "Train Iteration : 282 Training Loss : 0.32791242141101845 Training Accuracy 0.86795\n",
      "Test Iteration : 282 Test Loss : 0.3837892118923482 test Accuracy 0.8464\n",
      "Train Iteration : 283 Training Loss : 0.3252273374756849 Training Accuracy 0.8679833333333333\n",
      "Test Iteration : 283 Test Loss : 0.38049919906005353 test Accuracy 0.8455\n",
      "Train Iteration : 284 Training Loss : 0.3237767415028059 Training Accuracy 0.8692166666666666\n",
      "Test Iteration : 284 Test Loss : 0.3792710514895187 test Accuracy 0.8468\n",
      "Train Iteration : 285 Training Loss : 0.3241190895821197 Training Accuracy 0.8693333333333333\n",
      "Test Iteration : 285 Test Loss : 0.38026393088107174 test Accuracy 0.8473\n",
      "Train Iteration : 286 Training Loss : 0.32508571277933157 Training Accuracy 0.8671\n",
      "Test Iteration : 286 Test Loss : 0.3806358514652952 test Accuracy 0.8443\n",
      "Train Iteration : 287 Training Loss : 0.3251244794024218 Training Accuracy 0.8693833333333333\n",
      "Test Iteration : 287 Test Loss : 0.38181333224760994 test Accuracy 0.8473\n",
      "Train Iteration : 288 Training Loss : 0.32361983403768485 Training Accuracy 0.8680666666666667\n",
      "Test Iteration : 288 Test Loss : 0.37952384633037284 test Accuracy 0.8444\n",
      "Train Iteration : 289 Training Loss : 0.32180237655539307 Training Accuracy 0.8699\n",
      "Test Iteration : 289 Test Loss : 0.37839432278620805 test Accuracy 0.8471\n",
      "Train Iteration : 290 Training Loss : 0.32098650928074407 Training Accuracy 0.8702333333333333\n",
      "Test Iteration : 290 Test Loss : 0.3775058267503519 test Accuracy 0.8477\n",
      "Train Iteration : 291 Training Loss : 0.3212699864626724 Training Accuracy 0.8696666666666667\n",
      "Test Iteration : 291 Test Loss : 0.37766969516289567 test Accuracy 0.8467\n",
      "Train Iteration : 292 Training Loss : 0.32161110898973927 Training Accuracy 0.871\n",
      "Test Iteration : 292 Test Loss : 0.378816203276116 test Accuracy 0.8484\n",
      "Train Iteration : 293 Training Loss : 0.32108688100239136 Training Accuracy 0.8694833333333334\n",
      "Test Iteration : 293 Test Loss : 0.37769952229151416 test Accuracy 0.846\n",
      "Train Iteration : 294 Training Loss : 0.31996972680041696 Training Accuracy 0.87125\n",
      "Test Iteration : 294 Test Loss : 0.3774080535467256 test Accuracy 0.8488\n",
      "Train Iteration : 295 Training Loss : 0.3190366353676544 Training Accuracy 0.8711666666666666\n",
      "Test Iteration : 295 Test Loss : 0.37618874148427683 test Accuracy 0.8479\n",
      "Train Iteration : 296 Training Loss : 0.3187591459155524 Training Accuracy 0.8711833333333333\n",
      "Test Iteration : 296 Test Loss : 0.37625604705998483 test Accuracy 0.8471\n",
      "Train Iteration : 297 Training Loss : 0.31880082227239137 Training Accuracy 0.8715\n",
      "Test Iteration : 297 Test Loss : 0.3765287725688272 test Accuracy 0.8485\n",
      "Train Iteration : 298 Training Loss : 0.3184803272803966 Training Accuracy 0.87075\n",
      "Test Iteration : 298 Test Loss : 0.3761566896790987 test Accuracy 0.847\n",
      "Train Iteration : 299 Training Loss : 0.31778207498838723 Training Accuracy 0.87195\n",
      "Test Iteration : 299 Test Loss : 0.3758129131777774 test Accuracy 0.8481\n",
      "Train Iteration : 300 Training Loss : 0.3169001090121525 Training Accuracy 0.87185\n",
      "Test Iteration : 300 Test Loss : 0.37486318938100194 test Accuracy 0.8476\n",
      "Train Iteration : 301 Training Loss : 0.31631829970003256 Training Accuracy 0.8720666666666667\n",
      "Test Iteration : 301 Test Loss : 0.3745339622978063 test Accuracy 0.8488\n",
      "Train Iteration : 302 Training Loss : 0.31610984207879517 Training Accuracy 0.8725\n",
      "Test Iteration : 302 Test Loss : 0.37447145680302746 test Accuracy 0.848\n",
      "Train Iteration : 303 Training Loss : 0.31598985769684296 Training Accuracy 0.8720833333333333\n",
      "Test Iteration : 303 Test Loss : 0.3744839162478117 test Accuracy 0.8488\n",
      "Train Iteration : 304 Training Loss : 0.3157201971206669 Training Accuracy 0.8729833333333333\n",
      "Test Iteration : 304 Test Loss : 0.37445698310177483 test Accuracy 0.8486\n",
      "Train Iteration : 305 Training Loss : 0.3151468128473842 Training Accuracy 0.8724333333333333\n",
      "Test Iteration : 305 Test Loss : 0.3740105630259322 test Accuracy 0.8494\n",
      "Train Iteration : 306 Training Loss : 0.31453567206400984 Training Accuracy 0.8730666666666667\n",
      "Test Iteration : 306 Test Loss : 0.3735321623431489 test Accuracy 0.8479\n",
      "Train Iteration : 307 Training Loss : 0.31404445768936035 Training Accuracy 0.87345\n",
      "Test Iteration : 307 Test Loss : 0.37335871954531374 test Accuracy 0.8493\n",
      "Train Iteration : 308 Training Loss : 0.3137440230678515 Training Accuracy 0.8730333333333333\n",
      "Test Iteration : 308 Test Loss : 0.37291268196430466 test Accuracy 0.8476\n",
      "Train Iteration : 309 Training Loss : 0.3136164347540975 Training Accuracy 0.8735666666666667\n",
      "Test Iteration : 309 Test Loss : 0.37342483962615197 test Accuracy 0.8499\n",
      "Train Iteration : 310 Training Loss : 0.3134902066091661 Training Accuracy 0.8727833333333334\n",
      "Test Iteration : 310 Test Loss : 0.37285773518868054 test Accuracy 0.8483\n",
      "Train Iteration : 311 Training Loss : 0.31340808940868486 Training Accuracy 0.8740166666666667\n",
      "Test Iteration : 311 Test Loss : 0.37367097935516025 test Accuracy 0.8493\n",
      "Train Iteration : 312 Training Loss : 0.3132338592704697 Training Accuracy 0.87315\n",
      "Test Iteration : 312 Test Loss : 0.37284367119686657 test Accuracy 0.8487\n",
      "Train Iteration : 313 Training Loss : 0.3134438154878179 Training Accuracy 0.87405\n",
      "Test Iteration : 313 Test Loss : 0.374119612453566 test Accuracy 0.8498\n",
      "Train Iteration : 314 Training Loss : 0.3135026491728853 Training Accuracy 0.8727833333333334\n",
      "Test Iteration : 314 Test Loss : 0.37335690058911936 test Accuracy 0.848\n",
      "Train Iteration : 315 Training Loss : 0.3145486147830826 Training Accuracy 0.8735\n",
      "Test Iteration : 315 Test Loss : 0.3756946538890599 test Accuracy 0.8508\n",
      "Train Iteration : 316 Training Loss : 0.3146209740328348 Training Accuracy 0.8720166666666667\n",
      "Test Iteration : 316 Test Loss : 0.37472511639989414 test Accuracy 0.8488\n",
      "Train Iteration : 317 Training Loss : 0.3153429505912664 Training Accuracy 0.8733\n",
      "Test Iteration : 317 Test Loss : 0.3768369859195072 test Accuracy 0.8515\n",
      "Train Iteration : 318 Training Loss : 0.3134715695423167 Training Accuracy 0.8726833333333334\n",
      "Test Iteration : 318 Test Loss : 0.37389191832257396 test Accuracy 0.8497\n",
      "Train Iteration : 319 Training Loss : 0.3114643433617657 Training Accuracy 0.8745\n",
      "Test Iteration : 319 Test Loss : 0.3728824474837105 test Accuracy 0.8508\n",
      "Train Iteration : 320 Training Loss : 0.3092570972987561 Training Accuracy 0.8749\n",
      "Test Iteration : 320 Test Loss : 0.37023607505169837 test Accuracy 0.8509\n",
      "Train Iteration : 321 Training Loss : 0.30843359301584483 Training Accuracy 0.8751666666666666\n",
      "Test Iteration : 321 Test Loss : 0.3697324385428159 test Accuracy 0.8491\n",
      "Train Iteration : 322 Training Loss : 0.30889855763850055 Training Accuracy 0.8754166666666666\n",
      "Test Iteration : 322 Test Loss : 0.37066263720409914 test Accuracy 0.8513\n",
      "Train Iteration : 323 Training Loss : 0.30971063772982266 Training Accuracy 0.8747166666666667\n",
      "Test Iteration : 323 Test Loss : 0.3711892071457922 test Accuracy 0.8522\n",
      "Train Iteration : 324 Training Loss : 0.3103224715563666 Training Accuracy 0.8751\n",
      "Test Iteration : 324 Test Loss : 0.3727545677205459 test Accuracy 0.8522\n",
      "Train Iteration : 325 Training Loss : 0.3094186979047393 Training Accuracy 0.8742666666666666\n",
      "Test Iteration : 325 Test Loss : 0.37111492589405115 test Accuracy 0.8511\n",
      "Train Iteration : 326 Training Loss : 0.3082738560987563 Training Accuracy 0.8759166666666667\n",
      "Test Iteration : 326 Test Loss : 0.3709479932161853 test Accuracy 0.8517\n",
      "Train Iteration : 327 Training Loss : 0.30679871602076203 Training Accuracy 0.8755833333333334\n",
      "Test Iteration : 327 Test Loss : 0.36884990390128825 test Accuracy 0.8497\n",
      "Train Iteration : 328 Training Loss : 0.30609170658117524 Training Accuracy 0.8765666666666667\n",
      "Test Iteration : 328 Test Loss : 0.3687101723151772 test Accuracy 0.8507\n",
      "Train Iteration : 329 Training Loss : 0.30603431957707 Training Accuracy 0.8758833333333333\n",
      "Test Iteration : 329 Test Loss : 0.36872002957655614 test Accuracy 0.8512\n",
      "Train Iteration : 330 Training Loss : 0.3062085226222994 Training Accuracy 0.87605\n",
      "Test Iteration : 330 Test Loss : 0.3688932659121299 test Accuracy 0.8515\n",
      "Train Iteration : 331 Training Loss : 0.30603813222294646 Training Accuracy 0.8763833333333333\n",
      "Test Iteration : 331 Test Loss : 0.36927370974706897 test Accuracy 0.8517\n",
      "Train Iteration : 332 Training Loss : 0.30546010816937136 Training Accuracy 0.8763\n",
      "Test Iteration : 332 Test Loss : 0.3684018286644794 test Accuracy 0.8519\n",
      "Train Iteration : 333 Training Loss : 0.3047192986986196 Training Accuracy 0.8768166666666667\n",
      "Test Iteration : 333 Test Loss : 0.3682101198944615 test Accuracy 0.8522\n",
      "Train Iteration : 334 Training Loss : 0.3039983544098236 Training Accuracy 0.8769333333333333\n",
      "Test Iteration : 334 Test Loss : 0.36723680783556983 test Accuracy 0.8504\n",
      "Train Iteration : 335 Training Loss : 0.3036067497978127 Training Accuracy 0.8775833333333334\n",
      "Test Iteration : 335 Test Loss : 0.3673195938967329 test Accuracy 0.8515\n",
      "Train Iteration : 336 Training Loss : 0.3034765159841905 Training Accuracy 0.8766333333333334\n",
      "Test Iteration : 336 Test Loss : 0.3670788748994588 test Accuracy 0.8513\n",
      "Train Iteration : 337 Training Loss : 0.30344103925884036 Training Accuracy 0.87765\n",
      "Test Iteration : 337 Test Loss : 0.3674232738701827 test Accuracy 0.8526\n",
      "Train Iteration : 338 Training Loss : 0.30320776544727374 Training Accuracy 0.8769166666666667\n",
      "Test Iteration : 338 Test Loss : 0.3672226864038399 test Accuracy 0.852\n",
      "Train Iteration : 339 Training Loss : 0.30284508212406147 Training Accuracy 0.8777\n",
      "Test Iteration : 339 Test Loss : 0.3671366533925023 test Accuracy 0.8521\n",
      "Train Iteration : 340 Training Loss : 0.30223016330359825 Training Accuracy 0.8775166666666666\n",
      "Test Iteration : 340 Test Loss : 0.36662516614606 test Accuracy 0.8519\n",
      "Train Iteration : 341 Training Loss : 0.30162658815919313 Training Accuracy 0.8784833333333333\n",
      "Test Iteration : 341 Test Loss : 0.36627445480017085 test Accuracy 0.8526\n",
      "Train Iteration : 342 Training Loss : 0.30108699732857336 Training Accuracy 0.87805\n",
      "Test Iteration : 342 Test Loss : 0.36573884751594443 test Accuracy 0.8514\n",
      "Train Iteration : 343 Training Loss : 0.3006990385772215 Training Accuracy 0.8789\n",
      "Test Iteration : 343 Test Loss : 0.36576973159932236 test Accuracy 0.8531\n",
      "Train Iteration : 344 Training Loss : 0.30044274237352947 Training Accuracy 0.8785833333333334\n",
      "Test Iteration : 344 Test Loss : 0.36533356118770066 test Accuracy 0.8516\n",
      "Train Iteration : 345 Training Loss : 0.300291899410358 Training Accuracy 0.8791833333333333\n",
      "Test Iteration : 345 Test Loss : 0.3657624652023606 test Accuracy 0.8536\n",
      "Train Iteration : 346 Training Loss : 0.30011622433111834 Training Accuracy 0.8783833333333333\n",
      "Test Iteration : 346 Test Loss : 0.3652597861255812 test Accuracy 0.8532\n",
      "Train Iteration : 347 Training Loss : 0.3000041305073497 Training Accuracy 0.8790166666666667\n",
      "Test Iteration : 347 Test Loss : 0.36583722176942596 test Accuracy 0.8544\n",
      "Train Iteration : 348 Training Loss : 0.29975774726731674 Training Accuracy 0.8784833333333333\n",
      "Test Iteration : 348 Test Loss : 0.3651603134396629 test Accuracy 0.8532\n",
      "Train Iteration : 349 Training Loss : 0.29969017632384154 Training Accuracy 0.8791\n",
      "Test Iteration : 349 Test Loss : 0.3659233212766269 test Accuracy 0.855\n",
      "Train Iteration : 350 Training Loss : 0.2995048193529718 Training Accuracy 0.87835\n",
      "Test Iteration : 350 Test Loss : 0.3651402296858855 test Accuracy 0.8527\n",
      "Train Iteration : 351 Training Loss : 0.29978779177423104 Training Accuracy 0.8795833333333334\n",
      "Test Iteration : 351 Test Loss : 0.36649240858018295 test Accuracy 0.8548\n",
      "Train Iteration : 352 Training Loss : 0.3000846913680731 Training Accuracy 0.87795\n",
      "Test Iteration : 352 Test Loss : 0.36598111823232277 test Accuracy 0.8516\n",
      "Train Iteration : 353 Training Loss : 0.30125018956654787 Training Accuracy 0.87895\n",
      "Test Iteration : 353 Test Loss : 0.3685098828814766 test Accuracy 0.8537\n",
      "Train Iteration : 354 Training Loss : 0.30241535888746607 Training Accuracy 0.8758833333333333\n",
      "Test Iteration : 354 Test Loss : 0.368596275008138 test Accuracy 0.849\n",
      "Train Iteration : 355 Training Loss : 0.30403614859730177 Training Accuracy 0.8784333333333333\n",
      "Test Iteration : 355 Test Loss : 0.3718475455430239 test Accuracy 0.8543\n",
      "Train Iteration : 356 Training Loss : 0.30391474904687393 Training Accuracy 0.87505\n",
      "Test Iteration : 356 Test Loss : 0.37044114151571395 test Accuracy 0.8479\n",
      "Train Iteration : 357 Training Loss : 0.30183435921977086 Training Accuracy 0.8793333333333333\n",
      "Test Iteration : 357 Test Loss : 0.3697127938201746 test Accuracy 0.8535\n",
      "Train Iteration : 358 Training Loss : 0.2979597217773131 Training Accuracy 0.8788333333333334\n",
      "Test Iteration : 358 Test Loss : 0.36499224423374427 test Accuracy 0.8527\n",
      "Train Iteration : 359 Training Loss : 0.29543184766508496 Training Accuracy 0.8811666666666667\n",
      "Test Iteration : 359 Test Loss : 0.3628857993362996 test Accuracy 0.8557\n",
      "Train Iteration : 360 Training Loss : 0.29578551142798565 Training Accuracy 0.8814666666666666\n",
      "Test Iteration : 360 Test Loss : 0.36372461479757023 test Accuracy 0.8562\n",
      "Train Iteration : 361 Training Loss : 0.2975754303428995 Training Accuracy 0.8789333333333333\n",
      "Test Iteration : 361 Test Loss : 0.3650991047223545 test Accuracy 0.8518\n",
      "Train Iteration : 362 Training Loss : 0.29835269277611903 Training Accuracy 0.88055\n",
      "Test Iteration : 362 Test Loss : 0.36702462264866165 test Accuracy 0.8549\n",
      "Train Iteration : 363 Training Loss : 0.2966377223196573 Training Accuracy 0.8793333333333333\n",
      "Test Iteration : 363 Test Loss : 0.36452056195978555 test Accuracy 0.8521\n",
      "Train Iteration : 364 Training Loss : 0.2943397441213744 Training Accuracy 0.8819\n",
      "Test Iteration : 364 Test Loss : 0.36288572032387084 test Accuracy 0.8565\n",
      "Train Iteration : 365 Training Loss : 0.29343301862839727 Training Accuracy 0.8816833333333334\n",
      "Test Iteration : 365 Test Loss : 0.36189152395562285 test Accuracy 0.8553\n",
      "Train Iteration : 366 Training Loss : 0.29415204534588385 Training Accuracy 0.8804333333333333\n",
      "Test Iteration : 366 Test Loss : 0.36259182655987277 test Accuracy 0.8533\n",
      "Train Iteration : 367 Training Loss : 0.2949120820394802 Training Accuracy 0.8818333333333334\n",
      "Test Iteration : 367 Test Loss : 0.3640706016231371 test Accuracy 0.8569\n",
      "Train Iteration : 368 Training Loss : 0.29433410387338416 Training Accuracy 0.8803166666666666\n",
      "Test Iteration : 368 Test Loss : 0.3630580697787859 test Accuracy 0.8527\n",
      "Train Iteration : 369 Training Loss : 0.29295358659435233 Training Accuracy 0.8822666666666666\n",
      "Test Iteration : 369 Test Loss : 0.3623814598522061 test Accuracy 0.8576\n",
      "Train Iteration : 370 Training Loss : 0.2919645450922167 Training Accuracy 0.8818333333333334\n",
      "Test Iteration : 370 Test Loss : 0.36120288225712804 test Accuracy 0.8554\n",
      "Train Iteration : 371 Training Loss : 0.2920095154130877 Training Accuracy 0.8816833333333334\n",
      "Test Iteration : 371 Test Loss : 0.36152882872333325 test Accuracy 0.8543\n",
      "Train Iteration : 372 Training Loss : 0.2925154048143288 Training Accuracy 0.8818666666666667\n",
      "Test Iteration : 372 Test Loss : 0.3622914927671488 test Accuracy 0.8566\n",
      "Train Iteration : 373 Training Loss : 0.29241802228726343 Training Accuracy 0.8810333333333333\n",
      "Test Iteration : 373 Test Loss : 0.3621655692733887 test Accuracy 0.8534\n",
      "Train Iteration : 374 Training Loss : 0.29168219746264357 Training Accuracy 0.8821666666666667\n",
      "Test Iteration : 374 Test Loss : 0.3616913500758589 test Accuracy 0.8571\n",
      "Train Iteration : 375 Training Loss : 0.29064245961045215 Training Accuracy 0.8825833333333334\n",
      "Test Iteration : 375 Test Loss : 0.36074847393178544 test Accuracy 0.8545\n",
      "Train Iteration : 376 Training Loss : 0.2901114445786934 Training Accuracy 0.8827333333333334\n",
      "Test Iteration : 376 Test Loss : 0.3602609374763734 test Accuracy 0.8555\n",
      "Train Iteration : 377 Training Loss : 0.2901405009416574 Training Accuracy 0.8828666666666667\n",
      "Test Iteration : 377 Test Loss : 0.36079474978677994 test Accuracy 0.858\n",
      "Train Iteration : 378 Training Loss : 0.29023869997987 Training Accuracy 0.8823666666666666\n",
      "Test Iteration : 378 Test Loss : 0.3607019551471064 test Accuracy 0.8545\n",
      "Train Iteration : 379 Training Loss : 0.29001334729602396 Training Accuracy 0.8833666666666666\n",
      "Test Iteration : 379 Test Loss : 0.3611296403498808 test Accuracy 0.858\n",
      "Train Iteration : 380 Training Loss : 0.2893767423852918 Training Accuracy 0.8826833333333334\n",
      "Test Iteration : 380 Test Loss : 0.360230747836092 test Accuracy 0.8552\n",
      "Train Iteration : 381 Training Loss : 0.2887134642906003 Training Accuracy 0.8835333333333333\n",
      "Test Iteration : 381 Test Loss : 0.360058633914972 test Accuracy 0.8578\n",
      "Train Iteration : 382 Training Loss : 0.28830879824082806 Training Accuracy 0.8834333333333333\n",
      "Test Iteration : 382 Test Loss : 0.3595748673699907 test Accuracy 0.8563\n",
      "Train Iteration : 383 Training Loss : 0.28822499526959305 Training Accuracy 0.8837\n",
      "Test Iteration : 383 Test Loss : 0.3597628412847515 test Accuracy 0.855\n",
      "Train Iteration : 384 Training Loss : 0.2882825231535515 Training Accuracy 0.8836833333333334\n",
      "Test Iteration : 384 Test Loss : 0.3599425249579444 test Accuracy 0.8576\n",
      "Train Iteration : 385 Training Loss : 0.28818002143956595 Training Accuracy 0.8834666666666666\n",
      "Test Iteration : 385 Test Loss : 0.3601055250803445 test Accuracy 0.8549\n",
      "Train Iteration : 386 Training Loss : 0.28805444445292916 Training Accuracy 0.8835833333333334\n",
      "Test Iteration : 386 Test Loss : 0.36001338491755625 test Accuracy 0.8582\n",
      "Train Iteration : 387 Training Loss : 0.2878796198890055 Training Accuracy 0.8833\n",
      "Test Iteration : 387 Test Loss : 0.36035821956728126 test Accuracy 0.8557\n",
      "Train Iteration : 388 Training Loss : 0.28810813720011985 Training Accuracy 0.8835\n",
      "Test Iteration : 388 Test Loss : 0.36023479154856763 test Accuracy 0.8569\n",
      "Train Iteration : 389 Training Loss : 0.28906910585558354 Training Accuracy 0.88375\n",
      "Test Iteration : 389 Test Loss : 0.3622218288446847 test Accuracy 0.8572\n",
      "Train Iteration : 390 Training Loss : 0.290196903692097 Training Accuracy 0.8818666666666667\n",
      "Test Iteration : 390 Test Loss : 0.36244122720144667 test Accuracy 0.8553\n",
      "Train Iteration : 391 Training Loss : 0.293055453208275 Training Accuracy 0.8823\n",
      "Test Iteration : 391 Test Loss : 0.3669266897624402 test Accuracy 0.856\n",
      "Train Iteration : 392 Training Loss : 0.29266940180347556 Training Accuracy 0.8810333333333333\n",
      "Test Iteration : 392 Test Loss : 0.36505441012762163 test Accuracy 0.8552\n",
      "Train Iteration : 393 Training Loss : 0.2931123048971467 Training Accuracy 0.8821833333333333\n",
      "Test Iteration : 393 Test Loss : 0.3672392915149445 test Accuracy 0.8561\n",
      "Train Iteration : 394 Training Loss : 0.2885679650227502 Training Accuracy 0.8826333333333334\n",
      "Test Iteration : 394 Test Loss : 0.3613577158992488 test Accuracy 0.8559\n",
      "Train Iteration : 395 Training Loss : 0.28534774490044484 Training Accuracy 0.8846333333333334\n",
      "Test Iteration : 395 Test Loss : 0.3589724396477275 test Accuracy 0.8566\n",
      "Train Iteration : 396 Training Loss : 0.2843807869046252 Training Accuracy 0.8852333333333333\n",
      "Test Iteration : 396 Test Loss : 0.35815381176167704 test Accuracy 0.8591\n",
      "Train Iteration : 397 Training Loss : 0.2856332754908464 Training Accuracy 0.8842\n",
      "Test Iteration : 397 Test Loss : 0.35919067533783844 test Accuracy 0.8568\n",
      "Train Iteration : 398 Training Loss : 0.28772841718471565 Training Accuracy 0.8846666666666667\n",
      "Test Iteration : 398 Test Loss : 0.3625287702258003 test Accuracy 0.8568\n",
      "Train Iteration : 399 Training Loss : 0.28720052403926827 Training Accuracy 0.88325\n",
      "Test Iteration : 399 Test Loss : 0.3610519292722771 test Accuracy 0.8563\n",
      "Train Iteration : 400 Training Loss : 0.28589747480232547 Training Accuracy 0.8845333333333333\n",
      "Test Iteration : 400 Test Loss : 0.36085102511825456 test Accuracy 0.8578\n",
      "Train Iteration : 401 Training Loss : 0.2837556277723918 Training Accuracy 0.8852833333333333\n",
      "Test Iteration : 401 Test Loss : 0.35811284971101986 test Accuracy 0.8582\n",
      "Train Iteration : 402 Training Loss : 0.2828159278712291 Training Accuracy 0.8857333333333334\n",
      "Test Iteration : 402 Test Loss : 0.35741631803338825 test Accuracy 0.8565\n",
      "Train Iteration : 403 Training Loss : 0.28324491668751073 Training Accuracy 0.8858166666666667\n",
      "Test Iteration : 403 Test Loss : 0.3583732154016857 test Accuracy 0.8598\n",
      "Train Iteration : 404 Training Loss : 0.283847868361379 Training Accuracy 0.8844166666666666\n",
      "Test Iteration : 404 Test Loss : 0.3584900169737368 test Accuracy 0.8572\n",
      "Train Iteration : 405 Training Loss : 0.2841234441533994 Training Accuracy 0.88555\n",
      "Test Iteration : 405 Test Loss : 0.35984382390115427 test Accuracy 0.8576\n",
      "Train Iteration : 406 Training Loss : 0.2829791680247968 Training Accuracy 0.88545\n",
      "Test Iteration : 406 Test Loss : 0.3580295475171751 test Accuracy 0.8574\n",
      "Train Iteration : 407 Training Loss : 0.28179834675653875 Training Accuracy 0.8859\n",
      "Test Iteration : 407 Test Loss : 0.35748782262648826 test Accuracy 0.8575\n",
      "Train Iteration : 408 Training Loss : 0.2811535760787647 Training Accuracy 0.8865\n",
      "Test Iteration : 408 Test Loss : 0.3568351705144598 test Accuracy 0.8597\n",
      "Train Iteration : 409 Training Loss : 0.2812068971559058 Training Accuracy 0.88605\n",
      "Test Iteration : 409 Test Loss : 0.3567941214184695 test Accuracy 0.8574\n",
      "Train Iteration : 410 Training Loss : 0.281561137169332 Training Accuracy 0.8864166666666666\n",
      "Test Iteration : 410 Test Loss : 0.35786485146106 test Accuracy 0.86\n",
      "Train Iteration : 411 Training Loss : 0.28140164951635394 Training Accuracy 0.8858333333333334\n",
      "Test Iteration : 411 Test Loss : 0.3572179467474859 test Accuracy 0.8582\n",
      "Train Iteration : 412 Training Loss : 0.28090382645242024 Training Accuracy 0.8868333333333334\n",
      "Test Iteration : 412 Test Loss : 0.35751008110123994 test Accuracy 0.859\n",
      "Train Iteration : 413 Training Loss : 0.280045293523267 Training Accuracy 0.8871166666666667\n",
      "Test Iteration : 413 Test Loss : 0.3563900567907404 test Accuracy 0.859\n",
      "Train Iteration : 414 Training Loss : 0.27944071853482727 Training Accuracy 0.8874166666666666\n",
      "Test Iteration : 414 Test Loss : 0.35616741222072346 test Accuracy 0.8584\n",
      "Train Iteration : 415 Training Loss : 0.279238621441234 Training Accuracy 0.8872\n",
      "Test Iteration : 415 Test Loss : 0.35618673055234457 test Accuracy 0.86\n",
      "Train Iteration : 416 Training Loss : 0.2792868350498251 Training Accuracy 0.8868333333333334\n",
      "Test Iteration : 416 Test Loss : 0.3561421072336578 test Accuracy 0.8586\n",
      "Train Iteration : 417 Training Loss : 0.2793445952384685 Training Accuracy 0.8871833333333333\n",
      "Test Iteration : 417 Test Loss : 0.35680221955671104 test Accuracy 0.8601\n",
      "Train Iteration : 418 Training Loss : 0.2790837592152174 Training Accuracy 0.88705\n",
      "Test Iteration : 418 Test Loss : 0.35616592521515866 test Accuracy 0.8586\n",
      "Train Iteration : 419 Training Loss : 0.27867587708448693 Training Accuracy 0.8875666666666666\n",
      "Test Iteration : 419 Test Loss : 0.3564001088511006 test Accuracy 0.8611\n",
      "Train Iteration : 420 Training Loss : 0.27809874441551996 Training Accuracy 0.8874333333333333\n",
      "Test Iteration : 420 Test Loss : 0.35552498927530807 test Accuracy 0.8595\n",
      "Train Iteration : 421 Training Loss : 0.2776385713023551 Training Accuracy 0.88815\n",
      "Test Iteration : 421 Test Loss : 0.35551424661908143 test Accuracy 0.8599\n",
      "Train Iteration : 422 Training Loss : 0.2773628373505017 Training Accuracy 0.8884666666666666\n",
      "Test Iteration : 422 Test Loss : 0.35529346937263345 test Accuracy 0.8596\n",
      "Train Iteration : 423 Training Loss : 0.27731361997165144 Training Accuracy 0.8875833333333333\n",
      "Test Iteration : 423 Test Loss : 0.35536532202575905 test Accuracy 0.8602\n",
      "Train Iteration : 424 Training Loss : 0.27736932923855384 Training Accuracy 0.8885\n",
      "Test Iteration : 424 Test Loss : 0.3558308678974386 test Accuracy 0.8605\n",
      "Train Iteration : 425 Training Loss : 0.27744017999423093 Training Accuracy 0.88715\n",
      "Test Iteration : 425 Test Loss : 0.35571611263053304 test Accuracy 0.86\n",
      "Train Iteration : 426 Training Loss : 0.2774290371366256 Training Accuracy 0.8885166666666666\n",
      "Test Iteration : 426 Test Loss : 0.35627429576308534 test Accuracy 0.8607\n",
      "Train Iteration : 427 Training Loss : 0.27723241322763004 Training Accuracy 0.8872666666666666\n",
      "Test Iteration : 427 Test Loss : 0.3558489227207058 test Accuracy 0.8601\n",
      "Train Iteration : 428 Training Loss : 0.27671789931405943 Training Accuracy 0.8888833333333334\n",
      "Test Iteration : 428 Test Loss : 0.3558124134032312 test Accuracy 0.8615\n",
      "Train Iteration : 429 Training Loss : 0.2760546912208311 Training Accuracy 0.8878333333333334\n",
      "Test Iteration : 429 Test Loss : 0.3550940669655861 test Accuracy 0.8607\n",
      "Train Iteration : 430 Training Loss : 0.2753423889143454 Training Accuracy 0.8893666666666666\n",
      "Test Iteration : 430 Test Loss : 0.3546569779654401 test Accuracy 0.8602\n",
      "Train Iteration : 431 Training Loss : 0.27488994720187865 Training Accuracy 0.8894833333333333\n",
      "Test Iteration : 431 Test Loss : 0.3543642080690518 test Accuracy 0.8611\n",
      "Train Iteration : 432 Training Loss : 0.27471125471938734 Training Accuracy 0.8889333333333334\n",
      "Test Iteration : 432 Test Loss : 0.35425930028128266 test Accuracy 0.8605\n",
      "Train Iteration : 433 Training Loss : 0.2747072012304887 Training Accuracy 0.8893\n",
      "Test Iteration : 433 Test Loss : 0.3545588252890587 test Accuracy 0.8614\n",
      "Train Iteration : 434 Training Loss : 0.27471438230833656 Training Accuracy 0.88805\n",
      "Test Iteration : 434 Test Loss : 0.35449371443147126 test Accuracy 0.8601\n",
      "Train Iteration : 435 Training Loss : 0.2745791017867553 Training Accuracy 0.88975\n",
      "Test Iteration : 435 Test Loss : 0.35478048603765255 test Accuracy 0.861\n",
      "Train Iteration : 436 Training Loss : 0.2742729509712887 Training Accuracy 0.8883166666666666\n",
      "Test Iteration : 436 Test Loss : 0.35432718519564643 test Accuracy 0.8599\n",
      "Train Iteration : 437 Training Loss : 0.2738989146815764 Training Accuracy 0.8901333333333333\n",
      "Test Iteration : 437 Test Loss : 0.35440806184656864 test Accuracy 0.8613\n",
      "Train Iteration : 438 Training Loss : 0.27354228215685944 Training Accuracy 0.88885\n",
      "Test Iteration : 438 Test Loss : 0.35389849170473364 test Accuracy 0.8593\n",
      "Train Iteration : 439 Training Loss : 0.27339791441249633 Training Accuracy 0.8899833333333333\n",
      "Test Iteration : 439 Test Loss : 0.35428010812389504 test Accuracy 0.8616\n",
      "Train Iteration : 440 Training Loss : 0.27346327813707544 Training Accuracy 0.8889666666666667\n",
      "Test Iteration : 440 Test Loss : 0.3541608724160719 test Accuracy 0.8585\n",
      "Train Iteration : 441 Training Loss : 0.2738247137476811 Training Accuracy 0.88935\n",
      "Test Iteration : 441 Test Loss : 0.355077460607483 test Accuracy 0.861\n",
      "Train Iteration : 442 Training Loss : 0.2743362158785597 Training Accuracy 0.8887666666666667\n",
      "Test Iteration : 442 Test Loss : 0.35538345343176153 test Accuracy 0.8585\n",
      "Train Iteration : 443 Training Loss : 0.2753036779610136 Training Accuracy 0.8887333333333334\n",
      "Test Iteration : 443 Test Loss : 0.35697292118969115 test Accuracy 0.8616\n",
      "Train Iteration : 444 Training Loss : 0.27648965501937034 Training Accuracy 0.8871833333333333\n",
      "Test Iteration : 444 Test Loss : 0.3579027187450263 test Accuracy 0.8581\n",
      "Train Iteration : 445 Training Loss : 0.27849586371917656 Training Accuracy 0.888\n",
      "Test Iteration : 445 Test Loss : 0.36059033247575006 test Accuracy 0.8603\n",
      "Train Iteration : 446 Training Loss : 0.28008281553098746 Training Accuracy 0.8843166666666666\n",
      "Test Iteration : 446 Test Loss : 0.3619467346617998 test Accuracy 0.8554\n",
      "Train Iteration : 447 Training Loss : 0.2813835751646375 Training Accuracy 0.8873833333333333\n",
      "Test Iteration : 447 Test Loss : 0.36378066172073914 test Accuracy 0.86\n",
      "Train Iteration : 448 Training Loss : 0.2779690214751251 Training Accuracy 0.8858666666666667\n",
      "Test Iteration : 448 Test Loss : 0.36019589623699283 test Accuracy 0.8571\n",
      "Train Iteration : 449 Training Loss : 0.2738187260981346 Training Accuracy 0.8902333333333333\n",
      "Test Iteration : 449 Test Loss : 0.35613897049721044 test Accuracy 0.8615\n",
      "Train Iteration : 450 Training Loss : 0.2710389404779278 Training Accuracy 0.8907\n",
      "Test Iteration : 450 Test Loss : 0.35376313956992345 test Accuracy 0.8629\n",
      "Train Iteration : 451 Training Loss : 0.2722851596119365 Training Accuracy 0.88915\n",
      "Test Iteration : 451 Test Loss : 0.3548398621247653 test Accuracy 0.859\n",
      "Train Iteration : 452 Training Loss : 0.27502277187364893 Training Accuracy 0.8901666666666667\n",
      "Test Iteration : 452 Test Loss : 0.3584518363943093 test Accuracy 0.8615\n",
      "Train Iteration : 453 Training Loss : 0.2746396362412101 Training Accuracy 0.88765\n",
      "Test Iteration : 453 Test Loss : 0.35767231090240637 test Accuracy 0.8573\n",
      "Train Iteration : 454 Training Loss : 0.27153301550705833 Training Accuracy 0.89135\n",
      "Test Iteration : 454 Test Loss : 0.35507469015926985 test Accuracy 0.8626\n",
      "Train Iteration : 455 Training Loss : 0.2689991770895485 Training Accuracy 0.89135\n",
      "Test Iteration : 455 Test Loss : 0.35231079876540833 test Accuracy 0.8625\n",
      "Train Iteration : 456 Training Loss : 0.2696885940037753 Training Accuracy 0.8908833333333334\n",
      "Test Iteration : 456 Test Loss : 0.3532223259960152 test Accuracy 0.8603\n",
      "Train Iteration : 457 Training Loss : 0.2714863390176475 Training Accuracy 0.89185\n",
      "Test Iteration : 457 Test Loss : 0.3554008987628056 test Accuracy 0.8625\n",
      "Train Iteration : 458 Training Loss : 0.27099582060423 Training Accuracy 0.8895166666666666\n",
      "Test Iteration : 458 Test Loss : 0.35477250854371967 test Accuracy 0.859\n",
      "Train Iteration : 459 Training Loss : 0.2689661335267464 Training Accuracy 0.89215\n",
      "Test Iteration : 459 Test Loss : 0.35311501703159376 test Accuracy 0.8637\n",
      "Train Iteration : 460 Training Loss : 0.2678036542067604 Training Accuracy 0.89205\n",
      "Test Iteration : 460 Test Loss : 0.35187215200975636 test Accuracy 0.8626\n",
      "Train Iteration : 461 Training Loss : 0.2685048290910124 Training Accuracy 0.8911833333333333\n",
      "Test Iteration : 461 Test Loss : 0.3529597214188163 test Accuracy 0.8608\n",
      "Train Iteration : 462 Training Loss : 0.26951147039653595 Training Accuracy 0.89215\n",
      "Test Iteration : 462 Test Loss : 0.353924299965439 test Accuracy 0.8628\n",
      "Train Iteration : 463 Training Loss : 0.2688395445077736 Training Accuracy 0.8908\n",
      "Test Iteration : 463 Test Loss : 0.3535957093738234 test Accuracy 0.8603\n",
      "Train Iteration : 464 Training Loss : 0.26765905837130005 Training Accuracy 0.89255\n",
      "Test Iteration : 464 Test Loss : 0.3522407250909011 test Accuracy 0.8628\n",
      "Train Iteration : 465 Training Loss : 0.2669441821347727 Training Accuracy 0.8926333333333333\n",
      "Test Iteration : 465 Test Loss : 0.3519155156646862 test Accuracy 0.8634\n",
      "Train Iteration : 466 Training Loss : 0.2671781299646115 Training Accuracy 0.89105\n",
      "Test Iteration : 466 Test Loss : 0.351932564371861 test Accuracy 0.8613\n",
      "Train Iteration : 467 Training Loss : 0.2675213260651644 Training Accuracy 0.89275\n",
      "Test Iteration : 467 Test Loss : 0.3528795085508628 test Accuracy 0.8643\n",
      "Train Iteration : 468 Training Loss : 0.267057584835892 Training Accuracy 0.89125\n",
      "Test Iteration : 468 Test Loss : 0.3522031591933158 test Accuracy 0.8602\n",
      "Train Iteration : 469 Training Loss : 0.2663579529460062 Training Accuracy 0.8930166666666667\n",
      "Test Iteration : 469 Test Loss : 0.3522027613947064 test Accuracy 0.8634\n",
      "Train Iteration : 470 Training Loss : 0.2659400432048138 Training Accuracy 0.8927\n",
      "Test Iteration : 470 Test Loss : 0.3514360915779119 test Accuracy 0.8625\n",
      "Train Iteration : 471 Training Loss : 0.26598324266560813 Training Accuracy 0.89255\n",
      "Test Iteration : 471 Test Loss : 0.35222812492970257 test Accuracy 0.8626\n",
      "Train Iteration : 472 Training Loss : 0.26610371339645816 Training Accuracy 0.8932333333333333\n",
      "Test Iteration : 472 Test Loss : 0.35187699778840253 test Accuracy 0.8636\n",
      "Train Iteration : 473 Training Loss : 0.26584743819322515 Training Accuracy 0.89285\n",
      "Test Iteration : 473 Test Loss : 0.3524121762789665 test Accuracy 0.8621\n",
      "Train Iteration : 474 Training Loss : 0.26585630396437065 Training Accuracy 0.8923166666666666\n",
      "Test Iteration : 474 Test Loss : 0.35196353434310845 test Accuracy 0.8621\n",
      "Train Iteration : 475 Training Loss : 0.26611660424762856 Training Accuracy 0.8927833333333334\n",
      "Test Iteration : 475 Test Loss : 0.3529209138621435 test Accuracy 0.8623\n",
      "Train Iteration : 476 Training Loss : 0.2661212478558402 Training Accuracy 0.8918\n",
      "Test Iteration : 476 Test Loss : 0.35240001107247915 test Accuracy 0.8624\n"
     ]
    }
   ],
   "source": [
    "# # for easy multiplication now the shape of X_train(784,60000) and X_test(784,10000)\n",
    "X_train = tf.convert_to_tensor(tr_x.T, dtype=tf.float64)\n",
    "X_test  = tf.convert_to_tensor(te_x.T,dtype= tf.float64)\n",
    "tr_y =    tf.convert_to_tensor(tr_y ,dtype= tf.float64)\n",
    "te_y =    tf.convert_to_tensor(te_y ,dtype= tf.float64)\n",
    "\n",
    "no_of_neurons_layer1 = 300\n",
    "no_of_neurons_layer2 = 100\n",
    "#describes the number of types of clothes(10)\n",
    "no_of_output_units  = 10\n",
    "no_of_features = X_train.shape[0]\n",
    "\n",
    "\n",
    "# define the weight whos shape is as the no_of_neurons ; but the weights cannot be same . because if we give same weights then the layers won't learn properly to produce different outputs\n",
    "# Multiply with small factor , its just a way to initialize the weights, we can experiment keeping (0.01) or removing (0.01)\n",
    "W1 = tf.Variable(tf.random.normal(shape=(no_of_neurons_layer1, no_of_features), dtype=tf.float64) * 0.01)\n",
    "#define the bais as vector of zeros \n",
    "b1 = tf.Variable(tf.zeros((no_of_neurons_layer1, 1),dtype=tf.float64))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal(shape=(no_of_neurons_layer2, no_of_neurons_layer1), dtype=tf.float64) * 0.01)\n",
    "b2 = tf.Variable(tf.zeros((no_of_neurons_layer2, 1),dtype=tf.float64))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal(shape=(no_of_output_units, no_of_neurons_layer2), dtype=tf.float64) * 0.01)\n",
    "b3 =tf.Variable(tf.zeros((no_of_output_units, 1),dtype=tf.float64))\n",
    "\n",
    "train_loss_list , train_acc_list , test_loss_list, test_acc_list = gradient_descent(X_train,tr_y, X_test,te_y, W1 , b1 , W2 , b2 , W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86xYwrtt8w95"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(test_loss_list)\n",
    "plt.title(\"loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvOy-7p2-qHq"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list)\n",
    "plt.plot(test_acc_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'test'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pDuJnGh-qSp"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(train_acc_list))\n",
    "plt.plot(np.log(test_acc_list))\n",
    "plt.plot(np.log(train_loss_list))\n",
    "plt.plot(np.log(test_loss_list))\n",
    "plt.title(\"Accuracy/Loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy/Loss\")\n",
    "plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNTpVMqWpfDG5j1qCaQ/2be",
   "collapsed_sections": [],
   "name": "Question1_2_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
