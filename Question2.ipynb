{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3361,
     "status": "ok",
     "timestamp": 1618574551356,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "uu6Vk9ZkW5vf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4685,
     "status": "ok",
     "timestamp": 1618574552691,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "SxWzjlpfXC90",
    "outputId": "cfd6d786-d38a-4f81-8a95-cf0368e85bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4683,
     "status": "ok",
     "timestamp": 1618574552692,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "pBlzDL6yXDBs"
   },
   "outputs": [],
   "source": [
    "def softmax_activation(X):\n",
    "  expo = tf.exp(X - tf.reduce_max(X, axis=0))\n",
    "  s = tf.reduce_sum(expo, axis=0, keepdims=True)\n",
    "  output =  expo / s\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4682,
     "status": "ok",
     "timestamp": 1618574552693,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "q-4CCNRCXDGL"
   },
   "outputs": [],
   "source": [
    "def forward_pass( X , W1 , b1, W2 , b2 , W3 , b3):\n",
    "  #push all the weights and data along with the bias through the first layer\n",
    "  hypo_1 = tf.matmul(W1 , X) + b1\n",
    "  relu_act1 = tf.nn.relu(hypo_1)\n",
    "\n",
    "  hypo_2 = tf.matmul(W2 , relu_act1 ) + b2\n",
    "  relu_act2 = tf.nn.relu(hypo_2)\n",
    "\n",
    "  hypo_3 = tf.matmul(W3 , relu_act2 ) + b3\n",
    "  pred_output = softmax_activation(hypo_3)\n",
    "\n",
    "  return pred_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4682,
     "status": "ok",
     "timestamp": 1618574552694,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "Bh28ECz3XDIs"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(tr_y , predictedYProb):\n",
    "  reduce_sum = -tf.reduce_sum(tr_y * tf.math.log(predictedYProb),axis = 0)\n",
    "  loss =  tf.cast(tf.reduce_mean(reduce_sum), tf.float64)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4682,
     "status": "ok",
     "timestamp": 1618574552695,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "gAJ4Z-htXDK8"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y , predictedYProb):\n",
    "  predictedYValues = tf.round(predictedYProb) \n",
    "  #we need only those values whose actualy value being 1 matches the predicted value being 1 \n",
    "  pred_correction = tf.cast(tf.equal(np.argmax(predictedYValues, axis = 0),np.argmax(Y, axis = 0)),tf.float64)  \n",
    "  accuracy_sum = tf.reduce_sum(pred_correction)\n",
    "  acc_score = accuracy_sum / Y.shape[1]\n",
    "  return acc_score\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5394,
     "status": "ok",
     "timestamp": 1618574553409,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "8jvf8JVXcxu9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(Xtrain,Ytrain, Xtest, Ytest, W1 , b1 , W2 , b2, W3 , b3):\n",
    "  learning_rate = 0.001\n",
    "  adam_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "  train_loss_list =[]\n",
    "  train_acc_list = []\n",
    "\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  no_of_iter = 500\n",
    "\n",
    "  for i in range(no_of_iter + 1):\n",
    "    #GradientTape will record all the operation till it encounters gradient\n",
    "    with tf.GradientTape() as tape:\n",
    "      train_predictedYProb  = forward_pass(Xtrain, W1 , b1 , W2 , b2, W3 , b3 )\n",
    "      train_loss = cross_entropy(Ytrain ,train_predictedYProb)\n",
    "\n",
    "    gradients = tape.gradient(train_loss,[W1 , b1 , W2 , b2, W3 , b3])\n",
    "    train_accuracy  = calculate_accuracy(Ytrain ,train_predictedYProb)\n",
    "\n",
    "    test_predictedYProb  = forward_pass(Xtest, W1 , b1 , W2 , b2, W3 , b3 )\n",
    "    test_loss = cross_entropy(Ytest ,test_predictedYProb)\n",
    "    test_accuracy = calculate_accuracy(Ytest ,test_predictedYProb)\n",
    "    \n",
    "    train_loss_list.append(train_loss.numpy())\n",
    "    train_acc_list.append(train_accuracy.numpy())\n",
    "    test_loss_list.append(test_loss.numpy())\n",
    "    test_acc_list.append(test_accuracy.numpy())\n",
    "\n",
    "    adam_optimizer.apply_gradients(zip(gradients , [W1 , b1 , W2 , b2, W3 , b3]))\n",
    "\n",
    "    print(\"Train Iteration :\", i , \"Training Loss :\",train_loss.numpy() , \"Training Accuracy\",train_accuracy.numpy())\n",
    "    print(\"Test Iteration :\", i , \"Test Loss :\",test_loss.numpy() , \"test Accuracy\",test_accuracy.numpy())\n",
    "\n",
    "  return train_loss_list , train_acc_list , test_loss_list, test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx1-1MsxXDNb",
    "outputId": "19cebe54-5a9a-4b2c-91c8-c6342ffb1cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iteration : 0 Training Loss : 2.3025065571986003 Training Accuracy 0.1\n",
      "Test Iteration : 0 Test Loss : 2.3024991943319284 test Accuracy 0.1\n",
      "Train Iteration : 1 Training Loss : 2.299515151609259 Training Accuracy 0.1\n",
      "Test Iteration : 1 Test Loss : 2.299536943715438 test Accuracy 0.1\n",
      "Train Iteration : 2 Training Loss : 2.294201521765323 Training Accuracy 0.1\n",
      "Test Iteration : 2 Test Loss : 2.294247062402586 test Accuracy 0.1\n",
      "Train Iteration : 3 Training Loss : 2.2855668947021193 Training Accuracy 0.1\n",
      "Test Iteration : 3 Test Loss : 2.285646503958503 test Accuracy 0.1\n",
      "Train Iteration : 4 Training Loss : 2.2722734504518916 Training Accuracy 0.1\n",
      "Test Iteration : 4 Test Loss : 2.272414880803907 test Accuracy 0.1\n",
      "Train Iteration : 5 Training Loss : 2.2528573098506346 Training Accuracy 0.1\n",
      "Test Iteration : 5 Test Loss : 2.2530706142674863 test Accuracy 0.1\n",
      "Train Iteration : 6 Training Loss : 2.2259694870980686 Training Accuracy 0.1\n",
      "Test Iteration : 6 Test Loss : 2.2262645821999145 test Accuracy 0.1\n",
      "Train Iteration : 7 Training Loss : 2.1902036935605813 Training Accuracy 0.1\n",
      "Test Iteration : 7 Test Loss : 2.1906137456456216 test Accuracy 0.1\n",
      "Train Iteration : 8 Training Loss : 2.1447635559966756 Training Accuracy 0.1\n",
      "Test Iteration : 8 Test Loss : 2.145343029029904 test Accuracy 0.1\n",
      "Train Iteration : 9 Training Loss : 2.0892654544763594 Training Accuracy 0.1\n",
      "Test Iteration : 9 Test Loss : 2.090079868133927 test Accuracy 0.1\n",
      "Train Iteration : 10 Training Loss : 2.024024144462284 Training Accuracy 0.1\n",
      "Test Iteration : 10 Test Loss : 2.025157478456589 test Accuracy 0.1\n",
      "Train Iteration : 11 Training Loss : 1.9500322375630985 Training Accuracy 0.1\n",
      "Test Iteration : 11 Test Loss : 1.9515932024227798 test Accuracy 0.1\n",
      "Train Iteration : 12 Training Loss : 1.8693132473146423 Training Accuracy 0.1\n",
      "Test Iteration : 12 Test Loss : 1.8713716204363657 test Accuracy 0.1\n",
      "Train Iteration : 13 Training Loss : 1.7842550215619934 Training Accuracy 0.10003333333333334\n",
      "Test Iteration : 13 Test Loss : 1.7868185850601255 test Accuracy 0.1\n",
      "Train Iteration : 14 Training Loss : 1.6990445650901254 Training Accuracy 0.11121666666666667\n",
      "Test Iteration : 14 Test Loss : 1.7020456026388073 test Accuracy 0.1116\n",
      "Train Iteration : 15 Training Loss : 1.6177029399556093 Training Accuracy 0.1398\n",
      "Test Iteration : 15 Test Loss : 1.621019443400485 test Accuracy 0.1374\n",
      "Train Iteration : 16 Training Loss : 1.5415938271385987 Training Accuracy 0.16351666666666667\n",
      "Test Iteration : 16 Test Loss : 1.545127793115037 test Accuracy 0.1602\n",
      "Train Iteration : 17 Training Loss : 1.4697971463732014 Training Accuracy 0.18318333333333334\n",
      "Test Iteration : 17 Test Loss : 1.4735561379974613 test Accuracy 0.179\n",
      "Train Iteration : 18 Training Loss : 1.4014034309951653 Training Accuracy 0.19471666666666668\n",
      "Test Iteration : 18 Test Loss : 1.4055932916881086 test Accuracy 0.1912\n",
      "Train Iteration : 19 Training Loss : 1.3383080170935133 Training Accuracy 0.19846666666666668\n",
      "Test Iteration : 19 Test Loss : 1.343330026141952 test Accuracy 0.1968\n",
      "Train Iteration : 20 Training Loss : 1.2812550941911176 Training Accuracy 0.20976666666666666\n",
      "Test Iteration : 20 Test Loss : 1.2873211050964717 test Accuracy 0.2089\n",
      "Train Iteration : 21 Training Loss : 1.227142247499206 Training Accuracy 0.23058333333333333\n",
      "Test Iteration : 21 Test Loss : 1.2342063321173213 test Accuracy 0.2295\n",
      "Train Iteration : 22 Training Loss : 1.1772517518940426 Training Accuracy 0.24713333333333334\n",
      "Test Iteration : 22 Test Loss : 1.185273492500065 test Accuracy 0.2445\n",
      "Train Iteration : 23 Training Loss : 1.1357046945596505 Training Accuracy 0.2770166666666667\n",
      "Test Iteration : 23 Test Loss : 1.1448445073248066 test Accuracy 0.2768\n",
      "Train Iteration : 24 Training Loss : 1.0992768494340672 Training Accuracy 0.31171666666666664\n",
      "Test Iteration : 24 Test Loss : 1.1099170691975384 test Accuracy 0.3103\n",
      "Train Iteration : 25 Training Loss : 1.0669426564183746 Training Accuracy 0.35108333333333336\n",
      "Test Iteration : 25 Test Loss : 1.079152788888455 test Accuracy 0.3506\n",
      "Train Iteration : 26 Training Loss : 1.0358782767685857 Training Accuracy 0.3885166666666667\n",
      "Test Iteration : 26 Test Loss : 1.0491036649070888 test Accuracy 0.386\n",
      "Train Iteration : 27 Training Loss : 1.0081327444171337 Training Accuracy 0.41635\n",
      "Test Iteration : 27 Test Loss : 1.0221722282080796 test Accuracy 0.4162\n",
      "Train Iteration : 28 Training Loss : 0.9860840944106851 Training Accuracy 0.43695\n",
      "Test Iteration : 28 Test Loss : 1.0012084700284243 test Accuracy 0.4359\n",
      "Train Iteration : 29 Training Loss : 0.9660706427900897 Training Accuracy 0.4555166666666667\n",
      "Test Iteration : 29 Test Loss : 0.9824596475475419 test Accuracy 0.4528\n",
      "Train Iteration : 30 Training Loss : 0.9456776849710719 Training Accuracy 0.4708333333333333\n",
      "Test Iteration : 30 Test Loss : 0.963693792804222 test Accuracy 0.4674\n",
      "Train Iteration : 31 Training Loss : 0.9271238223496344 Training Accuracy 0.47835\n",
      "Test Iteration : 31 Test Loss : 0.9465351124208647 test Accuracy 0.4746\n",
      "Train Iteration : 32 Training Loss : 0.9118511640071953 Training Accuracy 0.48823333333333335\n",
      "Test Iteration : 32 Test Loss : 0.9322424866002279 test Accuracy 0.484\n",
      "Train Iteration : 33 Training Loss : 0.8982982917530627 Training Accuracy 0.5068333333333334\n",
      "Test Iteration : 33 Test Loss : 0.9195247768896716 test Accuracy 0.5002\n",
      "Train Iteration : 34 Training Loss : 0.883433124965486 Training Accuracy 0.51265\n",
      "Test Iteration : 34 Test Loss : 0.9051006049486457 test Accuracy 0.5042\n",
      "Train Iteration : 35 Training Loss : 0.8707855053232838 Training Accuracy 0.52525\n",
      "Test Iteration : 35 Test Loss : 0.8934844843915379 test Accuracy 0.5172\n",
      "Train Iteration : 36 Training Loss : 0.8603531783439773 Training Accuracy 0.5507833333333333\n",
      "Test Iteration : 36 Test Loss : 0.8839321422155213 test Accuracy 0.5414\n",
      "Train Iteration : 37 Training Loss : 0.8494492926249448 Training Accuracy 0.5487333333333333\n",
      "Test Iteration : 37 Test Loss : 0.8726993963825318 test Accuracy 0.5404\n",
      "Train Iteration : 38 Training Loss : 0.8389082978010437 Training Accuracy 0.5580333333333334\n",
      "Test Iteration : 38 Test Loss : 0.8622079100451087 test Accuracy 0.5493\n",
      "Train Iteration : 39 Training Loss : 0.8295400627015372 Training Accuracy 0.5742\n",
      "Test Iteration : 39 Test Loss : 0.8533070930972796 test Accuracy 0.5661\n",
      "Train Iteration : 40 Training Loss : 0.8218995345348149 Training Accuracy 0.57595\n",
      "Test Iteration : 40 Test Loss : 0.8464231778281858 test Accuracy 0.5691\n",
      "Train Iteration : 41 Training Loss : 0.8135179215424803 Training Accuracy 0.5780666666666666\n",
      "Test Iteration : 41 Test Loss : 0.8377576032855939 test Accuracy 0.5696\n",
      "Train Iteration : 42 Training Loss : 0.8021473839347331 Training Accuracy 0.5935333333333334\n",
      "Test Iteration : 42 Test Loss : 0.8260463343465592 test Accuracy 0.5873\n",
      "Train Iteration : 43 Training Loss : 0.7932567261827413 Training Accuracy 0.5941166666666666\n",
      "Test Iteration : 43 Test Loss : 0.8170518074968831 test Accuracy 0.5878\n",
      "Train Iteration : 44 Training Loss : 0.7866385153999108 Training Accuracy 0.59645\n",
      "Test Iteration : 44 Test Loss : 0.8102354123357072 test Accuracy 0.5896\n",
      "Train Iteration : 45 Training Loss : 0.7757541650570386 Training Accuracy 0.60315\n",
      "Test Iteration : 45 Test Loss : 0.798647500527498 test Accuracy 0.5967\n",
      "Train Iteration : 46 Training Loss : 0.7673646247837947 Training Accuracy 0.6089\n",
      "Test Iteration : 46 Test Loss : 0.7900095750336702 test Accuracy 0.6022\n",
      "Train Iteration : 47 Training Loss : 0.7599057576921019 Training Accuracy 0.6117333333333334\n",
      "Test Iteration : 47 Test Loss : 0.7829189731667054 test Accuracy 0.6077\n",
      "Train Iteration : 48 Training Loss : 0.7505801306351678 Training Accuracy 0.6112833333333333\n",
      "Test Iteration : 48 Test Loss : 0.7736142673304931 test Accuracy 0.6073\n",
      "Train Iteration : 49 Training Loss : 0.7430033337686063 Training Accuracy 0.6231\n",
      "Test Iteration : 49 Test Loss : 0.765707727403876 test Accuracy 0.617\n",
      "Train Iteration : 50 Training Loss : 0.7348250445745517 Training Accuracy 0.6241666666666666\n",
      "Test Iteration : 50 Test Loss : 0.7573495075079841 test Accuracy 0.6198\n",
      "Train Iteration : 51 Training Loss : 0.7275957829688718 Training Accuracy 0.61715\n",
      "Test Iteration : 51 Test Loss : 0.7503261489144133 test Accuracy 0.6104\n",
      "Train Iteration : 52 Training Loss : 0.7200948060744808 Training Accuracy 0.6361333333333333\n",
      "Test Iteration : 52 Test Loss : 0.7430297696392405 test Accuracy 0.6293\n",
      "Train Iteration : 53 Training Loss : 0.7119251132722777 Training Accuracy 0.6379833333333333\n",
      "Test Iteration : 53 Test Loss : 0.7349788281514004 test Accuracy 0.6322\n",
      "Train Iteration : 54 Training Loss : 0.706151416533855 Training Accuracy 0.6311833333333333\n",
      "Test Iteration : 54 Test Loss : 0.7295671046394422 test Accuracy 0.6262\n",
      "Train Iteration : 55 Training Loss : 0.6985882477459366 Training Accuracy 0.6518833333333334\n",
      "Test Iteration : 55 Test Loss : 0.7223040430797815 test Accuracy 0.6475\n",
      "Train Iteration : 56 Training Loss : 0.6917612959011422 Training Accuracy 0.65225\n",
      "Test Iteration : 56 Test Loss : 0.7156796169543889 test Accuracy 0.648\n",
      "Train Iteration : 57 Training Loss : 0.6856730538008934 Training Accuracy 0.6488\n",
      "Test Iteration : 57 Test Loss : 0.7099461742894596 test Accuracy 0.6421\n",
      "Train Iteration : 58 Training Loss : 0.6792491331743112 Training Accuracy 0.6631666666666667\n",
      "Test Iteration : 58 Test Loss : 0.7039706607390519 test Accuracy 0.6557\n",
      "Train Iteration : 59 Training Loss : 0.673626553672757 Training Accuracy 0.66595\n",
      "Test Iteration : 59 Test Loss : 0.698972183961343 test Accuracy 0.6592\n",
      "Train Iteration : 60 Training Loss : 0.6671958121482703 Training Accuracy 0.6719166666666667\n",
      "Test Iteration : 60 Test Loss : 0.6931242770666107 test Accuracy 0.6643\n",
      "Train Iteration : 61 Training Loss : 0.6616963146138252 Training Accuracy 0.6757666666666666\n",
      "Test Iteration : 61 Test Loss : 0.6878394398685652 test Accuracy 0.6677\n",
      "Train Iteration : 62 Training Loss : 0.6562979498391224 Training Accuracy 0.6792666666666667\n",
      "Test Iteration : 62 Test Loss : 0.6825419033491561 test Accuracy 0.6712\n",
      "Train Iteration : 63 Training Loss : 0.6506097982793588 Training Accuracy 0.688\n",
      "Test Iteration : 63 Test Loss : 0.6773596259387766 test Accuracy 0.6778\n",
      "Train Iteration : 64 Training Loss : 0.6456591324352228 Training Accuracy 0.6841833333333334\n",
      "Test Iteration : 64 Test Loss : 0.6728606472348669 test Accuracy 0.6743\n",
      "Train Iteration : 65 Training Loss : 0.6401168991247131 Training Accuracy 0.6942\n",
      "Test Iteration : 65 Test Loss : 0.6672692529024529 test Accuracy 0.6866\n",
      "Train Iteration : 66 Training Loss : 0.635067263927441 Training Accuracy 0.6977\n",
      "Test Iteration : 66 Test Loss : 0.6626203090394732 test Accuracy 0.6899\n",
      "Train Iteration : 67 Training Loss : 0.6304318927234627 Training Accuracy 0.6977\n",
      "Test Iteration : 67 Test Loss : 0.6587113157906639 test Accuracy 0.6886\n",
      "Train Iteration : 68 Training Loss : 0.625416745380404 Training Accuracy 0.70555\n",
      "Test Iteration : 68 Test Loss : 0.6537720109445891 test Accuracy 0.6963\n",
      "Train Iteration : 69 Training Loss : 0.6209244239818841 Training Accuracy 0.7052\n",
      "Test Iteration : 69 Test Loss : 0.6492647212196312 test Accuracy 0.6965\n",
      "Train Iteration : 70 Training Loss : 0.6163629762250294 Training Accuracy 0.7109166666666666\n",
      "Test Iteration : 70 Test Loss : 0.6451613596753418 test Accuracy 0.7005\n",
      "Train Iteration : 71 Training Loss : 0.6116195288949888 Training Accuracy 0.7101833333333334\n",
      "Test Iteration : 71 Test Loss : 0.6406915603084344 test Accuracy 0.6988\n",
      "Train Iteration : 72 Training Loss : 0.6072790323543114 Training Accuracy 0.7181833333333333\n",
      "Test Iteration : 72 Test Loss : 0.6364990102183634 test Accuracy 0.7081\n",
      "Train Iteration : 73 Training Loss : 0.6027665842593087 Training Accuracy 0.7162833333333334\n",
      "Test Iteration : 73 Test Loss : 0.6323370478763454 test Accuracy 0.7058\n",
      "Train Iteration : 74 Training Loss : 0.5983694925387991 Training Accuracy 0.7216833333333333\n",
      "Test Iteration : 74 Test Loss : 0.6282381359917019 test Accuracy 0.7128\n",
      "Train Iteration : 75 Training Loss : 0.5942600697510958 Training Accuracy 0.72335\n",
      "Test Iteration : 75 Test Loss : 0.6242380182362564 test Accuracy 0.7128\n",
      "Train Iteration : 76 Training Loss : 0.5900331116083531 Training Accuracy 0.726\n",
      "Test Iteration : 76 Test Loss : 0.620298642487145 test Accuracy 0.7147\n",
      "Train Iteration : 77 Training Loss : 0.5859676727305911 Training Accuracy 0.7275666666666667\n",
      "Test Iteration : 77 Test Loss : 0.6162807401639001 test Accuracy 0.7142\n",
      "Train Iteration : 78 Training Loss : 0.5821213331069562 Training Accuracy 0.7319166666666667\n",
      "Test Iteration : 78 Test Loss : 0.6126033728873738 test Accuracy 0.7212\n",
      "Train Iteration : 79 Training Loss : 0.5782552977749397 Training Accuracy 0.7307\n",
      "Test Iteration : 79 Test Loss : 0.6091495700906802 test Accuracy 0.7179\n",
      "Train Iteration : 80 Training Loss : 0.5745727288685348 Training Accuracy 0.73675\n",
      "Test Iteration : 80 Test Loss : 0.6055663488887624 test Accuracy 0.7237\n",
      "Train Iteration : 81 Training Loss : 0.5714245445504288 Training Accuracy 0.7324666666666667\n",
      "Test Iteration : 81 Test Loss : 0.6023727354228505 test Accuracy 0.72\n",
      "Train Iteration : 82 Training Loss : 0.5691161743233706 Training Accuracy 0.7420666666666667\n",
      "Test Iteration : 82 Test Loss : 0.6007240116077677 test Accuracy 0.7291\n",
      "Train Iteration : 83 Training Loss : 0.568122815306087 Training Accuracy 0.7305\n",
      "Test Iteration : 83 Test Loss : 0.5995909475715802 test Accuracy 0.7172\n",
      "Train Iteration : 84 Training Loss : 0.5662089594787009 Training Accuracy 0.7474833333333334\n",
      "Test Iteration : 84 Test Loss : 0.5981266031762384 test Accuracy 0.7355\n",
      "Train Iteration : 85 Training Loss : 0.559942156265841 Training Accuracy 0.73625\n",
      "Test Iteration : 85 Test Loss : 0.5919052780677665 test Accuracy 0.7225\n",
      "Train Iteration : 86 Training Loss : 0.5531390593429455 Training Accuracy 0.7468666666666667\n",
      "Test Iteration : 86 Test Loss : 0.5852865365642849 test Accuracy 0.7332\n",
      "Train Iteration : 87 Training Loss : 0.5521650671500259 Training Accuracy 0.75145\n",
      "Test Iteration : 87 Test Loss : 0.5845065523434964 test Accuracy 0.7373\n",
      "Train Iteration : 88 Training Loss : 0.5509658510134664 Training Accuracy 0.7420333333333333\n",
      "Test Iteration : 88 Test Loss : 0.5835469184363966 test Accuracy 0.7271\n",
      "Train Iteration : 89 Training Loss : 0.5448906555709808 Training Accuracy 0.7537166666666667\n",
      "Test Iteration : 89 Test Loss : 0.5778972902140673 test Accuracy 0.739\n",
      "Train Iteration : 90 Training Loss : 0.5412962891479945 Training Accuracy 0.7551666666666667\n",
      "Test Iteration : 90 Test Loss : 0.5742927781930238 test Accuracy 0.7401\n",
      "Train Iteration : 91 Training Loss : 0.5406041341544885 Training Accuracy 0.74955\n",
      "Test Iteration : 91 Test Loss : 0.5739725550296165 test Accuracy 0.7341\n",
      "Train Iteration : 92 Training Loss : 0.5365144210993887 Training Accuracy 0.75965\n",
      "Test Iteration : 92 Test Loss : 0.5701719183336268 test Accuracy 0.7438\n",
      "Train Iteration : 93 Training Loss : 0.5322882583177353 Training Accuracy 0.7589833333333333\n",
      "Test Iteration : 93 Test Loss : 0.5658535353645715 test Accuracy 0.7428\n",
      "Train Iteration : 94 Training Loss : 0.5309997760750619 Training Accuracy 0.75685\n",
      "Test Iteration : 94 Test Loss : 0.5650841139194105 test Accuracy 0.7409\n",
      "Train Iteration : 95 Training Loss : 0.5280509338159461 Training Accuracy 0.7642\n",
      "Test Iteration : 95 Test Loss : 0.5622166674056596 test Accuracy 0.7485\n",
      "Train Iteration : 96 Training Loss : 0.5239838833448761 Training Accuracy 0.7631166666666667\n",
      "Test Iteration : 96 Test Loss : 0.5581736127383988 test Accuracy 0.7481\n",
      "Train Iteration : 97 Training Loss : 0.5221800887425458 Training Accuracy 0.7630833333333333\n",
      "Test Iteration : 97 Test Loss : 0.5569287458169243 test Accuracy 0.748\n",
      "Train Iteration : 98 Training Loss : 0.5199489219112414 Training Accuracy 0.7688\n",
      "Test Iteration : 98 Test Loss : 0.5545908788098567 test Accuracy 0.7546\n",
      "Train Iteration : 99 Training Loss : 0.5161535706294322 Training Accuracy 0.76755\n",
      "Test Iteration : 99 Test Loss : 0.5510915079940569 test Accuracy 0.7515\n",
      "Train Iteration : 100 Training Loss : 0.5138194233104622 Training Accuracy 0.76855\n",
      "Test Iteration : 100 Test Loss : 0.5490250999707276 test Accuracy 0.7529\n",
      "Train Iteration : 101 Training Loss : 0.5120800370185145 Training Accuracy 0.7726833333333334\n",
      "Test Iteration : 101 Test Loss : 0.5472132165790292 test Accuracy 0.7582\n",
      "Train Iteration : 102 Training Loss : 0.5089063631204886 Training Accuracy 0.7717666666666667\n",
      "Test Iteration : 102 Test Loss : 0.544615126022605 test Accuracy 0.7568\n",
      "Train Iteration : 103 Training Loss : 0.5060729167203739 Training Accuracy 0.77405\n",
      "Test Iteration : 103 Test Loss : 0.5416425120519621 test Accuracy 0.7572\n",
      "Train Iteration : 104 Training Loss : 0.5042289750199809 Training Accuracy 0.7774166666666666\n",
      "Test Iteration : 104 Test Loss : 0.5401217441902721 test Accuracy 0.7625\n",
      "Train Iteration : 105 Training Loss : 0.5017237581547475 Training Accuracy 0.7767\n",
      "Test Iteration : 105 Test Loss : 0.5378676130908168 test Accuracy 0.7614\n",
      "Train Iteration : 106 Training Loss : 0.4988697372759866 Training Accuracy 0.7794333333333333\n",
      "Test Iteration : 106 Test Loss : 0.5350250805316847 test Accuracy 0.7646\n",
      "Train Iteration : 107 Training Loss : 0.49684833216134017 Training Accuracy 0.7819333333333334\n",
      "Test Iteration : 107 Test Loss : 0.5334797197703185 test Accuracy 0.7663\n",
      "Train Iteration : 108 Training Loss : 0.49487332505803305 Training Accuracy 0.78065\n",
      "Test Iteration : 108 Test Loss : 0.5312909920409981 test Accuracy 0.7662\n",
      "Train Iteration : 109 Training Loss : 0.4922897201070726 Training Accuracy 0.7850166666666667\n",
      "Test Iteration : 109 Test Loss : 0.5293324820353402 test Accuracy 0.7699\n",
      "Train Iteration : 110 Training Loss : 0.48989589539465267 Training Accuracy 0.7856333333333333\n",
      "Test Iteration : 110 Test Loss : 0.5266392772514431 test Accuracy 0.7715\n",
      "Train Iteration : 111 Training Loss : 0.4880894733505801 Training Accuracy 0.7864166666666667\n",
      "Test Iteration : 111 Test Loss : 0.5255158620836065 test Accuracy 0.771\n",
      "Train Iteration : 112 Training Loss : 0.48626511280665335 Training Accuracy 0.7886833333333333\n",
      "Test Iteration : 112 Test Loss : 0.5232257797496054 test Accuracy 0.7753\n",
      "Train Iteration : 113 Training Loss : 0.4840876074156038 Training Accuracy 0.7896333333333333\n",
      "Test Iteration : 113 Test Loss : 0.5220228086824064 test Accuracy 0.7743\n",
      "Train Iteration : 114 Training Loss : 0.48253927055012386 Training Accuracy 0.7900666666666667\n",
      "Test Iteration : 114 Test Loss : 0.5195802223558449 test Accuracy 0.7761\n",
      "Train Iteration : 115 Training Loss : 0.4813361369985869 Training Accuracy 0.7933166666666667\n",
      "Test Iteration : 115 Test Loss : 0.5198518813200093 test Accuracy 0.7788\n",
      "Train Iteration : 116 Training Loss : 0.4796196425036041 Training Accuracy 0.7918666666666667\n",
      "Test Iteration : 116 Test Loss : 0.5167713320998746 test Accuracy 0.7786\n",
      "Train Iteration : 117 Training Loss : 0.4768247949187838 Training Accuracy 0.7951166666666667\n",
      "Test Iteration : 117 Test Loss : 0.515522668588103 test Accuracy 0.7809\n",
      "Train Iteration : 118 Training Loss : 0.4739754429198872 Training Accuracy 0.7962\n",
      "Test Iteration : 118 Test Loss : 0.511757107878113 test Accuracy 0.7819\n",
      "Train Iteration : 119 Training Loss : 0.47169287268131205 Training Accuracy 0.7970166666666667\n",
      "Test Iteration : 119 Test Loss : 0.5100114523998114 test Accuracy 0.7835\n",
      "Train Iteration : 120 Training Loss : 0.47001519934970143 Training Accuracy 0.8002833333333333\n",
      "Test Iteration : 120 Test Loss : 0.5087042731307337 test Accuracy 0.7852\n",
      "Train Iteration : 121 Training Loss : 0.46859803804266326 Training Accuracy 0.7993833333333333\n",
      "Test Iteration : 121 Test Loss : 0.5067261103202875 test Accuracy 0.7848\n",
      "Train Iteration : 122 Training Loss : 0.46724255595461534 Training Accuracy 0.8003833333333333\n",
      "Test Iteration : 122 Test Loss : 0.5064741720419095 test Accuracy 0.7856\n",
      "Train Iteration : 123 Training Loss : 0.4658447979334413 Training Accuracy 0.8018166666666666\n",
      "Test Iteration : 123 Test Loss : 0.5042144650091662 test Accuracy 0.7882\n",
      "Train Iteration : 124 Training Loss : 0.4634320664809278 Training Accuracy 0.8019833333333334\n",
      "Test Iteration : 124 Test Loss : 0.5027273615497979 test Accuracy 0.7866\n",
      "Train Iteration : 125 Training Loss : 0.46102213528620284 Training Accuracy 0.8045666666666667\n",
      "Test Iteration : 125 Test Loss : 0.49994403593477926 test Accuracy 0.7912\n",
      "Train Iteration : 126 Training Loss : 0.45916551165475333 Training Accuracy 0.80575\n",
      "Test Iteration : 126 Test Loss : 0.4983202498635855 test Accuracy 0.792\n",
      "Train Iteration : 127 Training Loss : 0.45792491767902044 Training Accuracy 0.8054\n",
      "Test Iteration : 127 Test Loss : 0.49740455093949903 test Accuracy 0.7914\n",
      "Train Iteration : 128 Training Loss : 0.45681900081477106 Training Accuracy 0.8073833333333333\n",
      "Test Iteration : 128 Test Loss : 0.49590851332379854 test Accuracy 0.7939\n",
      "Train Iteration : 129 Training Loss : 0.45527355567620154 Training Accuracy 0.8070666666666667\n",
      "Test Iteration : 129 Test Loss : 0.49516771248534 test Accuracy 0.7917\n",
      "Train Iteration : 130 Training Loss : 0.45367110071168854 Training Accuracy 0.80845\n",
      "Test Iteration : 130 Test Loss : 0.4928725110477342 test Accuracy 0.7943\n",
      "Train Iteration : 131 Training Loss : 0.4518877413620827 Training Accuracy 0.8104833333333333\n",
      "Test Iteration : 131 Test Loss : 0.4919551605766981 test Accuracy 0.7949\n",
      "Train Iteration : 132 Training Loss : 0.45010758865120987 Training Accuracy 0.8100333333333334\n",
      "Test Iteration : 132 Test Loss : 0.489610786242244 test Accuracy 0.7956\n",
      "Train Iteration : 133 Training Loss : 0.44833672940936353 Training Accuracy 0.8125333333333333\n",
      "Test Iteration : 133 Test Loss : 0.4883608669475774 test Accuracy 0.798\n",
      "Train Iteration : 134 Training Loss : 0.4466854999858843 Training Accuracy 0.8123833333333333\n",
      "Test Iteration : 134 Test Loss : 0.4866817639543275 test Accuracy 0.7987\n",
      "Train Iteration : 135 Training Loss : 0.4452960003658276 Training Accuracy 0.8133333333333334\n",
      "Test Iteration : 135 Test Loss : 0.48524068058740083 test Accuracy 0.799\n",
      "Train Iteration : 136 Training Loss : 0.4441276732282222 Training Accuracy 0.8146666666666667\n",
      "Test Iteration : 136 Test Loss : 0.48458176818670257 test Accuracy 0.7987\n",
      "Train Iteration : 137 Training Loss : 0.44303231382951613 Training Accuracy 0.8141166666666667\n",
      "Test Iteration : 137 Test Loss : 0.4829645442129091 test Accuracy 0.7987\n",
      "Train Iteration : 138 Training Loss : 0.441894527750902 Training Accuracy 0.8159666666666666\n",
      "Test Iteration : 138 Test Loss : 0.4826858610495163 test Accuracy 0.7997\n",
      "Train Iteration : 139 Training Loss : 0.44075287793246876 Training Accuracy 0.8158666666666666\n",
      "Test Iteration : 139 Test Loss : 0.48078465283590816 test Accuracy 0.8005\n",
      "Train Iteration : 140 Training Loss : 0.4394763341652809 Training Accuracy 0.8166833333333333\n",
      "Test Iteration : 140 Test Loss : 0.4804267992860084 test Accuracy 0.8003\n",
      "Train Iteration : 141 Training Loss : 0.43835379234424643 Training Accuracy 0.8175833333333333\n",
      "Test Iteration : 141 Test Loss : 0.47857587879314945 test Accuracy 0.8028\n",
      "Train Iteration : 142 Training Loss : 0.43671701596274104 Training Accuracy 0.81705\n",
      "Test Iteration : 142 Test Loss : 0.47769075942027284 test Accuracy 0.8012\n",
      "Train Iteration : 143 Training Loss : 0.43509572206156516 Training Accuracy 0.81935\n",
      "Test Iteration : 143 Test Loss : 0.47560318243497857 test Accuracy 0.8052\n",
      "Train Iteration : 144 Training Loss : 0.4332427182986212 Training Accuracy 0.8190166666666666\n",
      "Test Iteration : 144 Test Loss : 0.474185410117864 test Accuracy 0.803\n",
      "Train Iteration : 145 Training Loss : 0.43164946532219645 Training Accuracy 0.8206\n",
      "Test Iteration : 145 Test Loss : 0.4724723367713478 test Accuracy 0.8058\n",
      "Train Iteration : 146 Training Loss : 0.4303770169157634 Training Accuracy 0.8213333333333334\n",
      "Test Iteration : 146 Test Loss : 0.4713025958885859 test Accuracy 0.8056\n",
      "Train Iteration : 147 Training Loss : 0.42935848973669466 Training Accuracy 0.8215333333333333\n",
      "Test Iteration : 147 Test Loss : 0.4704812190312856 test Accuracy 0.8054\n",
      "Train Iteration : 148 Training Loss : 0.428484678348113 Training Accuracy 0.8231333333333334\n",
      "Test Iteration : 148 Test Loss : 0.46943852924571977 test Accuracy 0.8072\n",
      "Train Iteration : 149 Training Loss : 0.4275424335423493 Training Accuracy 0.8226666666666667\n",
      "Test Iteration : 149 Test Loss : 0.4689336796991928 test Accuracy 0.8073\n",
      "Train Iteration : 150 Training Loss : 0.42668618522066526 Training Accuracy 0.82395\n",
      "Test Iteration : 150 Test Loss : 0.46762366705633196 test Accuracy 0.8089\n",
      "Train Iteration : 151 Training Loss : 0.42555573217441645 Training Accuracy 0.8241666666666667\n",
      "Test Iteration : 151 Test Loss : 0.4671728873819417 test Accuracy 0.8085\n",
      "Train Iteration : 152 Training Loss : 0.42451065680320177 Training Accuracy 0.8246666666666667\n",
      "Test Iteration : 152 Test Loss : 0.4654551500548498 test Accuracy 0.81\n",
      "Train Iteration : 153 Training Loss : 0.4232012567660563 Training Accuracy 0.82615\n",
      "Test Iteration : 153 Test Loss : 0.4649719290330263 test Accuracy 0.8102\n",
      "Train Iteration : 154 Training Loss : 0.42183896758363204 Training Accuracy 0.8261\n",
      "Test Iteration : 154 Test Loss : 0.4629124333344022 test Accuracy 0.8109\n",
      "Train Iteration : 155 Training Loss : 0.4204152113565697 Training Accuracy 0.8282833333333334\n",
      "Test Iteration : 155 Test Loss : 0.4622153060397102 test Accuracy 0.8116\n",
      "Train Iteration : 156 Training Loss : 0.4190109565196077 Training Accuracy 0.82725\n",
      "Test Iteration : 156 Test Loss : 0.46037528067607675 test Accuracy 0.8116\n",
      "Train Iteration : 157 Training Loss : 0.4177480059723606 Training Accuracy 0.82905\n",
      "Test Iteration : 157 Test Loss : 0.45946546310452613 test Accuracy 0.8134\n",
      "Train Iteration : 158 Training Loss : 0.41665854177612394 Training Accuracy 0.82895\n",
      "Test Iteration : 158 Test Loss : 0.45837339686257983 test Accuracy 0.8125\n",
      "Train Iteration : 159 Training Loss : 0.4157337382613461 Training Accuracy 0.8300166666666666\n",
      "Test Iteration : 159 Test Loss : 0.4573580246913505 test Accuracy 0.8147\n",
      "Train Iteration : 160 Training Loss : 0.4149078802727637 Training Accuracy 0.83005\n",
      "Test Iteration : 160 Test Loss : 0.4569315202683456 test Accuracy 0.8145\n",
      "Train Iteration : 161 Training Loss : 0.4141644889896959 Training Accuracy 0.8306333333333333\n",
      "Test Iteration : 161 Test Loss : 0.4557408130361586 test Accuracy 0.8156\n",
      "Train Iteration : 162 Training Loss : 0.41340258017035414 Training Accuracy 0.83105\n",
      "Test Iteration : 162 Test Loss : 0.45565134644428945 test Accuracy 0.8152\n",
      "Train Iteration : 163 Training Loss : 0.41265469391457155 Training Accuracy 0.8311166666666666\n",
      "Test Iteration : 163 Test Loss : 0.45424614106012623 test Accuracy 0.8155\n",
      "Train Iteration : 164 Training Loss : 0.4117418565564526 Training Accuracy 0.8320166666666666\n",
      "Test Iteration : 164 Test Loss : 0.45412796120713667 test Accuracy 0.8163\n",
      "Train Iteration : 165 Training Loss : 0.41082239270730747 Training Accuracy 0.83235\n",
      "Test Iteration : 165 Test Loss : 0.45251287471964385 test Accuracy 0.8173\n",
      "Train Iteration : 166 Training Loss : 0.40962038653448607 Training Accuracy 0.8324666666666667\n",
      "Test Iteration : 166 Test Loss : 0.452005802366424 test Accuracy 0.8177\n",
      "Train Iteration : 167 Training Loss : 0.40850938432256917 Training Accuracy 0.8335666666666667\n",
      "Test Iteration : 167 Test Loss : 0.4504171825172116 test Accuracy 0.8174\n",
      "Train Iteration : 168 Training Loss : 0.40727565414307165 Training Accuracy 0.8336\n",
      "Test Iteration : 168 Test Loss : 0.44953600300336094 test Accuracy 0.8181\n",
      "Train Iteration : 169 Training Loss : 0.40625149559415474 Training Accuracy 0.8351166666666666\n",
      "Test Iteration : 169 Test Loss : 0.4484675556997363 test Accuracy 0.8202\n",
      "Train Iteration : 170 Training Loss : 0.4053209663204515 Training Accuracy 0.8342833333333334\n",
      "Test Iteration : 170 Test Loss : 0.4474432980840165 test Accuracy 0.8176\n",
      "Train Iteration : 171 Training Loss : 0.40452356122657585 Training Accuracy 0.8362833333333334\n",
      "Test Iteration : 171 Test Loss : 0.447048679320261 test Accuracy 0.8214\n",
      "Train Iteration : 172 Training Loss : 0.4037731232724994 Training Accuracy 0.83485\n",
      "Test Iteration : 172 Test Loss : 0.4458292678406644 test Accuracy 0.8189\n",
      "Train Iteration : 173 Training Loss : 0.40303403329130105 Training Accuracy 0.8374\n",
      "Test Iteration : 173 Test Loss : 0.44579245581168664 test Accuracy 0.8226\n",
      "Train Iteration : 174 Training Loss : 0.40218447381780326 Training Accuracy 0.8355333333333334\n",
      "Test Iteration : 174 Test Loss : 0.4442552741244941 test Accuracy 0.8199\n",
      "Train Iteration : 175 Training Loss : 0.4012742550563581 Training Accuracy 0.8379333333333333\n",
      "Test Iteration : 175 Test Loss : 0.4441683318298263 test Accuracy 0.8236\n",
      "Train Iteration : 176 Training Loss : 0.40019669033637606 Training Accuracy 0.8368\n",
      "Test Iteration : 176 Test Loss : 0.44240562818464213 test Accuracy 0.8203\n",
      "Train Iteration : 177 Training Loss : 0.3990983946068376 Training Accuracy 0.8384333333333334\n",
      "Test Iteration : 177 Test Loss : 0.4420284307394 test Accuracy 0.8243\n",
      "Train Iteration : 178 Training Loss : 0.3979991701866288 Training Accuracy 0.8384666666666667\n",
      "Test Iteration : 178 Test Loss : 0.44047978706547647 test Accuracy 0.8215\n",
      "Train Iteration : 179 Training Loss : 0.39700051364879274 Training Accuracy 0.8391166666666666\n",
      "Test Iteration : 179 Test Loss : 0.43989888782062264 test Accuracy 0.8244\n",
      "Train Iteration : 180 Training Loss : 0.39612233010983633 Training Accuracy 0.8398333333333333\n",
      "Test Iteration : 180 Test Loss : 0.4389552345323329 test Accuracy 0.8246\n",
      "Train Iteration : 181 Training Loss : 0.39536495788360404 Training Accuracy 0.8396833333333333\n",
      "Test Iteration : 181 Test Loss : 0.4382718106004759 test Accuracy 0.8234\n",
      "Train Iteration : 182 Training Loss : 0.3947308162586624 Training Accuracy 0.8404833333333334\n",
      "Test Iteration : 182 Test Loss : 0.4379093955472883 test Accuracy 0.825\n",
      "Train Iteration : 183 Training Loss : 0.39423033226573556 Training Accuracy 0.8395333333333334\n",
      "Test Iteration : 183 Test Loss : 0.4372035167172958 test Accuracy 0.8217\n",
      "Train Iteration : 184 Training Loss : 0.3939558669800231 Training Accuracy 0.8409666666666666\n",
      "Test Iteration : 184 Test Loss : 0.43744417580659994 test Accuracy 0.8257\n",
      "Train Iteration : 185 Training Loss : 0.39395286854693723 Training Accuracy 0.8389333333333333\n",
      "Test Iteration : 185 Test Loss : 0.43700795504228374 test Accuracy 0.8211\n",
      "Train Iteration : 186 Training Loss : 0.3944745400256584 Training Accuracy 0.8415\n",
      "Test Iteration : 186 Test Loss : 0.438247634998728 test Accuracy 0.8246\n",
      "Train Iteration : 187 Training Loss : 0.3947120657366237 Training Accuracy 0.8375166666666667\n",
      "Test Iteration : 187 Test Loss : 0.4379570313848798 test Accuracy 0.8209\n",
      "Train Iteration : 188 Training Loss : 0.3950376479141285 Training Accuracy 0.8411666666666666\n",
      "Test Iteration : 188 Test Loss : 0.43889361176940456 test Accuracy 0.8257\n",
      "Train Iteration : 189 Training Loss : 0.39246278769719806 Training Accuracy 0.8394833333333334\n",
      "Test Iteration : 189 Test Loss : 0.43609732473727675 test Accuracy 0.8226\n",
      "Train Iteration : 190 Training Loss : 0.3904949549293691 Training Accuracy 0.8424833333333334\n",
      "Test Iteration : 190 Test Loss : 0.43387879167669874 test Accuracy 0.8256\n",
      "Train Iteration : 191 Training Loss : 0.3893967046914997 Training Accuracy 0.84315\n",
      "Test Iteration : 191 Test Loss : 0.43364638988728127 test Accuracy 0.8276\n",
      "Train Iteration : 192 Training Loss : 0.3898473004661886 Training Accuracy 0.8402166666666666\n",
      "Test Iteration : 192 Test Loss : 0.4328816673062238 test Accuracy 0.8237\n",
      "Train Iteration : 193 Training Loss : 0.3900275261954674 Training Accuracy 0.8435166666666667\n",
      "Test Iteration : 193 Test Loss : 0.4347613034151334 test Accuracy 0.8279\n",
      "Train Iteration : 194 Training Loss : 0.3878856867221688 Training Accuracy 0.84085\n",
      "Test Iteration : 194 Test Loss : 0.43128053892540813 test Accuracy 0.8249\n",
      "Train Iteration : 195 Training Loss : 0.38519828344394025 Training Accuracy 0.8454333333333334\n",
      "Test Iteration : 195 Test Loss : 0.429445319190364 test Accuracy 0.8295\n",
      "Train Iteration : 196 Training Loss : 0.3843669213008908 Training Accuracy 0.8454666666666667\n",
      "Test Iteration : 196 Test Loss : 0.42866949007725613 test Accuracy 0.8291\n",
      "Train Iteration : 197 Training Loss : 0.38510260307534894 Training Accuracy 0.8428833333333333\n",
      "Test Iteration : 197 Test Loss : 0.4288023768727161 test Accuracy 0.8258\n",
      "Train Iteration : 198 Training Loss : 0.385142131128008 Training Accuracy 0.8454\n",
      "Test Iteration : 198 Test Loss : 0.4300699956764017 test Accuracy 0.8292\n",
      "Train Iteration : 199 Training Loss : 0.38346649694004165 Training Accuracy 0.8438166666666667\n",
      "Test Iteration : 199 Test Loss : 0.42727662327365407 test Accuracy 0.8275\n",
      "Train Iteration : 200 Training Loss : 0.3818561118983241 Training Accuracy 0.84575\n",
      "Test Iteration : 200 Test Loss : 0.4264739013968579 test Accuracy 0.8289\n",
      "Train Iteration : 201 Training Loss : 0.3815272195733173 Training Accuracy 0.8469833333333333\n",
      "Test Iteration : 201 Test Loss : 0.4261009452126147 test Accuracy 0.8291\n",
      "Train Iteration : 202 Training Loss : 0.38149781545371936 Training Accuracy 0.844\n",
      "Test Iteration : 202 Test Loss : 0.42579860601718744 test Accuracy 0.8282\n",
      "Train Iteration : 203 Training Loss : 0.38063544846706526 Training Accuracy 0.8472\n",
      "Test Iteration : 203 Test Loss : 0.42565187728887366 test Accuracy 0.8303\n",
      "Train Iteration : 204 Training Loss : 0.37916839122215124 Training Accuracy 0.8459333333333333\n",
      "Test Iteration : 204 Test Loss : 0.42370052030449207 test Accuracy 0.8298\n",
      "Train Iteration : 205 Training Loss : 0.3783742838312239 Training Accuracy 0.8466166666666667\n",
      "Test Iteration : 205 Test Loss : 0.42323475519098586 test Accuracy 0.8297\n",
      "Train Iteration : 206 Training Loss : 0.37834403579683795 Training Accuracy 0.8478833333333333\n",
      "Test Iteration : 206 Test Loss : 0.4234921427078498 test Accuracy 0.8303\n",
      "Train Iteration : 207 Training Loss : 0.37802620614028226 Training Accuracy 0.8459166666666667\n",
      "Test Iteration : 207 Test Loss : 0.4229015538519679 test Accuracy 0.83\n",
      "Train Iteration : 208 Training Loss : 0.37711644188637655 Training Accuracy 0.84825\n",
      "Test Iteration : 208 Test Loss : 0.4224006400803317 test Accuracy 0.8306\n",
      "Train Iteration : 209 Training Loss : 0.3760096706262645 Training Accuracy 0.8477333333333333\n",
      "Test Iteration : 209 Test Loss : 0.4213003975608356 test Accuracy 0.8311\n",
      "Train Iteration : 210 Training Loss : 0.3754403142930109 Training Accuracy 0.8481666666666666\n",
      "Test Iteration : 210 Test Loss : 0.42049216999353356 test Accuracy 0.8309\n",
      "Train Iteration : 211 Training Loss : 0.3751767976848442 Training Accuracy 0.8494333333333334\n",
      "Test Iteration : 211 Test Loss : 0.42097003630156876 test Accuracy 0.832\n",
      "Train Iteration : 212 Training Loss : 0.3746043831774825 Training Accuracy 0.8477\n",
      "Test Iteration : 212 Test Loss : 0.4197227635222969 test Accuracy 0.8318\n",
      "Train Iteration : 213 Training Loss : 0.3736784501711368 Training Accuracy 0.8499166666666667\n",
      "Test Iteration : 213 Test Loss : 0.4195711173343911 test Accuracy 0.8323\n",
      "Train Iteration : 214 Training Loss : 0.37278236379582436 Training Accuracy 0.8494666666666667\n",
      "Test Iteration : 214 Test Loss : 0.4183573962034043 test Accuracy 0.8325\n",
      "Train Iteration : 215 Training Loss : 0.37222981764848884 Training Accuracy 0.8492333333333333\n",
      "Test Iteration : 215 Test Loss : 0.4180412435489394 test Accuracy 0.8327\n",
      "Train Iteration : 216 Training Loss : 0.3718652846843605 Training Accuracy 0.8503333333333334\n",
      "Test Iteration : 216 Test Loss : 0.4179252283093434 test Accuracy 0.8331\n",
      "Train Iteration : 217 Training Loss : 0.37130960681825026 Training Accuracy 0.8495\n",
      "Test Iteration : 217 Test Loss : 0.4172280724599643 test Accuracy 0.8332\n",
      "Train Iteration : 218 Training Loss : 0.3705785971277406 Training Accuracy 0.8508\n",
      "Test Iteration : 218 Test Loss : 0.41679258796616914 test Accuracy 0.8339\n",
      "Train Iteration : 219 Training Loss : 0.36981970610638737 Training Accuracy 0.8504\n",
      "Test Iteration : 219 Test Loss : 0.416106311636821 test Accuracy 0.833\n",
      "Train Iteration : 220 Training Loss : 0.36929286277572465 Training Accuracy 0.8506\n",
      "Test Iteration : 220 Test Loss : 0.4154386373253077 test Accuracy 0.8328\n",
      "Train Iteration : 221 Training Loss : 0.36899713928329 Training Accuracy 0.85155\n",
      "Test Iteration : 221 Test Loss : 0.4157605977048821 test Accuracy 0.8339\n",
      "Train Iteration : 222 Training Loss : 0.368805913776915 Training Accuracy 0.8506333333333334\n",
      "Test Iteration : 222 Test Loss : 0.41491033528825655 test Accuracy 0.8329\n",
      "Train Iteration : 223 Training Loss : 0.36881131686538027 Training Accuracy 0.8516333333333334\n",
      "Test Iteration : 223 Test Loss : 0.41599398758029094 test Accuracy 0.8345\n",
      "Train Iteration : 224 Training Loss : 0.36903398352122707 Training Accuracy 0.8501666666666666\n",
      "Test Iteration : 224 Test Loss : 0.4152668373048412 test Accuracy 0.8338\n",
      "Train Iteration : 225 Training Loss : 0.36973662989950073 Training Accuracy 0.85075\n",
      "Test Iteration : 225 Test Loss : 0.41726698634827125 test Accuracy 0.833\n",
      "Train Iteration : 226 Training Loss : 0.3709425889326648 Training Accuracy 0.85035\n",
      "Test Iteration : 226 Test Loss : 0.41746009039242254 test Accuracy 0.834\n",
      "Train Iteration : 227 Training Loss : 0.3702476603964875 Training Accuracy 0.8499333333333333\n",
      "Test Iteration : 227 Test Loss : 0.4179857864943431 test Accuracy 0.8326\n",
      "Train Iteration : 228 Training Loss : 0.3686696097216175 Training Accuracy 0.8516833333333333\n",
      "Test Iteration : 228 Test Loss : 0.4155030284362278 test Accuracy 0.8341\n",
      "Train Iteration : 229 Training Loss : 0.3650127776522444 Training Accuracy 0.85175\n",
      "Test Iteration : 229 Test Loss : 0.41250550959131904 test Accuracy 0.8351\n",
      "Train Iteration : 230 Training Loss : 0.36328348112920866 Training Accuracy 0.8534\n",
      "Test Iteration : 230 Test Loss : 0.4106338529072828 test Accuracy 0.8355\n",
      "Train Iteration : 231 Training Loss : 0.3638946045958901 Training Accuracy 0.85325\n",
      "Test Iteration : 231 Test Loss : 0.4111816717812053 test Accuracy 0.8353\n",
      "Train Iteration : 232 Training Loss : 0.364861317304872 Training Accuracy 0.8522333333333333\n",
      "Test Iteration : 232 Test Loss : 0.4129461035720626 test Accuracy 0.8346\n",
      "Train Iteration : 233 Training Loss : 0.3647849889397552 Training Accuracy 0.8529666666666667\n",
      "Test Iteration : 233 Test Loss : 0.4121925573830356 test Accuracy 0.8353\n",
      "Train Iteration : 234 Training Loss : 0.36249350167316036 Training Accuracy 0.8533833333333334\n",
      "Test Iteration : 234 Test Loss : 0.4107347410131398 test Accuracy 0.8369\n",
      "Train Iteration : 235 Training Loss : 0.36069509118166604 Training Accuracy 0.8540166666666666\n",
      "Test Iteration : 235 Test Loss : 0.40844821831614186 test Accuracy 0.8355\n",
      "Train Iteration : 236 Training Loss : 0.36046341107885055 Training Accuracy 0.8545166666666667\n",
      "Test Iteration : 236 Test Loss : 0.40844185126031834 test Accuracy 0.8363\n",
      "Train Iteration : 237 Training Loss : 0.3608787889348604 Training Accuracy 0.8532\n",
      "Test Iteration : 237 Test Loss : 0.40921929454054884 test Accuracy 0.8368\n",
      "Train Iteration : 238 Training Loss : 0.3607789492603869 Training Accuracy 0.8548\n",
      "Test Iteration : 238 Test Loss : 0.408700931764525 test Accuracy 0.8365\n",
      "Train Iteration : 239 Training Loss : 0.35935683374269695 Training Accuracy 0.8541166666666666\n",
      "Test Iteration : 239 Test Loss : 0.4079654423359675 test Accuracy 0.8377\n",
      "Train Iteration : 240 Training Loss : 0.35810818960074764 Training Accuracy 0.8551666666666666\n",
      "Test Iteration : 240 Test Loss : 0.4062834648784176 test Accuracy 0.8363\n",
      "Train Iteration : 241 Training Loss : 0.35772493675065237 Training Accuracy 0.8558833333333333\n",
      "Test Iteration : 241 Test Loss : 0.4063545313694025 test Accuracy 0.8366\n",
      "Train Iteration : 242 Training Loss : 0.35770594274439765 Training Accuracy 0.8541\n",
      "Test Iteration : 242 Test Loss : 0.4063935697126226 test Accuracy 0.8378\n",
      "Train Iteration : 243 Training Loss : 0.3574116745037522 Training Accuracy 0.8560333333333333\n",
      "Test Iteration : 243 Test Loss : 0.40614223624484286 test Accuracy 0.8371\n",
      "Train Iteration : 244 Training Loss : 0.35636572587588466 Training Accuracy 0.8553833333333334\n",
      "Test Iteration : 244 Test Loss : 0.4054586337400352 test Accuracy 0.8388\n",
      "Train Iteration : 245 Training Loss : 0.3554319020866865 Training Accuracy 0.8561833333333333\n",
      "Test Iteration : 245 Test Loss : 0.4042964160789451 test Accuracy 0.8373\n",
      "Train Iteration : 246 Training Loss : 0.35499107908560945 Training Accuracy 0.8569833333333333\n",
      "Test Iteration : 246 Test Loss : 0.4043182036953741 test Accuracy 0.8387\n",
      "Train Iteration : 247 Training Loss : 0.3548330679636282 Training Accuracy 0.8557166666666667\n",
      "Test Iteration : 247 Test Loss : 0.4039793954428282 test Accuracy 0.8388\n",
      "Train Iteration : 248 Training Loss : 0.35453425243246006 Training Accuracy 0.8573666666666667\n",
      "Test Iteration : 248 Test Loss : 0.40405686044937567 test Accuracy 0.8388\n",
      "Train Iteration : 249 Training Loss : 0.35372523701053893 Training Accuracy 0.8566333333333334\n",
      "Test Iteration : 249 Test Loss : 0.40317568557661104 test Accuracy 0.8392\n",
      "Train Iteration : 250 Training Loss : 0.35284372914372225 Training Accuracy 0.85785\n",
      "Test Iteration : 250 Test Loss : 0.4024921439148636 test Accuracy 0.8392\n",
      "Train Iteration : 251 Training Loss : 0.3521829110111258 Training Accuracy 0.8577166666666667\n",
      "Test Iteration : 251 Test Loss : 0.4018934228678392 test Accuracy 0.8397\n",
      "Train Iteration : 252 Training Loss : 0.35182054571460636 Training Accuracy 0.8577833333333333\n",
      "Test Iteration : 252 Test Loss : 0.4016554239715222 test Accuracy 0.8396\n",
      "Train Iteration : 253 Training Loss : 0.3515477670911963 Training Accuracy 0.8584833333333334\n",
      "Test Iteration : 253 Test Loss : 0.4015639176890306 test Accuracy 0.8397\n",
      "Train Iteration : 254 Training Loss : 0.35108315061080064 Training Accuracy 0.8578\n",
      "Test Iteration : 254 Test Loss : 0.4011457488547837 test Accuracy 0.84\n",
      "Train Iteration : 255 Training Loss : 0.35046708959215117 Training Accuracy 0.85875\n",
      "Test Iteration : 255 Test Loss : 0.4007759079703135 test Accuracy 0.84\n",
      "Train Iteration : 256 Training Loss : 0.3497626855725816 Training Accuracy 0.8585166666666667\n",
      "Test Iteration : 256 Test Loss : 0.3999942563681079 test Accuracy 0.8395\n",
      "Train Iteration : 257 Training Loss : 0.34915986907087815 Training Accuracy 0.8593333333333333\n",
      "Test Iteration : 257 Test Loss : 0.3997237102059776 test Accuracy 0.8399\n",
      "Train Iteration : 258 Training Loss : 0.3486382598259902 Training Accuracy 0.8591166666666666\n",
      "Test Iteration : 258 Test Loss : 0.39908318488144956 test Accuracy 0.8408\n",
      "Train Iteration : 259 Training Loss : 0.34825221650581745 Training Accuracy 0.8595\n",
      "Test Iteration : 259 Test Loss : 0.3991556276084875 test Accuracy 0.8418\n",
      "Train Iteration : 260 Training Loss : 0.34794297324752704 Training Accuracy 0.8595166666666667\n",
      "Test Iteration : 260 Test Loss : 0.3985272548777933 test Accuracy 0.8407\n",
      "Train Iteration : 261 Training Loss : 0.3476300335615507 Training Accuracy 0.8596\n",
      "Test Iteration : 261 Test Loss : 0.3988771822907247 test Accuracy 0.841\n",
      "Train Iteration : 262 Training Loss : 0.3472991629108594 Training Accuracy 0.8595833333333334\n",
      "Test Iteration : 262 Test Loss : 0.3980490986583828 test Accuracy 0.8402\n",
      "Train Iteration : 263 Training Loss : 0.34710890942397976 Training Accuracy 0.8606666666666667\n",
      "Test Iteration : 263 Test Loss : 0.3988872648499344 test Accuracy 0.8424\n",
      "Train Iteration : 264 Training Loss : 0.34725547944950136 Training Accuracy 0.8587166666666667\n",
      "Test Iteration : 264 Test Loss : 0.3979651163282039 test Accuracy 0.8412\n",
      "Train Iteration : 265 Training Loss : 0.34826165297257616 Training Accuracy 0.8606666666666667\n",
      "Test Iteration : 265 Test Loss : 0.40076032686009155 test Accuracy 0.8418\n",
      "Train Iteration : 266 Training Loss : 0.3498953953627355 Training Accuracy 0.8559333333333333\n",
      "Test Iteration : 266 Test Loss : 0.40038111195339304 test Accuracy 0.837\n",
      "Train Iteration : 267 Training Loss : 0.35260242248142976 Training Accuracy 0.85945\n",
      "Test Iteration : 267 Test Loss : 0.40593377872150865 test Accuracy 0.8412\n",
      "Train Iteration : 268 Training Loss : 0.3525553722832311 Training Accuracy 0.8541666666666666\n",
      "Test Iteration : 268 Test Loss : 0.40293616159764784 test Accuracy 0.835\n",
      "Train Iteration : 269 Training Loss : 0.3497302982438328 Training Accuracy 0.8604333333333334\n",
      "Test Iteration : 269 Test Loss : 0.4029915678788337 test Accuracy 0.8423\n",
      "Train Iteration : 270 Training Loss : 0.3443079271203733 Training Accuracy 0.8599833333333333\n",
      "Test Iteration : 270 Test Loss : 0.39574078006675245 test Accuracy 0.8409\n",
      "Train Iteration : 271 Training Loss : 0.3423926169055229 Training Accuracy 0.8616666666666667\n",
      "Test Iteration : 271 Test Loss : 0.3943197762175138 test Accuracy 0.8428\n",
      "Train Iteration : 272 Training Loss : 0.34461295069320064 Training Accuracy 0.8621\n",
      "Test Iteration : 272 Test Loss : 0.39779156548230366 test Accuracy 0.8427\n",
      "Train Iteration : 273 Training Loss : 0.34600907595996827 Training Accuracy 0.8579333333333333\n",
      "Test Iteration : 273 Test Loss : 0.3974854871458424 test Accuracy 0.8393\n",
      "Train Iteration : 274 Training Loss : 0.3441906975915744 Training Accuracy 0.8620333333333333\n",
      "Test Iteration : 274 Test Loss : 0.39775120781564627 test Accuracy 0.8429\n",
      "Train Iteration : 275 Training Loss : 0.34077305472417135 Training Accuracy 0.8619666666666667\n",
      "Test Iteration : 275 Test Loss : 0.39324148520939955 test Accuracy 0.8417\n",
      "Train Iteration : 276 Training Loss : 0.34026748678964785 Training Accuracy 0.86205\n",
      "Test Iteration : 276 Test Loss : 0.3928800413024413 test Accuracy 0.842\n",
      "Train Iteration : 277 Training Loss : 0.3419068751852775 Training Accuracy 0.863\n",
      "Test Iteration : 277 Test Loss : 0.395744054563027 test Accuracy 0.8432\n",
      "Train Iteration : 278 Training Loss : 0.34186670764132854 Training Accuracy 0.8600333333333333\n",
      "Test Iteration : 278 Test Loss : 0.39423240671577797 test Accuracy 0.841\n",
      "Train Iteration : 279 Training Loss : 0.33982460648985446 Training Accuracy 0.8634\n",
      "Test Iteration : 279 Test Loss : 0.3936180070107643 test Accuracy 0.8435\n",
      "Train Iteration : 280 Training Loss : 0.3381246024165038 Training Accuracy 0.8637166666666667\n",
      "Test Iteration : 280 Test Loss : 0.391444931305321 test Accuracy 0.8435\n",
      "Train Iteration : 281 Training Loss : 0.33847718222667617 Training Accuracy 0.8624333333333334\n",
      "Test Iteration : 281 Test Loss : 0.39159830121686057 test Accuracy 0.8423\n",
      "Train Iteration : 282 Training Loss : 0.3390350918045002 Training Accuracy 0.8640833333333333\n",
      "Test Iteration : 282 Test Loss : 0.39333810541652164 test Accuracy 0.8439\n",
      "Train Iteration : 283 Training Loss : 0.33793845065977757 Training Accuracy 0.8623666666666666\n",
      "Test Iteration : 283 Test Loss : 0.39113960621830496 test Accuracy 0.8423\n",
      "Train Iteration : 284 Training Loss : 0.336365608291675 Training Accuracy 0.8643833333333333\n",
      "Test Iteration : 284 Test Loss : 0.39048418515833433 test Accuracy 0.8446\n",
      "Train Iteration : 285 Training Loss : 0.3359704639388754 Training Accuracy 0.8646833333333334\n",
      "Test Iteration : 285 Test Loss : 0.3902161936139791 test Accuracy 0.8443\n",
      "Train Iteration : 286 Training Loss : 0.3363588310556172 Training Accuracy 0.8633166666666666\n",
      "Test Iteration : 286 Test Loss : 0.39018329080865183 test Accuracy 0.8426\n",
      "Train Iteration : 287 Training Loss : 0.33605805289310764 Training Accuracy 0.8652333333333333\n",
      "Test Iteration : 287 Test Loss : 0.3909294895825765 test Accuracy 0.8445\n",
      "Train Iteration : 288 Training Loss : 0.33481516766391684 Training Accuracy 0.8639833333333333\n",
      "Test Iteration : 288 Test Loss : 0.3889490003740759 test Accuracy 0.843\n",
      "Train Iteration : 289 Training Loss : 0.3338382232043424 Training Accuracy 0.8651333333333333\n",
      "Test Iteration : 289 Test Loss : 0.38846953626764835 test Accuracy 0.8445\n",
      "Train Iteration : 290 Training Loss : 0.3337222857885938 Training Accuracy 0.8656\n",
      "Test Iteration : 290 Test Loss : 0.3887391663441037 test Accuracy 0.845\n",
      "Train Iteration : 291 Training Loss : 0.3337425210577563 Training Accuracy 0.8641333333333333\n",
      "Test Iteration : 291 Test Loss : 0.3881929372161444 test Accuracy 0.8428\n",
      "Train Iteration : 292 Training Loss : 0.333172214713303 Training Accuracy 0.8660333333333333\n",
      "Test Iteration : 292 Test Loss : 0.3885061573625712 test Accuracy 0.8448\n",
      "Train Iteration : 293 Training Loss : 0.3321952214210744 Training Accuracy 0.8654166666666666\n",
      "Test Iteration : 293 Test Loss : 0.38710120985344215 test Accuracy 0.8437\n",
      "Train Iteration : 294 Training Loss : 0.3315593714560973 Training Accuracy 0.8658166666666667\n",
      "Test Iteration : 294 Test Loss : 0.38675719229693667 test Accuracy 0.8442\n",
      "Train Iteration : 295 Training Loss : 0.33139433480701724 Training Accuracy 0.8663333333333333\n",
      "Test Iteration : 295 Test Loss : 0.38708884943415506 test Accuracy 0.8455\n",
      "Train Iteration : 296 Training Loss : 0.3312029095073384 Training Accuracy 0.8654166666666666\n",
      "Test Iteration : 296 Test Loss : 0.386391536485863 test Accuracy 0.8435\n",
      "Train Iteration : 297 Training Loss : 0.3306579328490004 Training Accuracy 0.8667166666666667\n",
      "Test Iteration : 297 Test Loss : 0.3867014758772222 test Accuracy 0.8459\n",
      "Train Iteration : 298 Training Loss : 0.32990112750711265 Training Accuracy 0.8661\n",
      "Test Iteration : 298 Test Loss : 0.3855933002086626 test Accuracy 0.845\n",
      "Train Iteration : 299 Training Loss : 0.3293406071045843 Training Accuracy 0.8664833333333334\n",
      "Test Iteration : 299 Test Loss : 0.38544349538464584 test Accuracy 0.8459\n",
      "Train Iteration : 300 Training Loss : 0.3290783483489241 Training Accuracy 0.8671333333333333\n",
      "Test Iteration : 300 Test Loss : 0.38539510169021307 test Accuracy 0.845\n",
      "Train Iteration : 301 Training Loss : 0.32888927362899084 Training Accuracy 0.86655\n",
      "Test Iteration : 301 Test Loss : 0.38508366547181866 test Accuracy 0.8463\n",
      "Train Iteration : 302 Training Loss : 0.32864923298207793 Training Accuracy 0.8675833333333334\n",
      "Test Iteration : 302 Test Loss : 0.3853024381199471 test Accuracy 0.8451\n",
      "Train Iteration : 303 Training Loss : 0.32829717107330586 Training Accuracy 0.8670666666666667\n",
      "Test Iteration : 303 Test Loss : 0.3849505928459206 test Accuracy 0.8461\n",
      "Train Iteration : 304 Training Loss : 0.32835835546189257 Training Accuracy 0.8672833333333333\n",
      "Test Iteration : 304 Test Loss : 0.3850407883660807 test Accuracy 0.8455\n",
      "Train Iteration : 305 Training Loss : 0.3287916352277777 Training Accuracy 0.8673333333333333\n",
      "Test Iteration : 305 Test Loss : 0.38609819102326626 test Accuracy 0.8464\n",
      "Train Iteration : 306 Training Loss : 0.3301453048523768 Training Accuracy 0.8665166666666667\n",
      "Test Iteration : 306 Test Loss : 0.386771620298501 test Accuracy 0.8459\n",
      "Train Iteration : 307 Training Loss : 0.33121542231895534 Training Accuracy 0.8664\n",
      "Test Iteration : 307 Test Loss : 0.38916583055860743 test Accuracy 0.846\n",
      "Train Iteration : 308 Training Loss : 0.3320834041666118 Training Accuracy 0.86555\n",
      "Test Iteration : 308 Test Loss : 0.388875148958785 test Accuracy 0.8454\n",
      "Train Iteration : 309 Training Loss : 0.3297307677572389 Training Accuracy 0.8668\n",
      "Test Iteration : 309 Test Loss : 0.3877564051940079 test Accuracy 0.8457\n",
      "Train Iteration : 310 Training Loss : 0.32693925600460394 Training Accuracy 0.8678833333333333\n",
      "Test Iteration : 310 Test Loss : 0.3841948653944575 test Accuracy 0.8459\n",
      "Train Iteration : 311 Training Loss : 0.3244547870620868 Training Accuracy 0.8683833333333333\n",
      "Test Iteration : 311 Test Loss : 0.38198969536896266 test Accuracy 0.847\n",
      "Train Iteration : 312 Training Loss : 0.3243027038699591 Training Accuracy 0.8692666666666666\n",
      "Test Iteration : 312 Test Loss : 0.3824129801211727 test Accuracy 0.8479\n",
      "Train Iteration : 313 Training Loss : 0.3253931727032056 Training Accuracy 0.86815\n",
      "Test Iteration : 313 Test Loss : 0.38290997858388964 test Accuracy 0.8457\n",
      "Train Iteration : 314 Training Loss : 0.32573939720694456 Training Accuracy 0.8684833333333334\n",
      "Test Iteration : 314 Test Loss : 0.3844450178465854 test Accuracy 0.8471\n",
      "Train Iteration : 315 Training Loss : 0.3249920461856913 Training Accuracy 0.8685166666666667\n",
      "Test Iteration : 315 Test Loss : 0.3831501565417846 test Accuracy 0.8469\n",
      "Train Iteration : 316 Training Loss : 0.32302090074809264 Training Accuracy 0.86925\n",
      "Test Iteration : 316 Test Loss : 0.3815562739396498 test Accuracy 0.8476\n",
      "Train Iteration : 317 Training Loss : 0.3219242840314716 Training Accuracy 0.86955\n",
      "Test Iteration : 317 Test Loss : 0.380784940007746 test Accuracy 0.8473\n",
      "Train Iteration : 318 Training Loss : 0.3218185768775865 Training Accuracy 0.8689\n",
      "Test Iteration : 318 Test Loss : 0.38014100498771086 test Accuracy 0.8451\n",
      "Train Iteration : 319 Training Loss : 0.3221258567092689 Training Accuracy 0.87025\n",
      "Test Iteration : 319 Test Loss : 0.38133023719594844 test Accuracy 0.8481\n",
      "Train Iteration : 320 Training Loss : 0.3220652114970852 Training Accuracy 0.86935\n",
      "Test Iteration : 320 Test Loss : 0.38075819622667395 test Accuracy 0.8466\n",
      "Train Iteration : 321 Training Loss : 0.32113244162290694 Training Accuracy 0.87015\n",
      "Test Iteration : 321 Test Loss : 0.3802850715086046 test Accuracy 0.849\n",
      "Train Iteration : 322 Training Loss : 0.32017461833036487 Training Accuracy 0.8705166666666667\n",
      "Test Iteration : 322 Test Loss : 0.379433793970585 test Accuracy 0.8467\n",
      "Train Iteration : 323 Training Loss : 0.31944784293535283 Training Accuracy 0.87005\n",
      "Test Iteration : 323 Test Loss : 0.3785262096232302 test Accuracy 0.8478\n",
      "Train Iteration : 324 Training Loss : 0.31921509655443514 Training Accuracy 0.8706833333333334\n",
      "Test Iteration : 324 Test Loss : 0.3788910061343782 test Accuracy 0.8488\n",
      "Train Iteration : 325 Training Loss : 0.3191211629990731 Training Accuracy 0.8704\n",
      "Test Iteration : 325 Test Loss : 0.37845164867133363 test Accuracy 0.8462\n",
      "Train Iteration : 326 Training Loss : 0.3188305275702728 Training Accuracy 0.8712833333333333\n",
      "Test Iteration : 326 Test Loss : 0.37880839297586977 test Accuracy 0.8483\n",
      "Train Iteration : 327 Training Loss : 0.3183018643119805 Training Accuracy 0.8709\n",
      "Test Iteration : 327 Test Loss : 0.3780413960205052 test Accuracy 0.8474\n",
      "Train Iteration : 328 Training Loss : 0.31755985263877945 Training Accuracy 0.8710666666666667\n",
      "Test Iteration : 328 Test Loss : 0.3776125954132217 test Accuracy 0.8486\n",
      "Train Iteration : 329 Training Loss : 0.3169643506066995 Training Accuracy 0.8714666666666666\n",
      "Test Iteration : 329 Test Loss : 0.3772045081528123 test Accuracy 0.8486\n",
      "Train Iteration : 330 Training Loss : 0.3165573718830411 Training Accuracy 0.87155\n",
      "Test Iteration : 330 Test Loss : 0.3766339130201489 test Accuracy 0.8483\n",
      "Train Iteration : 331 Training Loss : 0.3163036776997045 Training Accuracy 0.8717333333333334\n",
      "Test Iteration : 331 Test Loss : 0.3770199964044341 test Accuracy 0.8493\n",
      "Train Iteration : 332 Training Loss : 0.3160485650541632 Training Accuracy 0.8716333333333334\n",
      "Test Iteration : 332 Test Loss : 0.37634194322721454 test Accuracy 0.8474\n",
      "Train Iteration : 333 Training Loss : 0.3157060844702619 Training Accuracy 0.8725166666666667\n",
      "Test Iteration : 333 Test Loss : 0.3766910468403154 test Accuracy 0.8499\n",
      "Train Iteration : 334 Training Loss : 0.31526120566616755 Training Accuracy 0.8721166666666667\n",
      "Test Iteration : 334 Test Loss : 0.37600723507668965 test Accuracy 0.8478\n",
      "Train Iteration : 335 Training Loss : 0.3147181943566304 Training Accuracy 0.8725333333333334\n",
      "Test Iteration : 335 Test Loss : 0.3758151340364569 test Accuracy 0.8496\n",
      "Train Iteration : 336 Training Loss : 0.31419236147433904 Training Accuracy 0.8723\n",
      "Test Iteration : 336 Test Loss : 0.37540074274886487 test Accuracy 0.8488\n",
      "Train Iteration : 337 Training Loss : 0.31369223679017305 Training Accuracy 0.8728666666666667\n",
      "Test Iteration : 337 Test Loss : 0.3749454814829357 test Accuracy 0.8503\n",
      "Train Iteration : 338 Training Loss : 0.3132741061579224 Training Accuracy 0.8729\n",
      "Test Iteration : 338 Test Loss : 0.374906068566686 test Accuracy 0.8502\n",
      "Train Iteration : 339 Training Loss : 0.31292093546524447 Training Accuracy 0.8727333333333334\n",
      "Test Iteration : 339 Test Loss : 0.37442702524932653 test Accuracy 0.8493\n",
      "Train Iteration : 340 Training Loss : 0.31261554847169853 Training Accuracy 0.87345\n",
      "Test Iteration : 340 Test Loss : 0.3746525103292901 test Accuracy 0.8508\n",
      "Train Iteration : 341 Training Loss : 0.3123225204472078 Training Accuracy 0.8727833333333334\n",
      "Test Iteration : 341 Test Loss : 0.37408350582217437 test Accuracy 0.8491\n",
      "Train Iteration : 342 Training Loss : 0.3120507699530516 Training Accuracy 0.8736\n",
      "Test Iteration : 342 Test Loss : 0.37446996880689803 test Accuracy 0.8507\n",
      "Train Iteration : 343 Training Loss : 0.31177982203543325 Training Accuracy 0.87365\n",
      "Test Iteration : 343 Test Loss : 0.3738137970759641 test Accuracy 0.8491\n",
      "Train Iteration : 344 Training Loss : 0.311520701747604 Training Accuracy 0.8739666666666667\n",
      "Test Iteration : 344 Test Loss : 0.37426278782075934 test Accuracy 0.8517\n",
      "Train Iteration : 345 Training Loss : 0.3112811994127034 Training Accuracy 0.8740833333333333\n",
      "Test Iteration : 345 Test Loss : 0.37359108432358273 test Accuracy 0.8494\n",
      "Train Iteration : 346 Training Loss : 0.31103010268594095 Training Accuracy 0.8743833333333333\n",
      "Test Iteration : 346 Test Loss : 0.37404543813824687 test Accuracy 0.8514\n",
      "Train Iteration : 347 Training Loss : 0.3108390701766047 Training Accuracy 0.8744333333333333\n",
      "Test Iteration : 347 Test Loss : 0.3734307203556466 test Accuracy 0.8494\n",
      "Train Iteration : 348 Training Loss : 0.3105967166744026 Training Accuracy 0.87475\n",
      "Test Iteration : 348 Test Loss : 0.37391077956438445 test Accuracy 0.8527\n",
      "Train Iteration : 349 Training Loss : 0.31043702087413294 Training Accuracy 0.8744833333333333\n",
      "Test Iteration : 349 Test Loss : 0.37326782705424355 test Accuracy 0.8494\n",
      "Train Iteration : 350 Training Loss : 0.3101429693602085 Training Accuracy 0.8751333333333333\n",
      "Test Iteration : 350 Test Loss : 0.3737985799862232 test Accuracy 0.8526\n",
      "Train Iteration : 351 Training Loss : 0.3099084704137118 Training Accuracy 0.8747166666666667\n",
      "Test Iteration : 351 Test Loss : 0.37297926439850443 test Accuracy 0.8495\n",
      "Train Iteration : 352 Training Loss : 0.30946273514553024 Training Accuracy 0.8753833333333333\n",
      "Test Iteration : 352 Test Loss : 0.3734387907600997 test Accuracy 0.8531\n",
      "Train Iteration : 353 Training Loss : 0.3090223078850629 Training Accuracy 0.8750833333333333\n",
      "Test Iteration : 353 Test Loss : 0.3723755521804872 test Accuracy 0.8502\n",
      "Train Iteration : 354 Training Loss : 0.308378848969766 Training Accuracy 0.8756833333333334\n",
      "Test Iteration : 354 Test Loss : 0.37260322134429924 test Accuracy 0.8535\n",
      "Train Iteration : 355 Training Loss : 0.30772249907919985 Training Accuracy 0.8759\n",
      "Test Iteration : 355 Test Loss : 0.37140736167751887 test Accuracy 0.851\n",
      "Train Iteration : 356 Training Loss : 0.30702496724578093 Training Accuracy 0.8759333333333333\n",
      "Test Iteration : 356 Test Loss : 0.3714666032269307 test Accuracy 0.8534\n",
      "Train Iteration : 357 Training Loss : 0.30637374122140815 Training Accuracy 0.8758666666666667\n",
      "Test Iteration : 357 Test Loss : 0.3703989862481399 test Accuracy 0.8521\n",
      "Train Iteration : 358 Training Loss : 0.30583761719143265 Training Accuracy 0.8761833333333333\n",
      "Test Iteration : 358 Test Loss : 0.37054440415547674 test Accuracy 0.8539\n",
      "Train Iteration : 359 Training Loss : 0.30542136792520014 Training Accuracy 0.8762833333333333\n",
      "Test Iteration : 359 Test Loss : 0.36974715952446935 test Accuracy 0.8529\n",
      "Train Iteration : 360 Training Loss : 0.30519025493808527 Training Accuracy 0.8766333333333334\n",
      "Test Iteration : 360 Test Loss : 0.3702127338617023 test Accuracy 0.8533\n",
      "Train Iteration : 361 Training Loss : 0.30519779430985866 Training Accuracy 0.8765166666666667\n",
      "Test Iteration : 361 Test Loss : 0.36977142657087914 test Accuracy 0.8532\n",
      "Train Iteration : 362 Training Loss : 0.3056261625978905 Training Accuracy 0.877\n",
      "Test Iteration : 362 Test Loss : 0.37106917573474163 test Accuracy 0.8534\n",
      "Train Iteration : 363 Training Loss : 0.3066499800177746 Training Accuracy 0.8753833333333333\n",
      "Test Iteration : 363 Test Loss : 0.3713945249189211 test Accuracy 0.8509\n",
      "Train Iteration : 364 Training Loss : 0.30869791635391847 Training Accuracy 0.8764333333333333\n",
      "Test Iteration : 364 Test Loss : 0.37476180049446944 test Accuracy 0.8515\n",
      "Train Iteration : 365 Training Loss : 0.3109323461186705 Training Accuracy 0.8730666666666667\n",
      "Test Iteration : 365 Test Loss : 0.37580010066412645 test Accuracy 0.8485\n",
      "Train Iteration : 366 Training Loss : 0.31257767008637005 Training Accuracy 0.8751\n",
      "Test Iteration : 366 Test Loss : 0.3792538389662296 test Accuracy 0.8512\n",
      "Train Iteration : 367 Training Loss : 0.30988912410100233 Training Accuracy 0.8733833333333333\n",
      "Test Iteration : 367 Test Loss : 0.3748928002863796 test Accuracy 0.8494\n",
      "Train Iteration : 368 Training Loss : 0.30492263721011975 Training Accuracy 0.8775666666666667\n",
      "Test Iteration : 368 Test Loss : 0.37129912392380776 test Accuracy 0.8536\n",
      "Train Iteration : 369 Training Loss : 0.30174262139903246 Training Accuracy 0.8780666666666667\n",
      "Test Iteration : 369 Test Loss : 0.3673279223046104 test Accuracy 0.8539\n",
      "Train Iteration : 370 Training Loss : 0.3030404748473553 Training Accuracy 0.8778333333333334\n",
      "Test Iteration : 370 Test Loss : 0.3690507864459839 test Accuracy 0.8535\n",
      "Train Iteration : 371 Training Loss : 0.3059899005548688 Training Accuracy 0.8774333333333333\n",
      "Test Iteration : 371 Test Loss : 0.3727229886719305 test Accuracy 0.8532\n",
      "Train Iteration : 372 Training Loss : 0.30498001723821616 Training Accuracy 0.87655\n",
      "Test Iteration : 372 Test Loss : 0.3712945898608475 test Accuracy 0.8526\n",
      "Train Iteration : 373 Training Loss : 0.30212017645550976 Training Accuracy 0.8786833333333334\n",
      "Test Iteration : 373 Test Loss : 0.36883113021448144 test Accuracy 0.8539\n",
      "Train Iteration : 374 Training Loss : 0.30042466316097255 Training Accuracy 0.8791\n",
      "Test Iteration : 374 Test Loss : 0.3674175796661101 test Accuracy 0.8555\n",
      "Train Iteration : 375 Training Loss : 0.30181322463615845 Training Accuracy 0.8772333333333333\n",
      "Test Iteration : 375 Test Loss : 0.3680820421329966 test Accuracy 0.8532\n",
      "Train Iteration : 376 Training Loss : 0.30325966157546597 Training Accuracy 0.8782166666666666\n",
      "Test Iteration : 376 Test Loss : 0.37116006570186266 test Accuracy 0.8553\n",
      "Train Iteration : 377 Training Loss : 0.3019013961175951 Training Accuracy 0.8771166666666667\n",
      "Test Iteration : 377 Test Loss : 0.3682426972292745 test Accuracy 0.853\n",
      "Train Iteration : 378 Training Loss : 0.30004681803588457 Training Accuracy 0.8795\n",
      "Test Iteration : 378 Test Loss : 0.3678439900738429 test Accuracy 0.8563\n",
      "Train Iteration : 379 Training Loss : 0.30023713329124124 Training Accuracy 0.8789333333333333\n",
      "Test Iteration : 379 Test Loss : 0.36729933503655376 test Accuracy 0.8553\n",
      "Train Iteration : 380 Training Loss : 0.3013524382940299 Training Accuracy 0.8781\n",
      "Test Iteration : 380 Test Loss : 0.3690878686824803 test Accuracy 0.8544\n",
      "Train Iteration : 381 Training Loss : 0.3019349576724253 Training Accuracy 0.8785833333333334\n",
      "Test Iteration : 381 Test Loss : 0.36958167612434734 test Accuracy 0.8547\n",
      "Train Iteration : 382 Training Loss : 0.29950182533394093 Training Accuracy 0.87965\n",
      "Test Iteration : 382 Test Loss : 0.36765190111442736 test Accuracy 0.8543\n",
      "Train Iteration : 383 Training Loss : 0.29852315566526755 Training Accuracy 0.8786333333333334\n",
      "Test Iteration : 383 Test Loss : 0.365956547531733 test Accuracy 0.8543\n",
      "Train Iteration : 384 Training Loss : 0.2990279028699389 Training Accuracy 0.8795333333333333\n",
      "Test Iteration : 384 Test Loss : 0.3679149537926458 test Accuracy 0.8558\n",
      "Train Iteration : 385 Training Loss : 0.298675154604537 Training Accuracy 0.8784\n",
      "Test Iteration : 385 Test Loss : 0.36636434239258286 test Accuracy 0.8543\n",
      "Train Iteration : 386 Training Loss : 0.2970666152362342 Training Accuracy 0.8800333333333333\n",
      "Test Iteration : 386 Test Loss : 0.36604074836610156 test Accuracy 0.8568\n",
      "Train Iteration : 387 Training Loss : 0.2957207399497822 Training Accuracy 0.88045\n",
      "Test Iteration : 387 Test Loss : 0.36411576419254565 test Accuracy 0.8572\n",
      "Train Iteration : 388 Training Loss : 0.29575874168466265 Training Accuracy 0.8806833333333334\n",
      "Test Iteration : 388 Test Loss : 0.3643521971863891 test Accuracy 0.8578\n",
      "Train Iteration : 389 Training Loss : 0.2959842888310875 Training Accuracy 0.8806\n",
      "Test Iteration : 389 Test Loss : 0.3650182329238986 test Accuracy 0.8568\n",
      "Train Iteration : 390 Training Loss : 0.29513663640466853 Training Accuracy 0.8807333333333334\n",
      "Test Iteration : 390 Test Loss : 0.3638236820159807 test Accuracy 0.8571\n",
      "Train Iteration : 391 Training Loss : 0.29415324807035337 Training Accuracy 0.8811833333333333\n",
      "Test Iteration : 391 Test Loss : 0.3632080768921862 test Accuracy 0.8568\n",
      "Train Iteration : 392 Training Loss : 0.2940185417182985 Training Accuracy 0.881\n",
      "Test Iteration : 392 Test Loss : 0.3631996916790099 test Accuracy 0.8571\n",
      "Train Iteration : 393 Training Loss : 0.29429284564458796 Training Accuracy 0.8814166666666666\n",
      "Test Iteration : 393 Test Loss : 0.36352149805229894 test Accuracy 0.8585\n",
      "Train Iteration : 394 Training Loss : 0.2942416867880672 Training Accuracy 0.8814166666666666\n",
      "Test Iteration : 394 Test Loss : 0.36369778354704635 test Accuracy 0.8572\n",
      "Train Iteration : 395 Training Loss : 0.2937941344299783 Training Accuracy 0.8820333333333333\n",
      "Test Iteration : 395 Test Loss : 0.3636268988452244 test Accuracy 0.8579\n",
      "Train Iteration : 396 Training Loss : 0.29407647211340504 Training Accuracy 0.8809166666666667\n",
      "Test Iteration : 396 Test Loss : 0.3632859751705267 test Accuracy 0.8564\n",
      "Train Iteration : 397 Training Loss : 0.29531877277300705 Training Accuracy 0.8813666666666666\n",
      "Test Iteration : 397 Test Loss : 0.3659051461659715 test Accuracy 0.8562\n",
      "Train Iteration : 398 Training Loss : 0.2967407164684243 Training Accuracy 0.8790833333333333\n",
      "Test Iteration : 398 Test Loss : 0.36585032580304544 test Accuracy 0.8551\n",
      "Train Iteration : 399 Training Loss : 0.2986158676771874 Training Accuracy 0.8805333333333333\n"
     ]
    }
   ],
   "source": [
    "# # for easy multiplication now the shape of X_train(784,60000) and X_test(784,10000)\n",
    "X_train = tf.convert_to_tensor(tr_x.T, dtype=tf.float64)\n",
    "X_test  = tf.convert_to_tensor(te_x.T,dtype= tf.float64)\n",
    "tr_y =    tf.convert_to_tensor(tr_y ,dtype= tf.float64)\n",
    "te_y =    tf.convert_to_tensor(te_y ,dtype= tf.float64)\n",
    "\n",
    "no_of_neurons_layer1 = 300\n",
    "no_of_neurons_layer2 = 100\n",
    "#describes the number of types of clothes(10)\n",
    "no_of_output_units  = 10\n",
    "no_of_features = X_train.shape[0]\n",
    "\n",
    "# define the weight whos shape is as the no_of_neurons ; but the weights cannot be same . because if we give same weights then the layers won't learn properly to produce different outputs\n",
    "# Multiply with small factor , its just a way to initialize the weights, we can experiment keeping (0.01) or removing (0.01)\n",
    "W1 = tf.Variable(tf.random.normal(shape=(no_of_neurons_layer1, no_of_features), dtype=tf.float64) * 0.01)\n",
    "#define the bais as vector of zeros \n",
    "b1 = tf.Variable(tf.zeros((no_of_neurons_layer1, 1),dtype=tf.float64))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal(shape=(no_of_neurons_layer2, no_of_neurons_layer1), dtype=tf.float64) * 0.01)\n",
    "b2 = tf.Variable(tf.zeros((no_of_neurons_layer2, 1),dtype=tf.float64))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal(shape=(no_of_output_units, no_of_neurons_layer2), dtype=tf.float64) * 0.01)\n",
    "b3 =tf.Variable(tf.zeros((no_of_output_units, 1),dtype=tf.float64))\n",
    "\n",
    "train_loss_list , train_acc_list , test_loss_list, test_acc_list = gradient_descent(X_train,tr_y, X_test,te_y, W1 , b1 , W2 , b2 , W3, b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6hmJsPFXDPt"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(test_loss_list)\n",
    "plt.title(\"loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgyD7cV-XDR2"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list)\n",
    "plt.plot(test_acc_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'test'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI4ktYi3lIhq"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(train_acc_list))\n",
    "plt.plot(np.log(test_acc_list))\n",
    "plt.plot(np.log(train_loss_list))\n",
    "plt.plot(np.log(test_loss_list))\n",
    "plt.title(\"Accuracy/Loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy/Loss\")\n",
    "plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMMU8O+bTHiONORdl1bumwz",
   "collapsed_sections": [],
   "name": "Question1_1_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
