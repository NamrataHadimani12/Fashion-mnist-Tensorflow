{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4330,
     "status": "ok",
     "timestamp": 1618574445710,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "4nTqXDbvuVqI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5634,
     "status": "ok",
     "timestamp": 1618574447030,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "_Ak5HgqQuvz1",
    "outputId": "58d32bfe-e687-447d-ae91-1d3a4b67c91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5633,
     "status": "ok",
     "timestamp": 1618574447032,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "ynkIzY1HEic9"
   },
   "outputs": [],
   "source": [
    "# gives the probability values asscoiated with the class\n",
    "def softmax_activation(X):\n",
    "  expo = tf.exp(X - tf.reduce_max(X, axis=0))\n",
    "  s = tf.reduce_sum(expo, axis=0, keepdims=True)\n",
    "  output =  expo / s\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6470,
     "status": "ok",
     "timestamp": 1618574447871,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "favl0GGhEfeH"
   },
   "outputs": [],
   "source": [
    "def forward_pass( X , W1 , b1, W2 , b2):\n",
    "  #push all the weights and data along with the bias through the first layer\n",
    "  hypo_1 = tf.matmul(W1 , X) + b1\n",
    "  relu_act1 = tf.nn.relu(hypo_1)\n",
    "\n",
    "  hypo_2 = tf.matmul(W2 , relu_act1 ) + b2\n",
    "  pred_output = softmax_activation(hypo_2)\n",
    "  return pred_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6468,
     "status": "ok",
     "timestamp": 1618574447872,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "C7M9wPmKEYax"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(tr_y , predictedYProb):\n",
    "  #Element wise\n",
    "  reduce_sum = -tf.reduce_sum(tr_y * tf.math.log(predictedYProb),axis = 0)\n",
    "  loss =  tf.cast(tf.reduce_mean(reduce_sum), tf.float64)\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6468,
     "status": "ok",
     "timestamp": 1618574447874,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "OtyxqaUGEYfx"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y , predictedYProb):\n",
    "  predictedYValues = tf.round(predictedYProb)  \n",
    "  #we need only those values whose actualy value being 1 matches the predicted value being 1\n",
    "  pred_correction = tf.cast(tf.equal(np.argmax(predictedYValues, axis = 0),np.argmax(Y, axis = 0)),tf.float64)  \n",
    "  accuracy_sum = tf.reduce_sum(pred_correction)\n",
    "  acc_score = accuracy_sum / Y.shape[1]\n",
    "  return acc_score\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6466,
     "status": "ok",
     "timestamp": 1618574447875,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "jK0dwEPPOIiw"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(Xtrain,Ytrain, Xtest, Ytest, W1 , b1 , W2 , b2):\n",
    "  learning_rate = 0.001\n",
    "  adam_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "  train_loss_list =[]\n",
    "  train_acc_list = []\n",
    "\n",
    "  test_loss_list = []\n",
    "  test_acc_list = []\n",
    "\n",
    "  no_of_iter = 500\n",
    "\n",
    "  for i in range(no_of_iter + 1):\n",
    "    #GradientTape will record all the operation till it encounters gradient\n",
    "    with tf.GradientTape() as tape:\n",
    "      train_predictedYProb  = forward_pass(Xtrain, W1 , b1 , W2 , b2 )\n",
    "      train_loss = cross_entropy(Ytrain ,train_predictedYProb)\n",
    "\n",
    "    gradients = tape.gradient(train_loss,[W1 , b1 , W2 , b2])\n",
    "    train_accuracy  = calculate_accuracy(Ytrain ,train_predictedYProb)\n",
    "    \n",
    "    test_predictedYProb  = forward_pass(Xtest, W1 , b1 , W2 , b2 )\n",
    "    test_loss = cross_entropy(Ytest ,test_predictedYProb)\n",
    "    test_accuracy = calculate_accuracy(Ytest ,test_predictedYProb)\n",
    "    \n",
    "    train_loss_list.append(train_loss.numpy())\n",
    "    train_acc_list.append(train_accuracy.numpy())\n",
    "    test_loss_list.append(test_loss.numpy())\n",
    "    test_acc_list.append(test_accuracy.numpy())\n",
    "\n",
    "    adam_optimizer.apply_gradients(zip(gradients , [W1 , b1 , W2 , b2]))\n",
    "\n",
    "    print(\"Train Iteration :\", i , \"Training Loss :\",train_loss.numpy() , \"Training Accuracy\",train_accuracy.numpy())\n",
    "    print(\"Test Iteration :\", i , \"Test Loss :\",test_loss.numpy() , \"test Accuracy\",test_accuracy.numpy())\n",
    "\n",
    "  return train_loss_list , train_acc_list , test_loss_list, test_acc_list\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52790,
     "status": "ok",
     "timestamp": 1618574494205,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "6pznaoSj9aPI",
    "outputId": "f979b3e3-8df7-4c87-bfce-cee82c0f5803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iteration : 0 Training Loss : 2.303540126709316 Training Accuracy 0.1\n",
      "Test Iteration : 0 Test Loss : 2.3035314668495457 test Accuracy 0.1\n",
      "Train Iteration : 1 Training Loss : 2.267115318151695 Training Accuracy 0.1\n",
      "Test Iteration : 1 Test Loss : 2.2672966248245343 test Accuracy 0.1\n",
      "Train Iteration : 2 Training Loss : 2.2261160732092153 Training Accuracy 0.1\n",
      "Test Iteration : 2 Test Loss : 2.2264604208778795 test Accuracy 0.1\n",
      "Train Iteration : 3 Training Loss : 2.17521541837616 Training Accuracy 0.1\n",
      "Test Iteration : 3 Test Loss : 2.1757842444868403 test Accuracy 0.1\n",
      "Train Iteration : 4 Training Loss : 2.116819940594053 Training Accuracy 0.1\n",
      "Test Iteration : 4 Test Loss : 2.117681442061837 test Accuracy 0.1\n",
      "Train Iteration : 5 Training Loss : 2.0514764249279374 Training Accuracy 0.1\n",
      "Test Iteration : 5 Test Loss : 2.052703387781309 test Accuracy 0.1\n",
      "Train Iteration : 6 Training Loss : 1.9803336776954252 Training Accuracy 0.1\n",
      "Test Iteration : 6 Test Loss : 1.9819927096737628 test Accuracy 0.1\n",
      "Train Iteration : 7 Training Loss : 1.9046482711698276 Training Accuracy 0.1\n",
      "Test Iteration : 7 Test Loss : 1.906787118401099 test Accuracy 0.1\n",
      "Train Iteration : 8 Training Loss : 1.8258098576409363 Training Accuracy 0.1\n",
      "Test Iteration : 8 Test Loss : 1.828504075917363 test Accuracy 0.1\n",
      "Train Iteration : 9 Training Loss : 1.7452644375731918 Training Accuracy 0.1\n",
      "Test Iteration : 9 Test Loss : 1.748570800990385 test Accuracy 0.1\n",
      "Train Iteration : 10 Training Loss : 1.6649945972349425 Training Accuracy 0.1\n",
      "Test Iteration : 10 Test Loss : 1.6689303837393175 test Accuracy 0.1\n",
      "Train Iteration : 11 Training Loss : 1.5869200989057959 Training Accuracy 0.10045\n",
      "Test Iteration : 11 Test Loss : 1.5914776592549784 test Accuracy 0.1007\n",
      "Train Iteration : 12 Training Loss : 1.5124954596278175 Training Accuracy 0.1037\n",
      "Test Iteration : 12 Test Loss : 1.5176347352650956 test Accuracy 0.103\n",
      "Train Iteration : 13 Training Loss : 1.4425260839186946 Training Accuracy 0.11801666666666667\n",
      "Test Iteration : 13 Test Loss : 1.4482018991650665 test Accuracy 0.1168\n",
      "Train Iteration : 14 Training Loss : 1.3771706040406748 Training Accuracy 0.14755\n",
      "Test Iteration : 14 Test Loss : 1.3833676101237884 test Accuracy 0.1442\n",
      "Train Iteration : 15 Training Loss : 1.3165989086739032 Training Accuracy 0.18691666666666668\n",
      "Test Iteration : 15 Test Loss : 1.323345700995673 test Accuracy 0.1827\n",
      "Train Iteration : 16 Training Loss : 1.2608854042593136 Training Accuracy 0.22081666666666666\n",
      "Test Iteration : 16 Test Loss : 1.2682596083700173 test Accuracy 0.2164\n",
      "Train Iteration : 17 Training Loss : 1.2098943952517496 Training Accuracy 0.2536\n",
      "Test Iteration : 17 Test Loss : 1.2179546340334746 test Accuracy 0.2503\n",
      "Train Iteration : 18 Training Loss : 1.1630808329223055 Training Accuracy 0.2868\n",
      "Test Iteration : 18 Test Loss : 1.1718662830214066 test Accuracy 0.281\n",
      "Train Iteration : 19 Training Loss : 1.1202150021773185 Training Accuracy 0.3227833333333333\n",
      "Test Iteration : 19 Test Loss : 1.1297542786725916 test Accuracy 0.3178\n",
      "Train Iteration : 20 Training Loss : 1.0813040572306507 Training Accuracy 0.358\n",
      "Test Iteration : 20 Test Loss : 1.0916073032927873 test Accuracy 0.3551\n",
      "Train Iteration : 21 Training Loss : 1.0459973155258204 Training Accuracy 0.3887833333333333\n",
      "Test Iteration : 21 Test Loss : 1.057051491993789 test Accuracy 0.3866\n",
      "Train Iteration : 22 Training Loss : 1.013742691073246 Training Accuracy 0.4138833333333333\n",
      "Test Iteration : 22 Test Loss : 1.025525063756584 test Accuracy 0.411\n",
      "Train Iteration : 23 Training Loss : 0.984229885145591 Training Accuracy 0.4338\n",
      "Test Iteration : 23 Test Loss : 0.996734938653569 test Accuracy 0.4299\n",
      "Train Iteration : 24 Training Loss : 0.9572687706079108 Training Accuracy 0.4535166666666667\n",
      "Test Iteration : 24 Test Loss : 0.9705178251388639 test Accuracy 0.4489\n",
      "Train Iteration : 25 Training Loss : 0.9326797561574588 Training Accuracy 0.4722166666666667\n",
      "Test Iteration : 25 Test Loss : 0.9466901897638358 test Accuracy 0.4667\n",
      "Train Iteration : 26 Training Loss : 0.9103100862773011 Training Accuracy 0.48895\n",
      "Test Iteration : 26 Test Loss : 0.9250629569179042 test Accuracy 0.4835\n",
      "Train Iteration : 27 Training Loss : 0.8899161137145449 Training Accuracy 0.5050833333333333\n",
      "Test Iteration : 27 Test Loss : 0.9053592596084381 test Accuracy 0.4994\n",
      "Train Iteration : 28 Training Loss : 0.8713227718115655 Training Accuracy 0.51935\n",
      "Test Iteration : 28 Test Loss : 0.8874225523298 test Accuracy 0.5131\n",
      "Train Iteration : 29 Training Loss : 0.8542922958493715 Training Accuracy 0.5324666666666666\n",
      "Test Iteration : 29 Test Loss : 0.8710547086233619 test Accuracy 0.525\n",
      "Train Iteration : 30 Training Loss : 0.8385844378405023 Training Accuracy 0.5434666666666667\n",
      "Test Iteration : 30 Test Loss : 0.8560029715417482 test Accuracy 0.5342\n",
      "Train Iteration : 31 Training Loss : 0.824114993450307 Training Accuracy 0.5524\n",
      "Test Iteration : 31 Test Loss : 0.842122755677619 test Accuracy 0.5448\n",
      "Train Iteration : 32 Training Loss : 0.8107363063533977 Training Accuracy 0.5602\n",
      "Test Iteration : 32 Test Loss : 0.829211888870224 test Accuracy 0.5529\n",
      "Train Iteration : 33 Training Loss : 0.7983128429735706 Training Accuracy 0.56755\n",
      "Test Iteration : 33 Test Loss : 0.8171453204420456 test Accuracy 0.5602\n",
      "Train Iteration : 34 Training Loss : 0.7866392929793016 Training Accuracy 0.5745833333333333\n",
      "Test Iteration : 34 Test Loss : 0.8057794322922064 test Accuracy 0.5672\n",
      "Train Iteration : 35 Training Loss : 0.7756088363915136 Training Accuracy 0.5826\n",
      "Test Iteration : 35 Test Loss : 0.795066375241853 test Accuracy 0.5731\n",
      "Train Iteration : 36 Training Loss : 0.7651153585746946 Training Accuracy 0.5895666666666667\n",
      "Test Iteration : 36 Test Loss : 0.7849067778675108 test Accuracy 0.5792\n",
      "Train Iteration : 37 Training Loss : 0.7550916822723913 Training Accuracy 0.5976\n",
      "Test Iteration : 37 Test Loss : 0.7752262480218256 test Accuracy 0.5877\n",
      "Train Iteration : 38 Training Loss : 0.7454530773010675 Training Accuracy 0.6050833333333333\n",
      "Test Iteration : 38 Test Loss : 0.765931383236461 test Accuracy 0.5949\n",
      "Train Iteration : 39 Training Loss : 0.7361324194691085 Training Accuracy 0.6121666666666666\n",
      "Test Iteration : 39 Test Loss : 0.7569280948342605 test Accuracy 0.601\n",
      "Train Iteration : 40 Training Loss : 0.7270672510358736 Training Accuracy 0.6189166666666667\n",
      "Test Iteration : 40 Test Loss : 0.7480909662069722 test Accuracy 0.6086\n",
      "Train Iteration : 41 Training Loss : 0.7182358887269935 Training Accuracy 0.62605\n",
      "Test Iteration : 41 Test Loss : 0.7393726285515121 test Accuracy 0.6167\n",
      "Train Iteration : 42 Training Loss : 0.7096178646013943 Training Accuracy 0.6334166666666666\n",
      "Test Iteration : 42 Test Loss : 0.7308273491343332 test Accuracy 0.6239\n",
      "Train Iteration : 43 Training Loss : 0.7011906720422958 Training Accuracy 0.6399833333333333\n",
      "Test Iteration : 43 Test Loss : 0.7225040087069683 test Accuracy 0.63\n",
      "Train Iteration : 44 Training Loss : 0.6929319621965663 Training Accuracy 0.6461\n",
      "Test Iteration : 44 Test Loss : 0.7143610323235807 test Accuracy 0.6368\n",
      "Train Iteration : 45 Training Loss : 0.6848609707860864 Training Accuracy 0.6527666666666667\n",
      "Test Iteration : 45 Test Loss : 0.7063972422017876 test Accuracy 0.6438\n",
      "Train Iteration : 46 Training Loss : 0.6769776686923777 Training Accuracy 0.6581333333333333\n",
      "Test Iteration : 46 Test Loss : 0.6986567793092284 test Accuracy 0.6485\n",
      "Train Iteration : 47 Training Loss : 0.66927505216472 Training Accuracy 0.6637166666666666\n",
      "Test Iteration : 47 Test Loss : 0.6911263566441413 test Accuracy 0.6553\n",
      "Train Iteration : 48 Training Loss : 0.6617469222661845 Training Accuracy 0.6696666666666666\n",
      "Test Iteration : 48 Test Loss : 0.6837484543392337 test Accuracy 0.66\n",
      "Train Iteration : 49 Training Loss : 0.6544245857474364 Training Accuracy 0.6768166666666666\n",
      "Test Iteration : 49 Test Loss : 0.6766001112967982 test Accuracy 0.6673\n",
      "Train Iteration : 50 Training Loss : 0.6473124215326519 Training Accuracy 0.6828166666666666\n",
      "Test Iteration : 50 Test Loss : 0.6697258955376089 test Accuracy 0.6737\n",
      "Train Iteration : 51 Training Loss : 0.640399699756792 Training Accuracy 0.6883\n",
      "Test Iteration : 51 Test Loss : 0.6630387289020561 test Accuracy 0.6802\n",
      "Train Iteration : 52 Training Loss : 0.6336774744308 Training Accuracy 0.69375\n",
      "Test Iteration : 52 Test Loss : 0.6565222920257711 test Accuracy 0.6853\n",
      "Train Iteration : 53 Training Loss : 0.6271578691921944 Training Accuracy 0.6985166666666667\n",
      "Test Iteration : 53 Test Loss : 0.6502393849416842 test Accuracy 0.6893\n",
      "Train Iteration : 54 Training Loss : 0.6208488031575897 Training Accuracy 0.7029666666666666\n",
      "Test Iteration : 54 Test Loss : 0.6441674028433694 test Accuracy 0.6939\n",
      "Train Iteration : 55 Training Loss : 0.6147356522695785 Training Accuracy 0.7086666666666667\n",
      "Test Iteration : 55 Test Loss : 0.6382835450578487 test Accuracy 0.6975\n",
      "Train Iteration : 56 Training Loss : 0.6088077080486917 Training Accuracy 0.7130833333333333\n",
      "Test Iteration : 56 Test Loss : 0.6326258720130545 test Accuracy 0.7004\n",
      "Train Iteration : 57 Training Loss : 0.6030686223906963 Training Accuracy 0.7179666666666666\n",
      "Test Iteration : 57 Test Loss : 0.6271723038641478 test Accuracy 0.7043\n",
      "Train Iteration : 58 Training Loss : 0.5975142979608273 Training Accuracy 0.7223166666666667\n",
      "Test Iteration : 58 Test Loss : 0.6218967466889357 test Accuracy 0.7081\n",
      "Train Iteration : 59 Training Loss : 0.5921372941115833 Training Accuracy 0.7261333333333333\n",
      "Test Iteration : 59 Test Loss : 0.6168249962657463 test Accuracy 0.7122\n",
      "Train Iteration : 60 Training Loss : 0.5869300657513964 Training Accuracy 0.7297833333333333\n",
      "Test Iteration : 60 Test Loss : 0.6119097688267405 test Accuracy 0.7152\n",
      "Train Iteration : 61 Training Loss : 0.5818920438929526 Training Accuracy 0.7329833333333333\n",
      "Test Iteration : 61 Test Loss : 0.6071366202018015 test Accuracy 0.7196\n",
      "Train Iteration : 62 Training Loss : 0.5770185913714195 Training Accuracy 0.73615\n",
      "Test Iteration : 62 Test Loss : 0.602524142245628 test Accuracy 0.7222\n",
      "Train Iteration : 63 Training Loss : 0.5722981987343897 Training Accuracy 0.73925\n",
      "Test Iteration : 63 Test Loss : 0.5980421895716432 test Accuracy 0.7258\n",
      "Train Iteration : 64 Training Loss : 0.5677244161338763 Training Accuracy 0.7425333333333334\n",
      "Test Iteration : 64 Test Loss : 0.5937114124434515 test Accuracy 0.7284\n",
      "Train Iteration : 65 Training Loss : 0.5632969024993156 Training Accuracy 0.7454\n",
      "Test Iteration : 65 Test Loss : 0.5895281605783773 test Accuracy 0.7314\n",
      "Train Iteration : 66 Training Loss : 0.5590047492430511 Training Accuracy 0.7481333333333333\n",
      "Test Iteration : 66 Test Loss : 0.5854743596580069 test Accuracy 0.7342\n",
      "Train Iteration : 67 Training Loss : 0.5548385911926783 Training Accuracy 0.7511666666666666\n",
      "Test Iteration : 67 Test Loss : 0.5815509607166148 test Accuracy 0.7365\n",
      "Train Iteration : 68 Training Loss : 0.5507970569858839 Training Accuracy 0.7539166666666667\n",
      "Test Iteration : 68 Test Loss : 0.577744450051115 test Accuracy 0.7398\n",
      "Train Iteration : 69 Training Loss : 0.546876007700211 Training Accuracy 0.756\n",
      "Test Iteration : 69 Test Loss : 0.5740776172649735 test Accuracy 0.7429\n",
      "Train Iteration : 70 Training Loss : 0.543067378556153 Training Accuracy 0.7586333333333334\n",
      "Test Iteration : 70 Test Loss : 0.5704991018982478 test Accuracy 0.7449\n",
      "Train Iteration : 71 Training Loss : 0.5393656974404492 Training Accuracy 0.7607833333333334\n",
      "Test Iteration : 71 Test Loss : 0.5670267428286426 test Accuracy 0.7473\n",
      "Train Iteration : 72 Training Loss : 0.535769203440725 Training Accuracy 0.7631333333333333\n",
      "Test Iteration : 72 Test Loss : 0.5636438229961303 test Accuracy 0.7501\n",
      "Train Iteration : 73 Training Loss : 0.5322753486245042 Training Accuracy 0.7653333333333333\n",
      "Test Iteration : 73 Test Loss : 0.5603545012744953 test Accuracy 0.7515\n",
      "Train Iteration : 74 Training Loss : 0.5288792215706825 Training Accuracy 0.7676333333333333\n",
      "Test Iteration : 74 Test Loss : 0.5571823474521207 test Accuracy 0.7533\n",
      "Train Iteration : 75 Training Loss : 0.5255788241280467 Training Accuracy 0.7696666666666667\n",
      "Test Iteration : 75 Test Loss : 0.5540916253622724 test Accuracy 0.7551\n",
      "Train Iteration : 76 Training Loss : 0.5223725762382896 Training Accuracy 0.7714\n",
      "Test Iteration : 76 Test Loss : 0.5511178485101085 test Accuracy 0.758\n",
      "Train Iteration : 77 Training Loss : 0.5192571866722232 Training Accuracy 0.7735666666666666\n",
      "Test Iteration : 77 Test Loss : 0.5482067885438399 test Accuracy 0.7596\n",
      "Train Iteration : 78 Training Loss : 0.5162300705760939 Training Accuracy 0.7759\n",
      "Test Iteration : 78 Test Loss : 0.5454184140524685 test Accuracy 0.7611\n",
      "Train Iteration : 79 Training Loss : 0.5132905243188721 Training Accuracy 0.7775166666666666\n",
      "Test Iteration : 79 Test Loss : 0.5426545545643556 test Accuracy 0.763\n",
      "Train Iteration : 80 Training Loss : 0.5104402141673551 Training Accuracy 0.77945\n",
      "Test Iteration : 80 Test Loss : 0.5400889731709132 test Accuracy 0.7639\n",
      "Train Iteration : 81 Training Loss : 0.507688444050402 Training Accuracy 0.78055\n",
      "Test Iteration : 81 Test Loss : 0.5374061781032137 test Accuracy 0.7654\n",
      "Train Iteration : 82 Training Loss : 0.5050756664181242 Training Accuracy 0.7829166666666667\n",
      "Test Iteration : 82 Test Loss : 0.5352584315585995 test Accuracy 0.7685\n",
      "Train Iteration : 83 Training Loss : 0.5027581190359791 Training Accuracy 0.7837833333333334\n",
      "Test Iteration : 83 Test Loss : 0.5326450023717444 test Accuracy 0.7689\n",
      "Train Iteration : 84 Training Loss : 0.5008494277241802 Training Accuracy 0.78595\n",
      "Test Iteration : 84 Test Loss : 0.5317540924179904 test Accuracy 0.7698\n",
      "Train Iteration : 85 Training Loss : 0.49907153679825256 Training Accuracy 0.7867666666666666\n",
      "Test Iteration : 85 Test Loss : 0.5290351098053622 test Accuracy 0.7713\n",
      "Train Iteration : 86 Training Loss : 0.4958206292485199 Training Accuracy 0.7886333333333333\n",
      "Test Iteration : 86 Test Loss : 0.5268986384777563 test Accuracy 0.7733\n",
      "Train Iteration : 87 Training Loss : 0.49313016178928204 Training Accuracy 0.7899333333333334\n",
      "Test Iteration : 87 Test Loss : 0.5241245951587218 test Accuracy 0.7747\n",
      "Train Iteration : 88 Training Loss : 0.4917763782615738 Training Accuracy 0.79075\n",
      "Test Iteration : 88 Test Loss : 0.5224213803009239 test Accuracy 0.775\n",
      "Train Iteration : 89 Training Loss : 0.48950333809316116 Training Accuracy 0.7927666666666666\n",
      "Test Iteration : 89 Test Loss : 0.521089728144284 test Accuracy 0.7757\n",
      "Train Iteration : 90 Training Loss : 0.48692537947714754 Training Accuracy 0.7935833333333333\n",
      "Test Iteration : 90 Test Loss : 0.5182818398374608 test Accuracy 0.7777\n",
      "Train Iteration : 91 Training Loss : 0.4854079323674148 Training Accuracy 0.7941666666666667\n",
      "Test Iteration : 91 Test Loss : 0.516624174750022 test Accuracy 0.7789\n",
      "Train Iteration : 92 Training Loss : 0.48354681765250884 Training Accuracy 0.79585\n",
      "Test Iteration : 92 Test Loss : 0.5155194645477758 test Accuracy 0.7803\n",
      "Train Iteration : 93 Training Loss : 0.4812582540707317 Training Accuracy 0.7964833333333333\n",
      "Test Iteration : 93 Test Loss : 0.5130253560300311 test Accuracy 0.7827\n",
      "Train Iteration : 94 Training Loss : 0.47968308499795165 Training Accuracy 0.7975\n",
      "Test Iteration : 94 Test Loss : 0.5114326173688388 test Accuracy 0.7831\n",
      "Train Iteration : 95 Training Loss : 0.47803433334816775 Training Accuracy 0.79885\n",
      "Test Iteration : 95 Test Loss : 0.5103902992353228 test Accuracy 0.7842\n",
      "Train Iteration : 96 Training Loss : 0.4759940512132405 Training Accuracy 0.7993\n",
      "Test Iteration : 96 Test Loss : 0.508150985906415 test Accuracy 0.7861\n",
      "Train Iteration : 97 Training Loss : 0.474407218828628 Training Accuracy 0.8001166666666667\n",
      "Test Iteration : 97 Test Loss : 0.5066446645430275 test Accuracy 0.7868\n",
      "Train Iteration : 98 Training Loss : 0.47290101681931873 Training Accuracy 0.8014666666666667\n",
      "Test Iteration : 98 Test Loss : 0.5056340852692183 test Accuracy 0.7879\n",
      "Train Iteration : 99 Training Loss : 0.47106942712496397 Training Accuracy 0.80175\n",
      "Test Iteration : 99 Test Loss : 0.5036252032144586 test Accuracy 0.7885\n",
      "Train Iteration : 100 Training Loss : 0.4694950031837463 Training Accuracy 0.8022\n",
      "Test Iteration : 100 Test Loss : 0.5021969172433722 test Accuracy 0.7895\n",
      "Train Iteration : 101 Training Loss : 0.46808479491658256 Training Accuracy 0.80365\n",
      "Test Iteration : 101 Test Loss : 0.5011718533636761 test Accuracy 0.7906\n",
      "Train Iteration : 102 Training Loss : 0.4664306425571956 Training Accuracy 0.8040666666666667\n",
      "Test Iteration : 102 Test Loss : 0.49937990453151343 test Accuracy 0.7914\n",
      "Train Iteration : 103 Training Loss : 0.4648859582925629 Training Accuracy 0.8046\n",
      "Test Iteration : 103 Test Loss : 0.49804609069351874 test Accuracy 0.7929\n",
      "Train Iteration : 104 Training Loss : 0.4635398258988348 Training Accuracy 0.8058833333333333\n",
      "Test Iteration : 104 Test Loss : 0.4969897295074154 test Accuracy 0.7934\n",
      "Train Iteration : 105 Training Loss : 0.4620362971832375 Training Accuracy 0.8065166666666667\n",
      "Test Iteration : 105 Test Loss : 0.49538276966643846 test Accuracy 0.793\n",
      "Train Iteration : 106 Training Loss : 0.46053397555315045 Training Accuracy 0.80735\n",
      "Test Iteration : 106 Test Loss : 0.4941178579036753 test Accuracy 0.7936\n",
      "Train Iteration : 107 Training Loss : 0.459223252971901 Training Accuracy 0.8084666666666667\n",
      "Test Iteration : 107 Test Loss : 0.4930097314736603 test Accuracy 0.7943\n",
      "Train Iteration : 108 Training Loss : 0.45785151587055395 Training Accuracy 0.8089833333333334\n",
      "Test Iteration : 108 Test Loss : 0.49157893418473814 test Accuracy 0.7948\n",
      "Train Iteration : 109 Training Loss : 0.45640494076539345 Training Accuracy 0.8102666666666667\n",
      "Test Iteration : 109 Test Loss : 0.4903744935159496 test Accuracy 0.7948\n",
      "Train Iteration : 110 Training Loss : 0.4550946572558296 Training Accuracy 0.8111333333333334\n",
      "Test Iteration : 110 Test Loss : 0.48919863571318456 test Accuracy 0.7967\n",
      "Train Iteration : 111 Training Loss : 0.45382531257730946 Training Accuracy 0.8116666666666666\n",
      "Test Iteration : 111 Test Loss : 0.48793057645864146 test Accuracy 0.7959\n",
      "Train Iteration : 112 Training Loss : 0.4524648339019659 Training Accuracy 0.8126166666666667\n",
      "Test Iteration : 112 Test Loss : 0.4868104110855411 test Accuracy 0.7978\n",
      "Train Iteration : 113 Training Loss : 0.4511419459201702 Training Accuracy 0.8134666666666667\n",
      "Test Iteration : 113 Test Loss : 0.48555417378586724 test Accuracy 0.7983\n",
      "Train Iteration : 114 Training Loss : 0.44991256694490633 Training Accuracy 0.8137\n",
      "Test Iteration : 114 Test Loss : 0.48436944212075167 test Accuracy 0.7977\n",
      "Train Iteration : 115 Training Loss : 0.4486583182864491 Training Accuracy 0.8147\n",
      "Test Iteration : 115 Test Loss : 0.4833350943166244 test Accuracy 0.7992\n",
      "Train Iteration : 116 Training Loss : 0.447359258306602 Training Accuracy 0.81525\n",
      "Test Iteration : 116 Test Loss : 0.4820599663807543 test Accuracy 0.8003\n",
      "Train Iteration : 117 Training Loss : 0.44611725670775465 Training Accuracy 0.8161\n",
      "Test Iteration : 117 Test Loss : 0.4809184232711981 test Accuracy 0.8007\n",
      "Train Iteration : 118 Training Loss : 0.44492559214488564 Training Accuracy 0.8168833333333333\n",
      "Test Iteration : 118 Test Loss : 0.4798957653293388 test Accuracy 0.8005\n",
      "Train Iteration : 119 Training Loss : 0.44370528181362895 Training Accuracy 0.817\n",
      "Test Iteration : 119 Test Loss : 0.4786780673677092 test Accuracy 0.801\n",
      "Train Iteration : 120 Training Loss : 0.4424706377328623 Training Accuracy 0.81795\n",
      "Test Iteration : 120 Test Loss : 0.477604240412775 test Accuracy 0.8015\n",
      "Train Iteration : 121 Training Loss : 0.44127781022018525 Training Accuracy 0.8185333333333333\n",
      "Test Iteration : 121 Test Loss : 0.47650455796107644 test Accuracy 0.8016\n",
      "Train Iteration : 122 Training Loss : 0.4401161958819202 Training Accuracy 0.8187333333333333\n",
      "Test Iteration : 122 Test Loss : 0.47538959619865734 test Accuracy 0.8019\n",
      "Train Iteration : 123 Training Loss : 0.43894184701259387 Training Accuracy 0.8194333333333333\n",
      "Test Iteration : 123 Test Loss : 0.47437998343038124 test Accuracy 0.8023\n",
      "Train Iteration : 124 Training Loss : 0.43775534288143697 Training Accuracy 0.8198166666666666\n",
      "Test Iteration : 124 Test Loss : 0.47324415474310816 test Accuracy 0.8028\n",
      "Train Iteration : 125 Training Loss : 0.4365911387039341 Training Accuracy 0.8205833333333333\n",
      "Test Iteration : 125 Test Loss : 0.4722048305411918 test Accuracy 0.803\n",
      "Train Iteration : 126 Training Loss : 0.43545324749698877 Training Accuracy 0.8215333333333333\n",
      "Test Iteration : 126 Test Loss : 0.47119553151971266 test Accuracy 0.803\n",
      "Train Iteration : 127 Training Loss : 0.43431627045335214 Training Accuracy 0.8216333333333333\n",
      "Test Iteration : 127 Test Loss : 0.47013134980360793 test Accuracy 0.8039\n",
      "Train Iteration : 128 Training Loss : 0.4331687041318328 Training Accuracy 0.82255\n",
      "Test Iteration : 128 Test Loss : 0.46913971211537736 test Accuracy 0.8041\n",
      "Train Iteration : 129 Training Loss : 0.4320232962431881 Training Accuracy 0.8229666666666666\n",
      "Test Iteration : 129 Test Loss : 0.4680998886673241 test Accuracy 0.8052\n",
      "Train Iteration : 130 Training Loss : 0.43089545128518225 Training Accuracy 0.8234\n",
      "Test Iteration : 130 Test Loss : 0.4670789784968523 test Accuracy 0.8054\n",
      "Train Iteration : 131 Training Loss : 0.42978012076842353 Training Accuracy 0.82415\n",
      "Test Iteration : 131 Test Loss : 0.4661030269842977 test Accuracy 0.8059\n",
      "Train Iteration : 132 Training Loss : 0.42866627799026835 Training Accuracy 0.8245\n",
      "Test Iteration : 132 Test Loss : 0.4650469930335986 test Accuracy 0.806\n",
      "Train Iteration : 133 Training Loss : 0.4275532210372957 Training Accuracy 0.8251833333333334\n",
      "Test Iteration : 133 Test Loss : 0.46410052810509334 test Accuracy 0.8073\n",
      "Train Iteration : 134 Training Loss : 0.4264566737052535 Training Accuracy 0.8258\n",
      "Test Iteration : 134 Test Loss : 0.46305887543226026 test Accuracy 0.8077\n",
      "Train Iteration : 135 Training Loss : 0.4254034617260679 Training Accuracy 0.8261166666666667\n",
      "Test Iteration : 135 Test Loss : 0.4621755054363817 test Accuracy 0.8084\n",
      "Train Iteration : 136 Training Loss : 0.4244394351905948 Training Accuracy 0.8271\n",
      "Test Iteration : 136 Test Loss : 0.4612507245840916 test Accuracy 0.8092\n",
      "Train Iteration : 137 Training Loss : 0.42365688671918383 Training Accuracy 0.8264166666666667\n",
      "Test Iteration : 137 Test Loss : 0.46070889633455053 test Accuracy 0.8088\n",
      "Train Iteration : 138 Training Loss : 0.42316750260595504 Training Accuracy 0.8279666666666666\n",
      "Test Iteration : 138 Test Loss : 0.4601577498974306 test Accuracy 0.8121\n",
      "Train Iteration : 139 Training Loss : 0.42264262099727556 Training Accuracy 0.8262\n",
      "Test Iteration : 139 Test Loss : 0.4600282740693015 test Accuracy 0.8085\n",
      "Train Iteration : 140 Training Loss : 0.4216452205310132 Training Accuracy 0.8285666666666667\n",
      "Test Iteration : 140 Test Loss : 0.45882020764562786 test Accuracy 0.8122\n",
      "Train Iteration : 141 Training Loss : 0.41981166093086675 Training Accuracy 0.8279333333333333\n",
      "Test Iteration : 141 Test Loss : 0.45736804831128064 test Accuracy 0.8086\n",
      "Train Iteration : 142 Training Loss : 0.4186002469553053 Training Accuracy 0.8289833333333333\n",
      "Test Iteration : 142 Test Loss : 0.45612494519474994 test Accuracy 0.8118\n",
      "Train Iteration : 143 Training Loss : 0.41788835547731734 Training Accuracy 0.8303333333333334\n",
      "Test Iteration : 143 Test Loss : 0.4555328932547375 test Accuracy 0.8123\n",
      "Train Iteration : 144 Training Loss : 0.41699143322863425 Training Accuracy 0.8293\n",
      "Test Iteration : 144 Test Loss : 0.45491405594837686 test Accuracy 0.811\n",
      "Train Iteration : 145 Training Loss : 0.4160514959363847 Training Accuracy 0.8306333333333333\n",
      "Test Iteration : 145 Test Loss : 0.4539236663511578 test Accuracy 0.8128\n",
      "Train Iteration : 146 Training Loss : 0.4150881493712058 Training Accuracy 0.8309\n",
      "Test Iteration : 146 Test Loss : 0.453238197911609 test Accuracy 0.8126\n",
      "Train Iteration : 147 Training Loss : 0.41392598326200974 Training Accuracy 0.8307333333333333\n",
      "Test Iteration : 147 Test Loss : 0.4521678200364986 test Accuracy 0.8132\n",
      "Train Iteration : 148 Training Loss : 0.41297582388673 Training Accuracy 0.8322666666666667\n",
      "Test Iteration : 148 Test Loss : 0.45130791973480616 test Accuracy 0.8154\n",
      "Train Iteration : 149 Training Loss : 0.41235659396328034 Training Accuracy 0.832\n",
      "Test Iteration : 149 Test Loss : 0.45096002444026445 test Accuracy 0.8146\n",
      "Train Iteration : 150 Training Loss : 0.41138457477434665 Training Accuracy 0.8326166666666667\n",
      "Test Iteration : 150 Test Loss : 0.4499943640872061 test Accuracy 0.8146\n",
      "Train Iteration : 151 Training Loss : 0.4101506816204281 Training Accuracy 0.8331\n",
      "Test Iteration : 151 Test Loss : 0.4489287938331304 test Accuracy 0.8148\n",
      "Train Iteration : 152 Training Loss : 0.40935888456171404 Training Accuracy 0.8336166666666667\n",
      "Test Iteration : 152 Test Loss : 0.44827922822257826 test Accuracy 0.815\n",
      "Train Iteration : 153 Training Loss : 0.4087466608636372 Training Accuracy 0.8339666666666666\n",
      "Test Iteration : 153 Test Loss : 0.44770727337187727 test Accuracy 0.8158\n",
      "Train Iteration : 154 Training Loss : 0.40775594164359175 Training Accuracy 0.8344666666666667\n",
      "Test Iteration : 154 Test Loss : 0.4469006835430203 test Accuracy 0.8163\n",
      "Train Iteration : 155 Training Loss : 0.4067295032982262 Training Accuracy 0.8351\n",
      "Test Iteration : 155 Test Loss : 0.4459145733362775 test Accuracy 0.8167\n",
      "Train Iteration : 156 Training Loss : 0.40600641193812165 Training Accuracy 0.8352666666666667\n",
      "Test Iteration : 156 Test Loss : 0.4453118673171198 test Accuracy 0.8159\n",
      "Train Iteration : 157 Training Loss : 0.4052312295347065 Training Accuracy 0.8356333333333333\n",
      "Test Iteration : 157 Test Loss : 0.4447077510008503 test Accuracy 0.8172\n",
      "Train Iteration : 158 Training Loss : 0.40431426951505334 Training Accuracy 0.83625\n",
      "Test Iteration : 158 Test Loss : 0.44384664480165803 test Accuracy 0.8178\n",
      "Train Iteration : 159 Training Loss : 0.4035133975508747 Training Accuracy 0.8361\n",
      "Test Iteration : 159 Test Loss : 0.4431995916172662 test Accuracy 0.8173\n",
      "Train Iteration : 160 Training Loss : 0.4027210085037288 Training Accuracy 0.8372333333333334\n",
      "Test Iteration : 160 Test Loss : 0.4425133877592825 test Accuracy 0.8184\n",
      "Train Iteration : 161 Training Loss : 0.40182153492061323 Training Accuracy 0.8375333333333334\n",
      "Test Iteration : 161 Test Loss : 0.4417197691491128 test Accuracy 0.8183\n",
      "Train Iteration : 162 Training Loss : 0.4010188124883249 Training Accuracy 0.8376166666666667\n",
      "Test Iteration : 162 Test Loss : 0.4410569664350382 test Accuracy 0.8183\n",
      "Train Iteration : 163 Training Loss : 0.40031409788915745 Training Accuracy 0.8388166666666667\n",
      "Test Iteration : 163 Test Loss : 0.4404456364603622 test Accuracy 0.8192\n",
      "Train Iteration : 164 Training Loss : 0.39948991060499706 Training Accuracy 0.8381833333333333\n",
      "Test Iteration : 164 Test Loss : 0.4397628568560385 test Accuracy 0.8191\n",
      "Train Iteration : 165 Training Loss : 0.3986028346238847 Training Accuracy 0.8394833333333334\n",
      "Test Iteration : 165 Test Loss : 0.43898649554523717 test Accuracy 0.8203\n",
      "Train Iteration : 166 Training Loss : 0.3978291545312315 Training Accuracy 0.8394\n",
      "Test Iteration : 166 Test Loss : 0.4383439985980214 test Accuracy 0.8203\n",
      "Train Iteration : 167 Training Loss : 0.39709880655821506 Training Accuracy 0.8393\n",
      "Test Iteration : 167 Test Loss : 0.43772652225469283 test Accuracy 0.8203\n",
      "Train Iteration : 168 Training Loss : 0.39630293308446907 Training Accuracy 0.8404333333333334\n",
      "Test Iteration : 168 Test Loss : 0.43704751965942396 test Accuracy 0.8212\n",
      "Train Iteration : 169 Training Loss : 0.3955079633306801 Training Accuracy 0.8402\n",
      "Test Iteration : 169 Test Loss : 0.43640232047841065 test Accuracy 0.8208\n",
      "Train Iteration : 170 Training Loss : 0.3947619918291961 Training Accuracy 0.8406833333333333\n",
      "Test Iteration : 170 Test Loss : 0.4357585510148101 test Accuracy 0.8217\n",
      "Train Iteration : 171 Training Loss : 0.39399209053928036 Training Accuracy 0.84085\n",
      "Test Iteration : 171 Test Loss : 0.4351383245564736 test Accuracy 0.8214\n",
      "Train Iteration : 172 Training Loss : 0.39318743970931525 Training Accuracy 0.8412833333333334\n",
      "Test Iteration : 172 Test Loss : 0.4344503279058301 test Accuracy 0.8224\n",
      "Train Iteration : 173 Training Loss : 0.3924238307863331 Training Accuracy 0.84175\n",
      "Test Iteration : 173 Test Loss : 0.4338245813342548 test Accuracy 0.8227\n",
      "Train Iteration : 174 Training Loss : 0.3917043141744828 Training Accuracy 0.8422333333333333\n",
      "Test Iteration : 174 Test Loss : 0.43324736377431 test Accuracy 0.8231\n",
      "Train Iteration : 175 Training Loss : 0.39096850281776013 Training Accuracy 0.8423166666666667\n",
      "Test Iteration : 175 Test Loss : 0.4326223409864211 test Accuracy 0.8239\n",
      "Train Iteration : 176 Training Loss : 0.39021015184663693 Training Accuracy 0.8424833333333334\n",
      "Test Iteration : 176 Test Loss : 0.4320083213650092 test Accuracy 0.8248\n",
      "Train Iteration : 177 Training Loss : 0.3894692144478629 Training Accuracy 0.8433333333333334\n",
      "Test Iteration : 177 Test Loss : 0.4313886480155967 test Accuracy 0.8245\n",
      "Train Iteration : 178 Training Loss : 0.38874873317100295 Training Accuracy 0.8430333333333333\n",
      "Test Iteration : 178 Test Loss : 0.4307965563133559 test Accuracy 0.8258\n",
      "Train Iteration : 179 Training Loss : 0.38801913178714953 Training Accuracy 0.8441833333333333\n",
      "Test Iteration : 179 Test Loss : 0.4301868937106772 test Accuracy 0.8258\n",
      "Train Iteration : 180 Training Loss : 0.38727307449066073 Training Accuracy 0.8437333333333333\n",
      "Test Iteration : 180 Test Loss : 0.42956872856065603 test Accuracy 0.8263\n",
      "Train Iteration : 181 Training Loss : 0.3865375654123835 Training Accuracy 0.84465\n",
      "Test Iteration : 181 Test Loss : 0.4289558104697836 test Accuracy 0.827\n",
      "Train Iteration : 182 Training Loss : 0.3858235247949606 Training Accuracy 0.8446166666666667\n",
      "Test Iteration : 182 Test Loss : 0.4283704265259468 test Accuracy 0.8272\n",
      "Train Iteration : 183 Training Loss : 0.38511452259822665 Training Accuracy 0.8451166666666666\n",
      "Test Iteration : 183 Test Loss : 0.42777309224979326 test Accuracy 0.8274\n",
      "Train Iteration : 184 Training Loss : 0.38439846393319105 Training Accuracy 0.8454166666666667\n",
      "Test Iteration : 184 Test Loss : 0.4271920354885541 test Accuracy 0.8274\n",
      "Train Iteration : 185 Training Loss : 0.38368387349243466 Training Accuracy 0.8459333333333333\n",
      "Test Iteration : 185 Test Loss : 0.42659542130220507 test Accuracy 0.828\n",
      "Train Iteration : 186 Training Loss : 0.3829841946225593 Training Accuracy 0.8458166666666667\n",
      "Test Iteration : 186 Test Loss : 0.4260239688123518 test Accuracy 0.8278\n",
      "Train Iteration : 187 Training Loss : 0.3823029367044743 Training Accuracy 0.8465666666666667\n",
      "Test Iteration : 187 Test Loss : 0.42547014098465386 test Accuracy 0.829\n",
      "Train Iteration : 188 Training Loss : 0.3816317932292065 Training Accuracy 0.8463\n",
      "Test Iteration : 188 Test Loss : 0.4249269222928282 test Accuracy 0.8285\n",
      "Train Iteration : 189 Training Loss : 0.3809800660516488 Training Accuracy 0.8471333333333333\n",
      "Test Iteration : 189 Test Loss : 0.42440615722509 test Accuracy 0.8297\n",
      "Train Iteration : 190 Training Loss : 0.38036709087181875 Training Accuracy 0.8470166666666666\n",
      "Test Iteration : 190 Test Loss : 0.42392424461939837 test Accuracy 0.8288\n",
      "Train Iteration : 191 Training Loss : 0.37986501022860575 Training Accuracy 0.8479666666666666\n",
      "Test Iteration : 191 Test Loss : 0.42356662583836285 test Accuracy 0.8304\n",
      "Train Iteration : 192 Training Loss : 0.37949224416992056 Training Accuracy 0.8474166666666667\n",
      "Test Iteration : 192 Test Loss : 0.4233246562700709 test Accuracy 0.8294\n",
      "Train Iteration : 193 Training Loss : 0.3794147805772206 Training Accuracy 0.84835\n",
      "Test Iteration : 193 Test Loss : 0.4234278064008002 test Accuracy 0.8308\n",
      "Train Iteration : 194 Training Loss : 0.3791343102272108 Training Accuracy 0.8473666666666667\n",
      "Test Iteration : 194 Test Loss : 0.4232401029873441 test Accuracy 0.829\n",
      "Train Iteration : 195 Training Loss : 0.37877500970147787 Training Accuracy 0.8489166666666667\n",
      "Test Iteration : 195 Test Loss : 0.42308632410141656 test Accuracy 0.8313\n",
      "Train Iteration : 196 Training Loss : 0.37719558821652927 Training Accuracy 0.8482166666666666\n",
      "Test Iteration : 196 Test Loss : 0.42151840159536386 test Accuracy 0.8303\n",
      "Train Iteration : 197 Training Loss : 0.3757826949335681 Training Accuracy 0.8496666666666667\n",
      "Test Iteration : 197 Test Loss : 0.42022186881129747 test Accuracy 0.8321\n",
      "Train Iteration : 198 Training Loss : 0.37512623893928265 Training Accuracy 0.8499\n",
      "Test Iteration : 198 Test Loss : 0.4196747654183493 test Accuracy 0.8324\n",
      "Train Iteration : 199 Training Loss : 0.3750728406768592 Training Accuracy 0.84935\n",
      "Test Iteration : 199 Test Loss : 0.41972004057553397 test Accuracy 0.8318\n",
      "Train Iteration : 200 Training Loss : 0.37489093572969573 Training Accuracy 0.85085\n",
      "Test Iteration : 200 Test Loss : 0.4197499987792964 test Accuracy 0.8327\n",
      "Train Iteration : 201 Training Loss : 0.37381153908170817 Training Accuracy 0.85\n",
      "Test Iteration : 201 Test Loss : 0.4187181795086131 test Accuracy 0.8322\n",
      "Train Iteration : 202 Training Loss : 0.3727049126369923 Training Accuracy 0.85135\n",
      "Test Iteration : 202 Test Loss : 0.4177623939614246 test Accuracy 0.8337\n",
      "Train Iteration : 203 Training Loss : 0.37214061000493076 Training Accuracy 0.8513333333333334\n",
      "Test Iteration : 203 Test Loss : 0.4173371447572905 test Accuracy 0.8337\n",
      "Train Iteration : 204 Training Loss : 0.37193650661539324 Training Accuracy 0.8509666666666666\n",
      "Test Iteration : 204 Test Loss : 0.4172356249121287 test Accuracy 0.833\n",
      "Train Iteration : 205 Training Loss : 0.3715231851303796 Training Accuracy 0.8522333333333333\n",
      "Test Iteration : 205 Test Loss : 0.4170340522296443 test Accuracy 0.8343\n",
      "Train Iteration : 206 Training Loss : 0.3705734053743348 Training Accuracy 0.8519666666666666\n",
      "Test Iteration : 206 Test Loss : 0.41613398223146514 test Accuracy 0.8333\n",
      "Train Iteration : 207 Training Loss : 0.3697496555302324 Training Accuracy 0.8525666666666667\n",
      "Test Iteration : 207 Test Loss : 0.41545213864339364 test Accuracy 0.8348\n",
      "Train Iteration : 208 Training Loss : 0.3693215485997817 Training Accuracy 0.8529166666666667\n",
      "Test Iteration : 208 Test Loss : 0.4151809967862624 test Accuracy 0.8346\n",
      "Train Iteration : 209 Training Loss : 0.36897753406164385 Training Accuracy 0.8525\n",
      "Test Iteration : 209 Test Loss : 0.4149164001379125 test Accuracy 0.8338\n",
      "Train Iteration : 210 Training Loss : 0.3684100521450079 Training Accuracy 0.8535666666666667\n",
      "Test Iteration : 210 Test Loss : 0.4145524831348735 test Accuracy 0.8348\n",
      "Train Iteration : 211 Training Loss : 0.3676019752535746 Training Accuracy 0.8530833333333333\n",
      "Test Iteration : 211 Test Loss : 0.41380997267949443 test Accuracy 0.8341\n",
      "Train Iteration : 212 Training Loss : 0.3669534639582865 Training Accuracy 0.8537\n",
      "Test Iteration : 212 Test Loss : 0.413303422038234 test Accuracy 0.8352\n",
      "Train Iteration : 213 Training Loss : 0.3665345094475029 Training Accuracy 0.8541333333333333\n",
      "Test Iteration : 213 Test Loss : 0.41304731087139507 test Accuracy 0.8355\n",
      "Train Iteration : 214 Training Loss : 0.3661024645957565 Training Accuracy 0.8539333333333333\n",
      "Test Iteration : 214 Test Loss : 0.41268910407097237 test Accuracy 0.8346\n",
      "Train Iteration : 215 Training Loss : 0.36551891180590246 Training Accuracy 0.8546833333333334\n",
      "Test Iteration : 215 Test Loss : 0.412295227694732 test Accuracy 0.8354\n",
      "Train Iteration : 216 Training Loss : 0.364824939024796 Training Accuracy 0.8546\n",
      "Test Iteration : 216 Test Loss : 0.4116776237933987 test Accuracy 0.835\n",
      "Train Iteration : 217 Training Loss : 0.3642359570681703 Training Accuracy 0.8550333333333333\n",
      "Test Iteration : 217 Test Loss : 0.41123314116122045 test Accuracy 0.8352\n",
      "Train Iteration : 218 Training Loss : 0.3637816730644587 Training Accuracy 0.8554166666666667\n",
      "Test Iteration : 218 Test Loss : 0.4109343091885041 test Accuracy 0.8364\n",
      "Train Iteration : 219 Training Loss : 0.36332690957364266 Training Accuracy 0.8552666666666666\n",
      "Test Iteration : 219 Test Loss : 0.41055806249080995 test Accuracy 0.8354\n",
      "Train Iteration : 220 Training Loss : 0.3627852292833651 Training Accuracy 0.8558833333333333\n",
      "Test Iteration : 220 Test Loss : 0.4102145088579331 test Accuracy 0.8366\n",
      "Train Iteration : 221 Training Loss : 0.362165248814365 Training Accuracy 0.8558833333333333\n",
      "Test Iteration : 221 Test Loss : 0.40967166327411125 test Accuracy 0.8358\n",
      "Train Iteration : 222 Training Loss : 0.3615869573002144 Training Accuracy 0.8563333333333333\n",
      "Test Iteration : 222 Test Loss : 0.4092474109898251 test Accuracy 0.8366\n",
      "Train Iteration : 223 Training Loss : 0.3610905531518241 Training Accuracy 0.8565166666666667\n",
      "Test Iteration : 223 Test Loss : 0.4088900748147487 test Accuracy 0.8375\n",
      "Train Iteration : 224 Training Loss : 0.36062710653459074 Training Accuracy 0.8565\n",
      "Test Iteration : 224 Test Loss : 0.4085319918512657 test Accuracy 0.8361\n",
      "Train Iteration : 225 Training Loss : 0.36013603453982795 Training Accuracy 0.8570166666666666\n",
      "Test Iteration : 225 Test Loss : 0.40823726014379197 test Accuracy 0.8374\n",
      "Train Iteration : 226 Training Loss : 0.3595875268640302 Training Accuracy 0.85715\n",
      "Test Iteration : 226 Test Loss : 0.40776753126627496 test Accuracy 0.837\n",
      "Train Iteration : 227 Training Loss : 0.3590272309054391 Training Accuracy 0.8573666666666667\n",
      "Test Iteration : 227 Test Loss : 0.4073923984780025 test Accuracy 0.8379\n",
      "Train Iteration : 228 Training Loss : 0.35849404258111633 Training Accuracy 0.8575833333333334\n",
      "Test Iteration : 228 Test Loss : 0.40697965892741494 test Accuracy 0.8381\n",
      "Train Iteration : 229 Training Loss : 0.35799980691347155 Training Accuracy 0.8579666666666667\n",
      "Test Iteration : 229 Test Loss : 0.4066269276069535 test Accuracy 0.8373\n",
      "Train Iteration : 230 Training Loss : 0.35752405741036514 Training Accuracy 0.8581166666666666\n",
      "Test Iteration : 230 Test Loss : 0.4063166594467223 test Accuracy 0.8384\n",
      "Train Iteration : 231 Training Loss : 0.35703824652178234 Training Accuracy 0.8584666666666667\n",
      "Test Iteration : 231 Test Loss : 0.40592712409495757 test Accuracy 0.8381\n",
      "Train Iteration : 232 Training Loss : 0.35653495406194446 Training Accuracy 0.8585666666666667\n",
      "Test Iteration : 232 Test Loss : 0.4056101245671835 test Accuracy 0.8385\n",
      "Train Iteration : 233 Training Loss : 0.3560131931650438 Training Accuracy 0.8587833333333333\n",
      "Test Iteration : 233 Test Loss : 0.40517287602038343 test Accuracy 0.8384\n",
      "Train Iteration : 234 Training Loss : 0.355494332572555 Training Accuracy 0.8590833333333333\n",
      "Test Iteration : 234 Test Loss : 0.4048302027627376 test Accuracy 0.8386\n",
      "Train Iteration : 235 Training Loss : 0.35498750819831865 Training Accuracy 0.8594\n",
      "Test Iteration : 235 Test Loss : 0.4044345845473796 test Accuracy 0.8388\n",
      "Train Iteration : 236 Training Loss : 0.3544967026830739 Training Accuracy 0.8593833333333334\n",
      "Test Iteration : 236 Test Loss : 0.40410543831588513 test Accuracy 0.8384\n",
      "Train Iteration : 237 Training Loss : 0.3540168786574738 Training Accuracy 0.8597\n",
      "Test Iteration : 237 Test Loss : 0.4037647975670245 test Accuracy 0.8392\n",
      "Train Iteration : 238 Training Loss : 0.3535394974903954 Training Accuracy 0.8599333333333333\n",
      "Test Iteration : 238 Test Loss : 0.4034193543102252 test Accuracy 0.8389\n",
      "Train Iteration : 239 Training Loss : 0.35306128046107454 Training Accuracy 0.8602\n",
      "Test Iteration : 239 Test Loss : 0.40310432691643483 test Accuracy 0.8396\n",
      "Train Iteration : 240 Training Loss : 0.3525767203977599 Training Accuracy 0.8603333333333333\n",
      "Test Iteration : 240 Test Loss : 0.4027479939421057 test Accuracy 0.8395\n",
      "Train Iteration : 241 Training Loss : 0.35208904595008533 Training Accuracy 0.8605\n",
      "Test Iteration : 241 Test Loss : 0.4024357048691059 test Accuracy 0.8396\n",
      "Train Iteration : 242 Training Loss : 0.35159908435294185 Training Accuracy 0.8610333333333333\n",
      "Test Iteration : 242 Test Loss : 0.40206565259165444 test Accuracy 0.8399\n",
      "Train Iteration : 243 Training Loss : 0.35111086120220847 Training Accuracy 0.86095\n",
      "Test Iteration : 243 Test Loss : 0.40175927864242683 test Accuracy 0.8405\n",
      "Train Iteration : 244 Training Loss : 0.350625522301565 Training Accuracy 0.8614166666666667\n",
      "Test Iteration : 244 Test Loss : 0.4013922349260259 test Accuracy 0.8407\n",
      "Train Iteration : 245 Training Loss : 0.35014611119032346 Training Accuracy 0.8614666666666667\n",
      "Test Iteration : 245 Test Loss : 0.40108473023674124 test Accuracy 0.8411\n",
      "Train Iteration : 246 Training Loss : 0.34967263694503475 Training Accuracy 0.8618833333333333\n",
      "Test Iteration : 246 Test Loss : 0.40072104412256276 test Accuracy 0.8412\n",
      "Train Iteration : 247 Training Loss : 0.34920686040502125 Training Accuracy 0.8618333333333333\n",
      "Test Iteration : 247 Test Loss : 0.4004401162889717 test Accuracy 0.8416\n",
      "Train Iteration : 248 Training Loss : 0.34874949674298616 Training Accuracy 0.8624333333333334\n",
      "Test Iteration : 248 Test Loss : 0.4000779900574432 test Accuracy 0.8415\n",
      "Train Iteration : 249 Training Loss : 0.3483062048916535 Training Accuracy 0.86195\n",
      "Test Iteration : 249 Test Loss : 0.3998387928284873 test Accuracy 0.8412\n",
      "Train Iteration : 250 Training Loss : 0.34789011430660977 Training Accuracy 0.8629\n",
      "Test Iteration : 250 Test Loss : 0.3994887221164235 test Accuracy 0.8419\n",
      "Train Iteration : 251 Training Loss : 0.3475196230992529 Training Accuracy 0.8622666666666666\n",
      "Test Iteration : 251 Test Loss : 0.3993763434008022 test Accuracy 0.8413\n",
      "Train Iteration : 252 Training Loss : 0.3472420957655327 Training Accuracy 0.8632333333333333\n",
      "Test Iteration : 252 Test Loss : 0.3990824944471528 test Accuracy 0.8433\n",
      "Train Iteration : 253 Training Loss : 0.347083423202711 Training Accuracy 0.8629333333333333\n",
      "Test Iteration : 253 Test Loss : 0.3992900640784336 test Accuracy 0.8408\n",
      "Train Iteration : 254 Training Loss : 0.34711643108235435 Training Accuracy 0.8630166666666667\n",
      "Test Iteration : 254 Test Loss : 0.39918156814468436 test Accuracy 0.8421\n",
      "Train Iteration : 255 Training Loss : 0.3470115295436119 Training Accuracy 0.8629333333333333\n",
      "Test Iteration : 255 Test Loss : 0.39957487813932985 test Accuracy 0.8414\n",
      "Train Iteration : 256 Training Loss : 0.3466082421892816 Training Accuracy 0.8629833333333333\n",
      "Test Iteration : 256 Test Loss : 0.3989098200773091 test Accuracy 0.8423\n",
      "Train Iteration : 257 Training Loss : 0.34543403007903073 Training Accuracy 0.8636833333333334\n",
      "Test Iteration : 257 Test Loss : 0.3981683938904761 test Accuracy 0.8417\n",
      "Train Iteration : 258 Training Loss : 0.3443193923177671 Training Accuracy 0.8643\n",
      "Test Iteration : 258 Test Loss : 0.3969781661506702 test Accuracy 0.8441\n",
      "Train Iteration : 259 Training Loss : 0.3438849036403293 Training Accuracy 0.86445\n",
      "Test Iteration : 259 Test Loss : 0.39666351077212236 test Accuracy 0.8445\n",
      "Train Iteration : 260 Training Loss : 0.3439458244888451 Training Accuracy 0.8643333333333333\n",
      "Test Iteration : 260 Test Loss : 0.3970360790273329 test Accuracy 0.8419\n",
      "Train Iteration : 261 Training Loss : 0.34380455898866447 Training Accuracy 0.86435\n",
      "Test Iteration : 261 Test Loss : 0.3967560208885239 test Accuracy 0.8447\n",
      "Train Iteration : 262 Training Loss : 0.34301078262210477 Training Accuracy 0.8645\n",
      "Test Iteration : 262 Test Loss : 0.3963517834844221 test Accuracy 0.8424\n",
      "Train Iteration : 263 Training Loss : 0.3421589122557211 Training Accuracy 0.8650166666666667\n",
      "Test Iteration : 263 Test Loss : 0.3954549528661886 test Accuracy 0.8449\n",
      "Train Iteration : 264 Training Loss : 0.3417852259457022 Training Accuracy 0.8652\n",
      "Test Iteration : 264 Test Loss : 0.3952087805688726 test Accuracy 0.8454\n",
      "Train Iteration : 265 Training Loss : 0.3417004374180615 Training Accuracy 0.8649666666666667\n",
      "Test Iteration : 265 Test Loss : 0.39541616472248126 test Accuracy 0.8428\n",
      "Train Iteration : 266 Training Loss : 0.34137404167342217 Training Accuracy 0.8649833333333333\n",
      "Test Iteration : 266 Test Loss : 0.3950057182174369 test Accuracy 0.8457\n",
      "Train Iteration : 267 Training Loss : 0.3406638215683071 Training Accuracy 0.86505\n",
      "Test Iteration : 267 Test Loss : 0.3945945935988448 test Accuracy 0.8434\n",
      "Train Iteration : 268 Training Loss : 0.3400547736056005 Training Accuracy 0.8658166666666667\n",
      "Test Iteration : 268 Test Loss : 0.3940534263759562 test Accuracy 0.8449\n",
      "Train Iteration : 269 Training Loss : 0.33977699449395365 Training Accuracy 0.8658166666666667\n",
      "Test Iteration : 269 Test Loss : 0.39385254124149527 test Accuracy 0.8458\n",
      "Train Iteration : 270 Training Loss : 0.33955863346831733 Training Accuracy 0.8657833333333333\n",
      "Test Iteration : 270 Test Loss : 0.39391948564107915 test Accuracy 0.8438\n",
      "Train Iteration : 271 Training Loss : 0.3391178679504718 Training Accuracy 0.8658\n",
      "Test Iteration : 271 Test Loss : 0.39341548768192836 test Accuracy 0.8457\n",
      "Train Iteration : 272 Training Loss : 0.3385155483833169 Training Accuracy 0.8662\n",
      "Test Iteration : 272 Test Loss : 0.39308571842293233 test Accuracy 0.8451\n",
      "Train Iteration : 273 Training Loss : 0.3380591956947409 Training Accuracy 0.8662833333333333\n",
      "Test Iteration : 273 Test Loss : 0.3926988718997763 test Accuracy 0.8452\n",
      "Train Iteration : 274 Training Loss : 0.3377938353826576 Training Accuracy 0.8667166666666667\n",
      "Test Iteration : 274 Test Loss : 0.3925221101825357 test Accuracy 0.8462\n",
      "Train Iteration : 275 Training Loss : 0.33750595605778255 Training Accuracy 0.86645\n",
      "Test Iteration : 275 Test Loss : 0.3924443290731165 test Accuracy 0.8449\n",
      "Train Iteration : 276 Training Loss : 0.33707432855855624 Training Accuracy 0.867\n",
      "Test Iteration : 276 Test Loss : 0.392064477870608 test Accuracy 0.8462\n",
      "Train Iteration : 277 Training Loss : 0.3366171598049491 Training Accuracy 0.8665833333333334\n",
      "Test Iteration : 277 Test Loss : 0.3917345204215095 test Accuracy 0.8455\n",
      "Train Iteration : 278 Training Loss : 0.3363312978716258 Training Accuracy 0.8672666666666666\n",
      "Test Iteration : 278 Test Loss : 0.39167301795567944 test Accuracy 0.8462\n",
      "Train Iteration : 279 Training Loss : 0.33620694181100225 Training Accuracy 0.8667833333333334\n",
      "Test Iteration : 279 Test Loss : 0.39148710103230333 test Accuracy 0.8463\n",
      "Train Iteration : 280 Training Loss : 0.336264727243302 Training Accuracy 0.8676\n",
      "Test Iteration : 280 Test Loss : 0.39197092758045093 test Accuracy 0.8463\n",
      "Train Iteration : 281 Training Loss : 0.33622386785037417 Training Accuracy 0.8668333333333333\n",
      "Test Iteration : 281 Test Loss : 0.39173192659220474 test Accuracy 0.8456\n",
      "Train Iteration : 282 Training Loss : 0.33655913810361476 Training Accuracy 0.8673333333333333\n",
      "Test Iteration : 282 Test Loss : 0.39256234017085734 test Accuracy 0.8458\n",
      "Train Iteration : 283 Training Loss : 0.33626158556593977 Training Accuracy 0.8665\n",
      "Test Iteration : 283 Test Loss : 0.3920864284757724 test Accuracy 0.8447\n",
      "Train Iteration : 284 Training Loss : 0.33565604889762846 Training Accuracy 0.8673666666666666\n",
      "Test Iteration : 284 Test Loss : 0.3918035807723335 test Accuracy 0.8465\n",
      "Train Iteration : 285 Training Loss : 0.33412101700532837 Training Accuracy 0.8674166666666666\n",
      "Test Iteration : 285 Test Loss : 0.39017261626069527 test Accuracy 0.846\n",
      "Train Iteration : 286 Training Loss : 0.3330602132941663 Training Accuracy 0.8683166666666666\n",
      "Test Iteration : 286 Test Loss : 0.3892444424697768 test Accuracy 0.8465\n",
      "Train Iteration : 287 Training Loss : 0.33302555750008384 Training Accuracy 0.8686333333333334\n",
      "Test Iteration : 287 Test Loss : 0.38937601639606273 test Accuracy 0.8475\n",
      "Train Iteration : 288 Training Loss : 0.33332490653178637 Training Accuracy 0.8678\n",
      "Test Iteration : 288 Test Loss : 0.3897315254180925 test Accuracy 0.846\n",
      "Train Iteration : 289 Training Loss : 0.33309171349347017 Training Accuracy 0.8686166666666667\n",
      "Test Iteration : 289 Test Loss : 0.38977860661320207 test Accuracy 0.847\n",
      "Train Iteration : 290 Training Loss : 0.3320743928709818 Training Accuracy 0.8685\n",
      "Test Iteration : 290 Test Loss : 0.3886794586329339 test Accuracy 0.8461\n",
      "Train Iteration : 291 Training Loss : 0.3313486160289444 Training Accuracy 0.8692166666666666\n",
      "Test Iteration : 291 Test Loss : 0.3882118954636962 test Accuracy 0.8477\n",
      "Train Iteration : 292 Training Loss : 0.3311804733501711 Training Accuracy 0.8688833333333333\n",
      "Test Iteration : 292 Test Loss : 0.38810181382144826 test Accuracy 0.8481\n",
      "Train Iteration : 293 Training Loss : 0.3310935490213475 Training Accuracy 0.8689\n",
      "Test Iteration : 293 Test Loss : 0.3881322177949281 test Accuracy 0.8469\n",
      "Train Iteration : 294 Training Loss : 0.33073715410754895 Training Accuracy 0.8695333333333334\n",
      "Test Iteration : 294 Test Loss : 0.38804084132862 test Accuracy 0.848\n",
      "Train Iteration : 295 Training Loss : 0.33015411366735437 Training Accuracy 0.86925\n",
      "Test Iteration : 295 Test Loss : 0.387355034698742 test Accuracy 0.8475\n",
      "Train Iteration : 296 Training Loss : 0.32970323008155106 Training Accuracy 0.8698666666666667\n",
      "Test Iteration : 296 Test Loss : 0.38722455320711413 test Accuracy 0.8479\n",
      "Train Iteration : 297 Training Loss : 0.3293562413860488 Training Accuracy 0.8698166666666667\n",
      "Test Iteration : 297 Test Loss : 0.38688381951382234 test Accuracy 0.8483\n",
      "Train Iteration : 298 Training Loss : 0.3290342748110841 Training Accuracy 0.8699833333333333\n",
      "Test Iteration : 298 Test Loss : 0.38667728187226075 test Accuracy 0.848\n",
      "Train Iteration : 299 Training Loss : 0.32874611881318355 Training Accuracy 0.8706833333333334\n",
      "Test Iteration : 299 Test Loss : 0.38665902584842915 test Accuracy 0.8482\n",
      "Train Iteration : 300 Training Loss : 0.3284177926867944 Training Accuracy 0.87005\n",
      "Test Iteration : 300 Test Loss : 0.38622167631389936 test Accuracy 0.8483\n",
      "Train Iteration : 301 Training Loss : 0.32799314681737585 Training Accuracy 0.8707\n",
      "Test Iteration : 301 Test Loss : 0.386137324494828 test Accuracy 0.8481\n",
      "Train Iteration : 302 Training Loss : 0.327527751516618 Training Accuracy 0.8703666666666666\n",
      "Test Iteration : 302 Test Loss : 0.38566726097098 test Accuracy 0.8491\n",
      "Train Iteration : 303 Training Loss : 0.32717651577525925 Training Accuracy 0.8706666666666667\n",
      "Test Iteration : 303 Test Loss : 0.3854340126093982 test Accuracy 0.8488\n",
      "Train Iteration : 304 Training Loss : 0.3269531688788703 Training Accuracy 0.8711166666666667\n",
      "Test Iteration : 304 Test Loss : 0.3854778104723039 test Accuracy 0.8484\n",
      "Train Iteration : 305 Training Loss : 0.32667813505625964 Training Accuracy 0.87065\n",
      "Test Iteration : 305 Test Loss : 0.3851246864289392 test Accuracy 0.8492\n",
      "Train Iteration : 306 Training Loss : 0.3262546897721641 Training Accuracy 0.8711666666666666\n",
      "Test Iteration : 306 Test Loss : 0.3850232603238978 test Accuracy 0.849\n",
      "Train Iteration : 307 Training Loss : 0.32579167948527343 Training Accuracy 0.8712333333333333\n",
      "Test Iteration : 307 Test Loss : 0.3845505842568086 test Accuracy 0.8497\n",
      "Train Iteration : 308 Training Loss : 0.3254421295360718 Training Accuracy 0.8713666666666666\n",
      "Test Iteration : 308 Test Loss : 0.3843410798367383 test Accuracy 0.8496\n",
      "Train Iteration : 309 Training Loss : 0.32519233568522576 Training Accuracy 0.8716333333333334\n",
      "Test Iteration : 309 Test Loss : 0.38432801658919324 test Accuracy 0.8493\n",
      "Train Iteration : 310 Training Loss : 0.32491093659963277 Training Accuracy 0.8713333333333333\n",
      "Test Iteration : 310 Test Loss : 0.38400639997977415 test Accuracy 0.8493\n",
      "Train Iteration : 311 Training Loss : 0.3245366309739334 Training Accuracy 0.872\n",
      "Test Iteration : 311 Test Loss : 0.3839391723754194 test Accuracy 0.8495\n",
      "Train Iteration : 312 Training Loss : 0.3241298651064637 Training Accuracy 0.8717666666666667\n",
      "Test Iteration : 312 Test Loss : 0.3835412675670546 test Accuracy 0.8489\n",
      "Train Iteration : 313 Training Loss : 0.323772647078427 Training Accuracy 0.8720333333333333\n",
      "Test Iteration : 313 Test Loss : 0.38336930440168726 test Accuracy 0.85\n",
      "Train Iteration : 314 Training Loss : 0.3234626948591043 Training Accuracy 0.8722166666666666\n",
      "Test Iteration : 314 Test Loss : 0.383235796021992 test Accuracy 0.8499\n",
      "Train Iteration : 315 Training Loss : 0.323153367320714 Training Accuracy 0.8721833333333333\n",
      "Test Iteration : 315 Test Loss : 0.3829621275849215 test Accuracy 0.8497\n",
      "Train Iteration : 316 Training Loss : 0.3228220576013979 Training Accuracy 0.8727\n",
      "Test Iteration : 316 Test Loss : 0.3828892633920736 test Accuracy 0.8499\n",
      "Train Iteration : 317 Training Loss : 0.32248098679203735 Training Accuracy 0.8722333333333333\n",
      "Test Iteration : 317 Test Loss : 0.3825662800234035 test Accuracy 0.8497\n",
      "Train Iteration : 318 Training Loss : 0.32214815385885454 Training Accuracy 0.8729\n",
      "Test Iteration : 318 Test Loss : 0.3824403229874477 test Accuracy 0.8506\n",
      "Train Iteration : 319 Training Loss : 0.32181533354265884 Training Accuracy 0.8726833333333334\n",
      "Test Iteration : 319 Test Loss : 0.3822025393258942 test Accuracy 0.8494\n",
      "Train Iteration : 320 Training Loss : 0.3214755340915247 Training Accuracy 0.8728833333333333\n",
      "Test Iteration : 320 Test Loss : 0.3819842303248205 test Accuracy 0.8506\n",
      "Train Iteration : 321 Training Loss : 0.32113128562222304 Training Accuracy 0.8731333333333333\n",
      "Test Iteration : 321 Test Loss : 0.38179693668891107 test Accuracy 0.8502\n",
      "Train Iteration : 322 Training Loss : 0.32079783004002044 Training Accuracy 0.8731166666666667\n",
      "Test Iteration : 322 Test Loss : 0.3815509729073533 test Accuracy 0.8502\n",
      "Train Iteration : 323 Training Loss : 0.3204827617437022 Training Accuracy 0.8733666666666666\n",
      "Test Iteration : 323 Test Loss : 0.38141433282750503 test Accuracy 0.8507\n",
      "Train Iteration : 324 Training Loss : 0.320176091294431 Training Accuracy 0.8733166666666666\n",
      "Test Iteration : 324 Test Loss : 0.3811934927788745 test Accuracy 0.85\n",
      "Train Iteration : 325 Training Loss : 0.3198639276893822 Training Accuracy 0.8737\n",
      "Test Iteration : 325 Test Loss : 0.3810389545519877 test Accuracy 0.8509\n",
      "Train Iteration : 326 Training Loss : 0.31953613831422395 Training Accuracy 0.87355\n",
      "Test Iteration : 326 Test Loss : 0.3807960870774207 test Accuracy 0.8498\n",
      "Train Iteration : 327 Training Loss : 0.3191993554011287 Training Accuracy 0.8739833333333333\n",
      "Test Iteration : 327 Test Loss : 0.38061857049726106 test Accuracy 0.8509\n",
      "Train Iteration : 328 Training Loss : 0.3188611344946758 Training Accuracy 0.87395\n",
      "Test Iteration : 328 Test Loss : 0.3803560922104779 test Accuracy 0.8502\n",
      "Train Iteration : 329 Training Loss : 0.31853038634421016 Training Accuracy 0.8741833333333333\n",
      "Test Iteration : 329 Test Loss : 0.38019392870009394 test Accuracy 0.8507\n",
      "Train Iteration : 330 Training Loss : 0.31820695062054094 Training Accuracy 0.87435\n",
      "Test Iteration : 330 Test Loss : 0.37994751326827064 test Accuracy 0.8507\n",
      "Train Iteration : 331 Training Loss : 0.31788746731678424 Training Accuracy 0.87445\n",
      "Test Iteration : 331 Test Loss : 0.3797967793057529 test Accuracy 0.8508\n",
      "Train Iteration : 332 Training Loss : 0.3175678637235349 Training Accuracy 0.8747333333333334\n",
      "Test Iteration : 332 Test Loss : 0.3795731586652102 test Accuracy 0.8512\n",
      "Train Iteration : 333 Training Loss : 0.31724735387803016 Training Accuracy 0.87455\n",
      "Test Iteration : 333 Test Loss : 0.3794072635526757 test Accuracy 0.8508\n",
      "Train Iteration : 334 Training Loss : 0.3169270502249029 Training Accuracy 0.8748833333333333\n",
      "Test Iteration : 334 Test Loss : 0.37921552161086663 test Accuracy 0.8516\n",
      "Train Iteration : 335 Training Loss : 0.31660954012982184 Training Accuracy 0.8749166666666667\n",
      "Test Iteration : 335 Test Loss : 0.3790100654087024 test Accuracy 0.8502\n",
      "Train Iteration : 336 Training Loss : 0.31629639548941624 Training Accuracy 0.8752\n",
      "Test Iteration : 336 Test Loss : 0.37886814056416407 test Accuracy 0.852\n",
      "Train Iteration : 337 Training Loss : 0.31598680468255574 Training Accuracy 0.875\n",
      "Test Iteration : 337 Test Loss : 0.37862240254887847 test Accuracy 0.8502\n",
      "Train Iteration : 338 Training Loss : 0.3156808965077074 Training Accuracy 0.87515\n",
      "Test Iteration : 338 Test Loss : 0.37853024488574494 test Accuracy 0.8518\n",
      "Train Iteration : 339 Training Loss : 0.3153776554354839 Training Accuracy 0.8751333333333333\n",
      "Test Iteration : 339 Test Loss : 0.3782514145944574 test Accuracy 0.85\n",
      "Train Iteration : 340 Training Loss : 0.3150794110487675 Training Accuracy 0.8755833333333334\n",
      "Test Iteration : 340 Test Loss : 0.3782032052296934 test Accuracy 0.852\n",
      "Train Iteration : 341 Training Loss : 0.31478711479764687 Training Accuracy 0.8752833333333333\n",
      "Test Iteration : 341 Test Loss : 0.37789315103231497 test Accuracy 0.8506\n",
      "Train Iteration : 342 Training Loss : 0.3145122947657002 Training Accuracy 0.87575\n",
      "Test Iteration : 342 Test Loss : 0.3779093079985624 test Accuracy 0.8526\n",
      "Train Iteration : 343 Training Loss : 0.3142543802971169 Training Accuracy 0.8751833333333333\n",
      "Test Iteration : 343 Test Loss : 0.37759361264128766 test Accuracy 0.8512\n",
      "Train Iteration : 344 Training Loss : 0.31404645099779555 Training Accuracy 0.8759333333333333\n",
      "Test Iteration : 344 Test Loss : 0.3777451976522763 test Accuracy 0.8528\n",
      "Train Iteration : 345 Training Loss : 0.31387479084793984 Training Accuracy 0.8753833333333333\n",
      "Test Iteration : 345 Test Loss : 0.3774308067313111 test Accuracy 0.8508\n",
      "Train Iteration : 346 Training Loss : 0.31383213546124966 Training Accuracy 0.8760666666666667\n",
      "Test Iteration : 346 Test Loss : 0.37786233703112265 test Accuracy 0.8523\n",
      "Train Iteration : 347 Training Loss : 0.3137875722585518 Training Accuracy 0.8752333333333333\n",
      "Test Iteration : 347 Test Loss : 0.3775572528302706 test Accuracy 0.8517\n",
      "Train Iteration : 348 Training Loss : 0.3139643095676547 Training Accuracy 0.8759333333333333\n",
      "Test Iteration : 348 Test Loss : 0.37835201889977405 test Accuracy 0.8524\n",
      "Train Iteration : 349 Training Loss : 0.313795467527664 Training Accuracy 0.8753833333333333\n",
      "Test Iteration : 349 Test Loss : 0.3777937338679947 test Accuracy 0.8507\n",
      "Train Iteration : 350 Training Loss : 0.31371271193952277 Training Accuracy 0.8759666666666667\n",
      "Test Iteration : 350 Test Loss : 0.37838885088001206 test Accuracy 0.8523\n",
      "Train Iteration : 351 Training Loss : 0.3128380237738227 Training Accuracy 0.8756833333333334\n",
      "Test Iteration : 351 Test Loss : 0.3771118810112568 test Accuracy 0.8517\n",
      "Train Iteration : 352 Training Loss : 0.31198064338836995 Training Accuracy 0.877\n",
      "Test Iteration : 352 Test Loss : 0.37673534947329945 test Accuracy 0.8522\n",
      "Train Iteration : 353 Training Loss : 0.3111695555017982 Training Accuracy 0.87695\n",
      "Test Iteration : 353 Test Loss : 0.3758184011284498 test Accuracy 0.8523\n",
      "Train Iteration : 354 Training Loss : 0.31078705867801026 Training Accuracy 0.8771833333333333\n",
      "Test Iteration : 354 Test Loss : 0.37555804233489537 test Accuracy 0.8526\n",
      "Train Iteration : 355 Training Loss : 0.31076452252664577 Training Accuracy 0.8773166666666666\n",
      "Test Iteration : 355 Test Loss : 0.37585492080130734 test Accuracy 0.8527\n",
      "Train Iteration : 356 Training Loss : 0.31078871865420066 Training Accuracy 0.8766333333333334\n",
      "Test Iteration : 356 Test Loss : 0.3757145943263305 test Accuracy 0.8527\n",
      "Train Iteration : 357 Training Loss : 0.3107045635304696 Training Accuracy 0.8774666666666666\n",
      "Test Iteration : 357 Test Loss : 0.37615000882139576 test Accuracy 0.8533\n",
      "Train Iteration : 358 Training Loss : 0.310191349398977 Training Accuracy 0.8769666666666667\n",
      "Test Iteration : 358 Test Loss : 0.3753873544329592 test Accuracy 0.8524\n",
      "Train Iteration : 359 Training Loss : 0.30959981476551807 Training Accuracy 0.87795\n",
      "Test Iteration : 359 Test Loss : 0.3751956917057347 test Accuracy 0.8526\n",
      "Train Iteration : 360 Training Loss : 0.30905682698440096 Training Accuracy 0.8778833333333333\n",
      "Test Iteration : 360 Test Loss : 0.374624535564181 test Accuracy 0.8529\n",
      "Train Iteration : 361 Training Loss : 0.30874992646763055 Training Accuracy 0.8782166666666666\n",
      "Test Iteration : 361 Test Loss : 0.3744353098395071 test Accuracy 0.8526\n",
      "Train Iteration : 362 Training Loss : 0.3086283239778309 Training Accuracy 0.87815\n",
      "Test Iteration : 362 Test Loss : 0.3745959153309388 test Accuracy 0.8529\n",
      "Train Iteration : 363 Training Loss : 0.30850761417587735 Training Accuracy 0.8776\n",
      "Test Iteration : 363 Test Loss : 0.3743610813381235 test Accuracy 0.8527\n",
      "Train Iteration : 364 Training Loss : 0.3083029148467644 Training Accuracy 0.8781666666666667\n",
      "Test Iteration : 364 Test Loss : 0.3745728736494168 test Accuracy 0.8527\n",
      "Train Iteration : 365 Training Loss : 0.3078941718454498 Training Accuracy 0.87775\n",
      "Test Iteration : 365 Test Loss : 0.37400739562504554 test Accuracy 0.8531\n",
      "Train Iteration : 366 Training Loss : 0.3074493090367146 Training Accuracy 0.8787333333333334\n",
      "Test Iteration : 366 Test Loss : 0.37389428940202185 test Accuracy 0.8534\n",
      "Train Iteration : 367 Training Loss : 0.3070372200024886 Training Accuracy 0.8785666666666667\n",
      "Test Iteration : 367 Test Loss : 0.3734907221220789 test Accuracy 0.8538\n",
      "Train Iteration : 368 Training Loss : 0.3067379375578358 Training Accuracy 0.8787333333333334\n",
      "Test Iteration : 368 Test Loss : 0.3733183012011983 test Accuracy 0.8535\n",
      "Train Iteration : 369 Training Loss : 0.306530048032454 Training Accuracy 0.8792166666666666\n",
      "Test Iteration : 369 Test Loss : 0.37335776630269374 test Accuracy 0.8532\n",
      "Train Iteration : 370 Training Loss : 0.30634081086750264 Training Accuracy 0.8787333333333334\n",
      "Test Iteration : 370 Test Loss : 0.3731219363216829 test Accuracy 0.8531\n",
      "Train Iteration : 371 Training Loss : 0.3061259893398202 Training Accuracy 0.87935\n",
      "Test Iteration : 371 Test Loss : 0.3732763936288694 test Accuracy 0.8535\n",
      "Train Iteration : 372 Training Loss : 0.30581674639027895 Training Accuracy 0.8789\n",
      "Test Iteration : 372 Test Loss : 0.3728471212138552 test Accuracy 0.8532\n",
      "Train Iteration : 373 Training Loss : 0.30547114594027114 Training Accuracy 0.8796166666666667\n",
      "Test Iteration : 373 Test Loss : 0.37284715823750986 test Accuracy 0.8536\n",
      "Train Iteration : 374 Training Loss : 0.3051028338643589 Training Accuracy 0.8792833333333333\n",
      "Test Iteration : 374 Test Loss : 0.37242907652028606 test Accuracy 0.8534\n",
      "Train Iteration : 375 Training Loss : 0.3047706530679813 Training Accuracy 0.8796333333333334\n",
      "Test Iteration : 375 Test Loss : 0.37231869209975377 test Accuracy 0.8538\n",
      "Train Iteration : 376 Training Loss : 0.3044866548253348 Training Accuracy 0.8797666666666667\n",
      "Test Iteration : 376 Test Loss : 0.37213766184965025 test Accuracy 0.8538\n",
      "Train Iteration : 377 Training Loss : 0.30424040210623715 Training Accuracy 0.8797666666666667\n",
      "Test Iteration : 377 Test Loss : 0.37197068987907606 test Accuracy 0.8534\n",
      "Train Iteration : 378 Training Loss : 0.30401052184731236 Training Accuracy 0.8798666666666667\n",
      "Test Iteration : 378 Test Loss : 0.3719700603439699 test Accuracy 0.8539\n",
      "Train Iteration : 379 Training Loss : 0.3037669030208718 Training Accuracy 0.8797\n",
      "Test Iteration : 379 Test Loss : 0.3717172115811268 test Accuracy 0.854\n",
      "Train Iteration : 380 Training Loss : 0.30350826093244476 Training Accuracy 0.8800666666666667\n",
      "Test Iteration : 380 Test Loss : 0.3717463119407867 test Accuracy 0.8537\n",
      "Train Iteration : 381 Training Loss : 0.3032161299492297 Training Accuracy 0.8800833333333333\n",
      "Test Iteration : 381 Test Loss : 0.37142633201701314 test Accuracy 0.8541\n",
      "Train Iteration : 382 Training Loss : 0.30291693779467144 Training Accuracy 0.8802\n",
      "Test Iteration : 382 Test Loss : 0.3713855170282485 test Accuracy 0.8543\n",
      "Train Iteration : 383 Training Loss : 0.30260790923426784 Training Accuracy 0.88015\n",
      "Test Iteration : 383 Test Loss : 0.37109634161166605 test Accuracy 0.8542\n",
      "Train Iteration : 384 Training Loss : 0.3023077928942643 Training Accuracy 0.8806\n",
      "Test Iteration : 384 Test Loss : 0.3709859181291651 test Accuracy 0.8546\n",
      "Train Iteration : 385 Training Loss : 0.3020188941863441 Training Accuracy 0.8803833333333333\n",
      "Test Iteration : 385 Test Loss : 0.37081229980047264 test Accuracy 0.8553\n",
      "Train Iteration : 386 Training Loss : 0.3017465115690742 Training Accuracy 0.8807333333333334\n",
      "Test Iteration : 386 Test Loss : 0.3706363447860144 test Accuracy 0.854\n",
      "Train Iteration : 387 Training Loss : 0.3014948973115661 Training Accuracy 0.8810166666666667\n",
      "Test Iteration : 387 Test Loss : 0.3706014827853519 test Accuracy 0.855\n",
      "Train Iteration : 388 Training Loss : 0.3012713072179395 Training Accuracy 0.8806166666666667\n",
      "Test Iteration : 388 Test Loss : 0.3703500640922068 test Accuracy 0.8549\n",
      "Train Iteration : 389 Training Loss : 0.30108798373112333 Training Accuracy 0.8810666666666667\n",
      "Test Iteration : 389 Test Loss : 0.3705137370212656 test Accuracy 0.8545\n",
      "Train Iteration : 390 Training Loss : 0.3009837784789355 Training Accuracy 0.8806833333333334\n",
      "Test Iteration : 390 Test Loss : 0.3702283835149161 test Accuracy 0.8558\n",
      "Train Iteration : 391 Training Loss : 0.3009889619225887 Training Accuracy 0.88125\n",
      "Test Iteration : 391 Test Loss : 0.37075346204480814 test Accuracy 0.8544\n",
      "Train Iteration : 392 Training Loss : 0.30122621184865356 Training Accuracy 0.8806666666666667\n",
      "Test Iteration : 392 Test Loss : 0.370609794479582 test Accuracy 0.8546\n",
      "Train Iteration : 393 Training Loss : 0.30160759101400786 Training Accuracy 0.88075\n",
      "Test Iteration : 393 Test Loss : 0.3717537526632195 test Accuracy 0.854\n",
      "Train Iteration : 394 Training Loss : 0.30214821390515634 Training Accuracy 0.8797166666666667\n",
      "Test Iteration : 394 Test Loss : 0.3717175851403008 test Accuracy 0.8537\n",
      "Train Iteration : 395 Training Loss : 0.3019644305769636 Training Accuracy 0.8799666666666667\n",
      "Test Iteration : 395 Test Loss : 0.37240110019619377 test Accuracy 0.8538\n",
      "Train Iteration : 396 Training Loss : 0.3009483674077563 Training Accuracy 0.8808333333333334\n",
      "Test Iteration : 396 Test Loss : 0.3708835521677151 test Accuracy 0.8553\n",
      "Train Iteration : 397 Training Loss : 0.29946346090241616 Training Accuracy 0.8812\n",
      "Test Iteration : 397 Test Loss : 0.36981205397289496 test Accuracy 0.8558\n",
      "Train Iteration : 398 Training Loss : 0.29881261278309634 Training Accuracy 0.8822166666666666\n",
      "Test Iteration : 398 Test Loss : 0.3693386355468225 test Accuracy 0.8555\n",
      "Train Iteration : 399 Training Loss : 0.2991546416267046 Training Accuracy 0.88075\n",
      "Test Iteration : 399 Test Loss : 0.3693774025116641 test Accuracy 0.8548\n",
      "Train Iteration : 400 Training Loss : 0.29938634830932737 Training Accuracy 0.8817333333333334\n",
      "Test Iteration : 400 Test Loss : 0.37040725978976413 test Accuracy 0.854\n",
      "Train Iteration : 401 Training Loss : 0.29880452685966774 Training Accuracy 0.88105\n",
      "Test Iteration : 401 Test Loss : 0.3692904752925378 test Accuracy 0.8558\n",
      "Train Iteration : 402 Training Loss : 0.29770784748188056 Training Accuracy 0.8823833333333333\n",
      "Test Iteration : 402 Test Loss : 0.3687050305319637 test Accuracy 0.8551\n",
      "Train Iteration : 403 Training Loss : 0.2973227618016067 Training Accuracy 0.8827833333333334\n",
      "Test Iteration : 403 Test Loss : 0.36844459868084983 test Accuracy 0.8557\n",
      "Train Iteration : 404 Training Loss : 0.2976359048387137 Training Accuracy 0.88135\n",
      "Test Iteration : 404 Test Loss : 0.3685433749538622 test Accuracy 0.855\n",
      "Train Iteration : 405 Training Loss : 0.2976458000467102 Training Accuracy 0.8825166666666666\n",
      "Test Iteration : 405 Test Loss : 0.36921878035102734 test Accuracy 0.8544\n",
      "Train Iteration : 406 Training Loss : 0.29700398074509504 Training Accuracy 0.8815\n",
      "Test Iteration : 406 Test Loss : 0.3681523601294289 test Accuracy 0.8555\n",
      "Train Iteration : 407 Training Loss : 0.2962860283601807 Training Accuracy 0.8830333333333333\n",
      "Test Iteration : 407 Test Loss : 0.36786576335258475 test Accuracy 0.8563\n",
      "Train Iteration : 408 Training Loss : 0.29614037765170553 Training Accuracy 0.8828333333333334\n",
      "Test Iteration : 408 Test Loss : 0.36784327712838744 test Accuracy 0.8552\n",
      "Train Iteration : 409 Training Loss : 0.29623128684972 Training Accuracy 0.8822833333333333\n",
      "Test Iteration : 409 Test Loss : 0.36777745277001345 test Accuracy 0.8563\n",
      "Train Iteration : 410 Training Loss : 0.295921019724435 Training Accuracy 0.8830833333333333\n",
      "Test Iteration : 410 Test Loss : 0.3679463220468388 test Accuracy 0.8559\n",
      "Train Iteration : 411 Training Loss : 0.295335895755992 Training Accuracy 0.8827833333333334\n",
      "Test Iteration : 411 Test Loss : 0.36718100870017145 test Accuracy 0.8565\n",
      "Train Iteration : 412 Training Loss : 0.294982389494915 Training Accuracy 0.8830166666666667\n",
      "Test Iteration : 412 Test Loss : 0.36705826466162245 test Accuracy 0.8574\n",
      "Train Iteration : 413 Training Loss : 0.2949547571442897 Training Accuracy 0.8832333333333333\n",
      "Test Iteration : 413 Test Loss : 0.3672702235189452 test Accuracy 0.8559\n",
      "Train Iteration : 414 Training Loss : 0.29484961129425963 Training Accuracy 0.8831\n",
      "Test Iteration : 414 Test Loss : 0.3670346314643488 test Accuracy 0.857\n",
      "Train Iteration : 415 Training Loss : 0.2944439756543754 Training Accuracy 0.8835166666666666\n",
      "Test Iteration : 415 Test Loss : 0.36698595187445765 test Accuracy 0.8559\n",
      "Train Iteration : 416 Training Loss : 0.29400707825102224 Training Accuracy 0.8835333333333333\n",
      "Test Iteration : 416 Test Loss : 0.36658479469551625 test Accuracy 0.8574\n",
      "Train Iteration : 417 Training Loss : 0.29379307541993865 Training Accuracy 0.8833166666666666\n",
      "Test Iteration : 417 Test Loss : 0.3664019296271449 test Accuracy 0.8568\n",
      "Train Iteration : 418 Training Loss : 0.29368655560401824 Training Accuracy 0.8839333333333333\n",
      "Test Iteration : 418 Test Loss : 0.3666406068172808 test Accuracy 0.8561\n",
      "Train Iteration : 419 Training Loss : 0.2934479159411075 Training Accuracy 0.8834333333333333\n",
      "Test Iteration : 419 Test Loss : 0.3662273285823686 test Accuracy 0.8568\n",
      "Train Iteration : 420 Training Loss : 0.2930603731191636 Training Accuracy 0.8842833333333333\n",
      "Test Iteration : 420 Test Loss : 0.36620776387065473 test Accuracy 0.8567\n",
      "Train Iteration : 421 Training Loss : 0.2927282023743725 Training Accuracy 0.8841166666666667\n",
      "Test Iteration : 421 Test Loss : 0.3658928376467678 test Accuracy 0.8568\n",
      "Train Iteration : 422 Training Loss : 0.2925370360902079 Training Accuracy 0.8841333333333333\n",
      "Test Iteration : 422 Test Loss : 0.3657710689374703 test Accuracy 0.8575\n",
      "Train Iteration : 423 Training Loss : 0.29237159291568254 Training Accuracy 0.8845166666666666\n",
      "Test Iteration : 423 Test Loss : 0.3658946682043207 test Accuracy 0.8567\n",
      "Train Iteration : 424 Training Loss : 0.29211249087899654 Training Accuracy 0.8844\n",
      "Test Iteration : 424 Test Loss : 0.3655651154833566 test Accuracy 0.8572\n",
      "Train Iteration : 425 Training Loss : 0.29178037041912974 Training Accuracy 0.8847166666666667\n",
      "Test Iteration : 425 Test Loss : 0.36551644259862226 test Accuracy 0.8565\n",
      "Train Iteration : 426 Training Loss : 0.2914949070357326 Training Accuracy 0.88475\n",
      "Test Iteration : 426 Test Loss : 0.3652830609579701 test Accuracy 0.8569\n",
      "Train Iteration : 427 Training Loss : 0.29129040857948363 Training Accuracy 0.8844833333333333\n",
      "Test Iteration : 427 Test Loss : 0.3651439365161453 test Accuracy 0.8572\n",
      "Train Iteration : 428 Training Loss : 0.29109806880213496 Training Accuracy 0.8849166666666667\n",
      "Test Iteration : 428 Test Loss : 0.3652048097151187 test Accuracy 0.8569\n",
      "Train Iteration : 429 Training Loss : 0.29084906864589394 Training Accuracy 0.8846833333333334\n",
      "Test Iteration : 429 Test Loss : 0.3649140923811081 test Accuracy 0.8577\n",
      "Train Iteration : 430 Training Loss : 0.29055717197363085 Training Accuracy 0.8848833333333334\n",
      "Test Iteration : 430 Test Loss : 0.3648467534446955 test Accuracy 0.857\n",
      "Train Iteration : 431 Training Loss : 0.2902834792726049 Training Accuracy 0.8850166666666667\n",
      "Test Iteration : 431 Test Loss : 0.36466018030395375 test Accuracy 0.8575\n",
      "Train Iteration : 432 Training Loss : 0.2900605498694159 Training Accuracy 0.8851333333333333\n",
      "Test Iteration : 432 Test Loss : 0.364501207584485 test Accuracy 0.8578\n",
      "Train Iteration : 433 Training Loss : 0.2898611964724205 Training Accuracy 0.8854666666666666\n",
      "Test Iteration : 433 Test Loss : 0.36456520461725006 test Accuracy 0.8576\n",
      "Train Iteration : 434 Training Loss : 0.2896475410318969 Training Accuracy 0.8851333333333333\n",
      "Test Iteration : 434 Test Loss : 0.36427533126982226 test Accuracy 0.8576\n",
      "Train Iteration : 435 Training Loss : 0.28941742260975334 Training Accuracy 0.8856166666666667\n",
      "Test Iteration : 435 Test Loss : 0.364380570059924 test Accuracy 0.8576\n",
      "Train Iteration : 436 Training Loss : 0.2892056062661834 Training Accuracy 0.88525\n",
      "Test Iteration : 436 Test Loss : 0.364126359991036 test Accuracy 0.8578\n",
      "Train Iteration : 437 Training Loss : 0.2890717398676501 Training Accuracy 0.8857666666666667\n",
      "Test Iteration : 437 Test Loss : 0.3642705612643713 test Accuracy 0.8575\n",
      "Train Iteration : 438 Training Loss : 0.28902912678410514 Training Accuracy 0.8854\n",
      "Test Iteration : 438 Test Loss : 0.36420995267320366 test Accuracy 0.8564\n",
      "Train Iteration : 439 Training Loss : 0.28915752000976835 Training Accuracy 0.8859333333333334\n",
      "Test Iteration : 439 Test Loss : 0.3646418528433192 test Accuracy 0.8578\n",
      "Train Iteration : 440 Training Loss : 0.2894070969783364 Training Accuracy 0.88475\n",
      "Test Iteration : 440 Test Loss : 0.3648391943272051 test Accuracy 0.8564\n",
      "Train Iteration : 441 Training Loss : 0.29004910708475995 Training Accuracy 0.88555\n",
      "Test Iteration : 441 Test Loss : 0.365952218917162 test Accuracy 0.857\n",
      "Train Iteration : 442 Training Loss : 0.29033857563722554 Training Accuracy 0.8837333333333334\n",
      "Test Iteration : 442 Test Loss : 0.36591215220493606 test Accuracy 0.8562\n",
      "Train Iteration : 443 Training Loss : 0.29091953747417615 Training Accuracy 0.8854\n",
      "Test Iteration : 443 Test Loss : 0.3672052409041929 test Accuracy 0.857\n",
      "Train Iteration : 444 Training Loss : 0.2895580391166108 Training Accuracy 0.8843333333333333\n",
      "Test Iteration : 444 Test Loss : 0.36531086728295786 test Accuracy 0.8566\n",
      "Train Iteration : 445 Training Loss : 0.28819903029700417 Training Accuracy 0.8866666666666667\n",
      "Test Iteration : 445 Test Loss : 0.36455820423788154 test Accuracy 0.8573\n",
      "Train Iteration : 446 Training Loss : 0.2868712785957226 Training Accuracy 0.8862166666666667\n",
      "Test Iteration : 446 Test Loss : 0.3629401766521588 test Accuracy 0.8575\n",
      "Train Iteration : 447 Training Loss : 0.2866742214660544 Training Accuracy 0.8866166666666667\n",
      "Test Iteration : 447 Test Loss : 0.36295397712661404 test Accuracy 0.8571\n",
      "Train Iteration : 448 Training Loss : 0.2872648523651831 Training Accuracy 0.8868333333333334\n",
      "Test Iteration : 448 Test Loss : 0.3639144258503481 test Accuracy 0.8579\n",
      "Train Iteration : 449 Training Loss : 0.28747560158081364 Training Accuracy 0.8853333333333333\n",
      "Test Iteration : 449 Test Loss : 0.3638904472411645 test Accuracy 0.8568\n",
      "Train Iteration : 450 Training Loss : 0.287167054149204 Training Accuracy 0.8869833333333333\n",
      "Test Iteration : 450 Test Loss : 0.36418592812995865 test Accuracy 0.8577\n",
      "Train Iteration : 451 Training Loss : 0.28604672215721455 Training Accuracy 0.88645\n",
      "Test Iteration : 451 Test Loss : 0.3627398339586154 test Accuracy 0.8563\n",
      "Train Iteration : 452 Training Loss : 0.28536441315646954 Training Accuracy 0.8874666666666666\n",
      "Test Iteration : 452 Test Loss : 0.3623870274373507 test Accuracy 0.858\n",
      "Train Iteration : 453 Training Loss : 0.2853797349902524 Training Accuracy 0.8872\n",
      "Test Iteration : 453 Test Loss : 0.3625691645111713 test Accuracy 0.8583\n",
      "Train Iteration : 454 Training Loss : 0.2856186963983677 Training Accuracy 0.8864833333333333\n",
      "Test Iteration : 454 Test Loss : 0.3627034636534098 test Accuracy 0.8564\n",
      "Train Iteration : 455 Training Loss : 0.2856110470290706 Training Accuracy 0.8876166666666667\n",
      "Test Iteration : 455 Test Loss : 0.36318015258680447 test Accuracy 0.8582\n",
      "Train Iteration : 456 Training Loss : 0.2849428156202521 Training Accuracy 0.8868833333333334\n",
      "Test Iteration : 456 Test Loss : 0.3622475337900907 test Accuracy 0.8565\n",
      "Train Iteration : 457 Training Loss : 0.28430953594494274 Training Accuracy 0.8879\n",
      "Test Iteration : 457 Test Loss : 0.36196562297172885 test Accuracy 0.8588\n",
      "Train Iteration : 458 Training Loss : 0.28404056482341383 Training Accuracy 0.88785\n",
      "Test Iteration : 458 Test Loss : 0.3617405580441057 test Accuracy 0.8586\n",
      "Train Iteration : 459 Training Loss : 0.28408659054158253 Training Accuracy 0.8875166666666666\n",
      "Test Iteration : 459 Test Loss : 0.36178807761525167 test Accuracy 0.8574\n",
      "Train Iteration : 460 Training Loss : 0.2841083670076773 Training Accuracy 0.8881166666666667\n",
      "Test Iteration : 460 Test Loss : 0.36219840357050703 test Accuracy 0.8585\n",
      "Train Iteration : 461 Training Loss : 0.28375247917967755 Training Accuracy 0.8874666666666666\n",
      "Test Iteration : 461 Test Loss : 0.3616625374018955 test Accuracy 0.857\n",
      "Train Iteration : 462 Training Loss : 0.28327541667935247 Training Accuracy 0.8882333333333333\n",
      "Test Iteration : 462 Test Loss : 0.36151897703041214 test Accuracy 0.8592\n",
      "Train Iteration : 463 Training Loss : 0.282902320576304 Training Accuracy 0.88795\n",
      "Test Iteration : 463 Test Loss : 0.3611314625069668 test Accuracy 0.8588\n",
      "Train Iteration : 464 Training Loss : 0.282769542894005 Training Accuracy 0.88805\n",
      "Test Iteration : 464 Test Loss : 0.3611061295623359 test Accuracy 0.8581\n",
      "Train Iteration : 465 Training Loss : 0.2827349247879139 Training Accuracy 0.8885\n",
      "Test Iteration : 465 Test Loss : 0.3613602854500239 test Accuracy 0.8588\n",
      "Train Iteration : 466 Training Loss : 0.2825506365665365 Training Accuracy 0.888\n",
      "Test Iteration : 466 Test Loss : 0.36107602074422446 test Accuracy 0.8576\n",
      "Train Iteration : 467 Training Loss : 0.2822319412796308 Training Accuracy 0.8886166666666667\n",
      "Test Iteration : 467 Test Loss : 0.36107884132749524 test Accuracy 0.8591\n",
      "Train Iteration : 468 Training Loss : 0.2818553525376993 Training Accuracy 0.88855\n",
      "Test Iteration : 468 Test Loss : 0.36067139057792946 test Accuracy 0.8588\n",
      "Train Iteration : 469 Training Loss : 0.2815882898303131 Training Accuracy 0.8888833333333334\n",
      "Test Iteration : 469 Test Loss : 0.36058198288550375 test Accuracy 0.8593\n",
      "Train Iteration : 470 Training Loss : 0.2814414312970235 Training Accuracy 0.8889\n",
      "Test Iteration : 470 Test Loss : 0.36061387562499375 test Accuracy 0.8588\n",
      "Train Iteration : 471 Training Loss : 0.2813134293425114 Training Accuracy 0.8885166666666666\n",
      "Test Iteration : 471 Test Loss : 0.36047782597503103 test Accuracy 0.8583\n",
      "Train Iteration : 472 Training Loss : 0.28112210782343 Training Accuracy 0.8891666666666667\n",
      "Test Iteration : 472 Test Loss : 0.36058413880653434 test Accuracy 0.8594\n",
      "Train Iteration : 473 Training Loss : 0.28083503593228454 Training Accuracy 0.8886833333333334\n",
      "Test Iteration : 473 Test Loss : 0.3602680260256249 test Accuracy 0.8585\n",
      "Train Iteration : 474 Training Loss : 0.28053731090757134 Training Accuracy 0.8893166666666666\n",
      "Test Iteration : 474 Test Loss : 0.3601873330337351 test Accuracy 0.8593\n",
      "Train Iteration : 475 Training Loss : 0.28028566600853994 Training Accuracy 0.88935\n",
      "Test Iteration : 475 Test Loss : 0.3600013072547917 test Accuracy 0.859\n",
      "Train Iteration : 476 Training Loss : 0.2800959174058485 Training Accuracy 0.8893666666666666\n",
      "Test Iteration : 476 Test Loss : 0.35991144326960983 test Accuracy 0.8593\n",
      "Train Iteration : 477 Training Loss : 0.27993395573906543 Training Accuracy 0.8895666666666666\n",
      "Test Iteration : 477 Test Loss : 0.359967075847159 test Accuracy 0.8593\n",
      "Train Iteration : 478 Training Loss : 0.27974586409168245 Training Accuracy 0.8893\n",
      "Test Iteration : 478 Test Loss : 0.35978779236682046 test Accuracy 0.859\n",
      "Train Iteration : 479 Training Loss : 0.27952269498291304 Training Accuracy 0.8898166666666667\n",
      "Test Iteration : 479 Test Loss : 0.35980761256153265 test Accuracy 0.8594\n",
      "Train Iteration : 480 Training Loss : 0.279264472305719 Training Accuracy 0.8896333333333334\n",
      "Test Iteration : 480 Test Loss : 0.3595694504879485 test Accuracy 0.8592\n",
      "Train Iteration : 481 Training Loss : 0.27901058424912567 Training Accuracy 0.8899666666666667\n",
      "Test Iteration : 481 Test Loss : 0.35949871502542746 test Accuracy 0.8593\n",
      "Train Iteration : 482 Training Loss : 0.2787804052771592 Training Accuracy 0.8899833333333333\n",
      "Test Iteration : 482 Test Loss : 0.3593595294122053 test Accuracy 0.8594\n",
      "Train Iteration : 483 Training Loss : 0.27857793829237537 Training Accuracy 0.8900166666666667\n",
      "Test Iteration : 483 Test Loss : 0.35924787686224174 test Accuracy 0.8596\n",
      "Train Iteration : 484 Training Loss : 0.2783908284098406 Training Accuracy 0.89025\n",
      "Test Iteration : 484 Test Loss : 0.3592525495269 test Accuracy 0.8595\n",
      "Train Iteration : 485 Training Loss : 0.2782002603697248 Training Accuracy 0.88995\n",
      "Test Iteration : 485 Test Loss : 0.3590767515919141 test Accuracy 0.8591\n",
      "Train Iteration : 486 Training Loss : 0.2779975687945202 Training Accuracy 0.8903\n",
      "Test Iteration : 486 Test Loss : 0.35909844986311884 test Accuracy 0.8598\n",
      "Train Iteration : 487 Training Loss : 0.277774256281198 Training Accuracy 0.89015\n",
      "Test Iteration : 487 Test Loss : 0.3589013653250977 test Accuracy 0.8595\n",
      "Train Iteration : 488 Training Loss : 0.27754372333233795 Training Accuracy 0.8904666666666666\n",
      "Test Iteration : 488 Test Loss : 0.3588813165197005 test Accuracy 0.86\n",
      "Train Iteration : 489 Training Loss : 0.2773096759257012 Training Accuracy 0.8903166666666666\n",
      "Test Iteration : 489 Test Loss : 0.35870712230526103 test Accuracy 0.8594\n",
      "Train Iteration : 490 Training Loss : 0.27708308066531373 Training Accuracy 0.8906833333333334\n",
      "Test Iteration : 490 Test Loss : 0.3586404387538171 test Accuracy 0.8593\n",
      "Train Iteration : 491 Training Loss : 0.2768650339983091 Training Accuracy 0.8905333333333333\n",
      "Test Iteration : 491 Test Loss : 0.358536615401035 test Accuracy 0.8595\n",
      "Train Iteration : 492 Training Loss : 0.2766564750588661 Training Accuracy 0.89065\n",
      "Test Iteration : 492 Test Loss : 0.3584352772615146 test Accuracy 0.8596\n",
      "Train Iteration : 493 Training Loss : 0.2764531713016375 Training Accuracy 0.8907666666666667\n",
      "Test Iteration : 493 Test Loss : 0.35838405181348787 test Accuracy 0.8594\n",
      "Train Iteration : 494 Training Loss : 0.2762516655956504 Training Accuracy 0.89065\n",
      "Test Iteration : 494 Test Loss : 0.35824672883311265 test Accuracy 0.8597\n",
      "Train Iteration : 495 Training Loss : 0.2760502315881854 Training Accuracy 0.8907166666666667\n",
      "Test Iteration : 495 Test Loss : 0.3582440435633671 test Accuracy 0.8596\n",
      "Train Iteration : 496 Training Loss : 0.2758443959287928 Training Accuracy 0.8907166666666667\n",
      "Test Iteration : 496 Test Loss : 0.358082208608752 test Accuracy 0.8598\n",
      "Train Iteration : 497 Training Loss : 0.2756365749318301 Training Accuracy 0.8910166666666667\n",
      "Test Iteration : 497 Test Loss : 0.3580816319449149 test Accuracy 0.8598\n",
      "Train Iteration : 498 Training Loss : 0.27542522056097446 Training Accuracy 0.8907666666666667\n",
      "Test Iteration : 498 Test Loss : 0.3579219817951336 test Accuracy 0.8601\n",
      "Train Iteration : 499 Training Loss : 0.2752132186521509 Training Accuracy 0.8911666666666667\n",
      "Test Iteration : 499 Test Loss : 0.357905770164335 test Accuracy 0.86\n",
      "Train Iteration : 500 Training Loss : 0.27499961103989895 Training Accuracy 0.8911166666666667\n",
      "Test Iteration : 500 Test Loss : 0.3577576759065685 test Accuracy 0.8602\n"
     ]
    }
   ],
   "source": [
    "# # for easy multiplication now the shape of X_train(784,60000) and X_test(784,10000)\n",
    "# take transpose of X(number of samples)\n",
    "X_train = tf.convert_to_tensor(tr_x.T, dtype=tf.float64)\n",
    "X_test  = tf.convert_to_tensor(te_x.T,dtype= tf.float64)\n",
    "tr_y =    tf.convert_to_tensor(tr_y ,dtype= tf.float64)\n",
    "te_y =    tf.convert_to_tensor(te_y ,dtype= tf.float64)\n",
    "\n",
    "no_of_neurons = 200\n",
    "#describes the number of types of clothes(10)\n",
    "no_of_output_units  = 10\n",
    "no_of_features = X_train.shape[0]\n",
    "\n",
    "# define the weight whos shape is as the no_of_neurons ; but the weights cannot be same . because if we give same weights then the layers won't learn properly to produce different outputs\n",
    "# Multiply with small factor , its just a way to initialize the weights, we can experiment keeping (0.01) or removing (0.01)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(shape=(no_of_neurons, no_of_features), dtype=tf.float64) * 0.01)\n",
    "#define the bais as vector of zeros \n",
    "b1 = tf.Variable(tf.zeros((no_of_neurons, 1),dtype=tf.float64))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal(shape=(no_of_output_units, no_of_neurons), dtype=tf.float64) * 0.01)\n",
    "# b2 = tf.cast(tf.Variable(tf.zeros((no_of_output_units, 1))), tf.float64)\n",
    "b2 = tf.Variable(tf.zeros((no_of_output_units, 1),dtype=tf.float64))\n",
    "\n",
    "train_loss_list , train_acc_list , test_loss_list, test_acc_list = gradient_descent(X_train,tr_y,X_test,te_y, W1 , b1 , W2 , b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 53406,
     "status": "ok",
     "timestamp": 1618574494827,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "QMy3tc3DB6Xf",
    "outputId": "4b2a4ebe-6738-4a7c-ec59-70bf9c7c37f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2ea05216d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(test_loss_list)\n",
    "plt.title(\"loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 54710,
     "status": "ok",
     "timestamp": 1618574496136,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "bzp4d5WF33qo",
    "outputId": "f1a43550-f603-4ca2-97cb-be26af737d0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2ea0480090>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc_list)\n",
    "plt.plot(test_acc_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'test'], loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 54707,
     "status": "ok",
     "timestamp": 1618574496138,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "PGSIJAymuv4z",
    "outputId": "20b99cde-00e2-4768-fd0a-f0cfe1495ce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2ea01c6090>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(train_acc_list))\n",
    "plt.plot(np.log(test_acc_list))\n",
    "plt.plot(np.log(train_loss_list))\n",
    "plt.plot(np.log(test_loss_list))\n",
    "plt.title(\"Accuracy/Loss\")\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel(\"Accuracy/Loss\")\n",
    "plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 54707,
     "status": "ok",
     "timestamp": 1618574496140,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "aY_tf_gpzd78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 54706,
     "status": "ok",
     "timestamp": 1618574496141,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "yiV15sW_zeDj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 54706,
     "status": "ok",
     "timestamp": 1618574496142,
     "user": {
      "displayName": "Namrata Hadimani",
      "photoUrl": "",
      "userId": "14346383599399913830"
     },
     "user_tz": -60
    },
    "id": "JAL_48Biuv7j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNx0HrYKkPZp1OHBHesKgFa",
   "collapsed_sections": [],
   "name": "Question1_1_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
